{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 10: Time Series in Keras**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 10 Material\n",
    "\n",
    "* Part 10.1: Time Series Data Encoding for Deep Learning [[Video]]() [[Notebook]](t81_558_class_10_1_timeseries.ipynb)\n",
    "* Part 10.2: Programming LSTM with Keras and TensorFlow [[Video]]() [[Notebook]](t81_558_class_10_2_lstm.ipynb)\n",
    "* **Part 10.3: Text Generation with Keras and TensorFlow** [[Video]]() [[Notebook]](t81_558_class_10_3_text_generation.ipynb)\n",
    "* Part 10.4: Image Captioning with Keras and TensorFlow [[Video]]() [[Notebook]](t81_558_class_10_4_captioning.ipynb)\n",
    "* Part 10.5: Temporal CNN in Keras and TensorFlow [[Video]]() [[Notebook]](t81_558_class_10_5_temporal_cnn.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 10.3: Text Generation with LSTM\n",
    "\n",
    "Recurrent neural networks are also known for their ability to generate text.  This can allow the output of the neural network to be free-form text.  In this part we will see how an LSTM can be trained on a textual document, such as classic literature, and learn to output new text that appears to be of the same form as the training material.  If you train your LSTM on [Shakespeare](https://en.wikipedia.org/wiki/William_Shakespeare), then it will learn to crank out new prose that is similar to what Shakespeare had written. \n",
    "\n",
    "Don't get your hopes up.  Your not going to each your deep neural network to write the next [Pulitzer Prize for Fiction](https://en.wikipedia.org/wiki/Pulitzer_Prize_for_Fiction).  The prose generated by your neural network will be nonsensical.  However, it will usually be nearly grammatically and of a similar style as the source training documents. \n",
    "\n",
    "A neural network generating nonsensical text based on literature may not seem terribly useful at first glance.  However, the reason that this technology gets so much interest is that it forms the foundation for many more advanced technologies.  The fact that the LSTM will typically learn human grammar from the source document opens a wide range of possibilities. Similar technology can be used to complete sentences when a user is entering text.  Simply the ability to output free-form text becomes the foundation of many other technologies.  In the next part, we will make use of this technique to create a neural network that can write captions for images to describe what is going on in the image. \n",
    "\n",
    "### Additional Information\n",
    "\n",
    "The following are some of the articles that I found useful putting this section together.\n",
    "\n",
    "* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "* [Text Generation With LSTM Recurrent Neural Networks in Python with Keras](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/)\n",
    "* [How to Develop a Word-Level Neural Language Model and Use it to Generate Text](https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/)\n",
    "\n",
    "### Character-Level Text Generation\n",
    "\n",
    "There are a number of different approaches to teaching a neural network to output free-form text.  The most basic question is if you wish the neural network to learn at the word or character level.  In many ways, lerning at the character level is the more interesting of the two.  The LSTM is learning construct its own words without even being shown what a word is.  We will begin with character-level text generation.  In the next module, we will see how we can use nearly the same technique to operate at the word level.  The automatic captioning that will be implemented in the next module is at the word level.\n",
    "\n",
    "We begin by importing the needed Python packages and defining the sequence length, named **CHAR_SEQ_LEN**.  Time-series neural networks always accept their input as a fixed length array.  Not all of the sequence might be used, it is common to fill extra elements with zeros.  The text will be divided into sequences of this length and the neural network will be trained to predict what comes after this sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "CHAR_SEQ_LEN = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this simple example we will train the neural network on the classic children's book [Treasure Island](https://en.wikipedia.org/wiki/Treasure_Island).  We begin by loading this text into a Python string and displaying the first 1,000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿the project gutenberg ebook of treasure island, by robert louis stevenson\r\n",
      "\r\n",
      "this ebook is for the use of anyone anywhere at no cost and with\r\n",
      "almost no restrictions whatsoever.  you may copy it, give it away or\r\n",
      "re-use it under the terms of the project gutenberg license included\r\n",
      "with this ebook or online at www.gutenberg.net\r\n",
      "\r\n",
      "\r\n",
      "title: treasure island\r\n",
      "\r\n",
      "author: robert louis stevenson\r\n",
      "\r\n",
      "illustrator: milo winter\r\n",
      "\r\n",
      "release date: january 12, 2009 [ebook #27780]\r\n",
      "\r\n",
      "language: english\r\n",
      "\r\n",
      "\r\n",
      "*** start of this project gutenberg ebook treasure island ***\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "produced by juliet sutherland, stephen blundell and the\r\n",
      "online distributed proofreading team at http://www.pgdp.net\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      " the illustrated children's library\r\n",
      "\r\n",
      "\r\n",
      "         _treasure island_\r\n",
      "\r\n",
      "       robert louis stevenson\r\n",
      "\r\n",
      "          _illustrated by_\r\n",
      "            milo winter\r\n",
      "\r\n",
      "\r\n",
      "           [illustration]\r\n",
      "\r\n",
      "\r\n",
      "           gramercy books\r\n",
      "              new york\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      " foreword copyright â© 1986 by random house v\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(\"https://data.heatonresearch.com/data/t81-558/text/treasure_island.txt\")\n",
    "raw_text = r.text.lower()\n",
    "\n",
    "print(raw_text[0:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extract all unique characters from the text and sort them.  This allows us to assign a unique ID to each character.  Because the characters are sorted, these IDs should remain the same.  If new characters were added to the original text, then the IDs would change.  We build up two dictionaries.  The first **char2idx** is used to convert a character into its ID.  The second **idx2char** converts an ID back into its character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_array = sorted(list(set(raw_text)))\n",
    "char2idx = dict((n, v) for v, n in enumerate(char_array))\n",
    "idx2char = dict((n, v) for n, v in enumerate(char_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete set of characters in *Treasure Island* is presented here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n|\\r| |!|\"|#|$|%|&|\\'|(|)|*|,|-|.|/|0|1|2|3|4|5|6|7|8|9|:|;|?|@|[|]|_|a|b|c|d|e|f|g|h|i|j|k|l|m|n|o|p|q|r|s|t|u|v|w|x|y|z|©|°|·|»|¿|â|ï'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'|'.join(char_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stats on this text are shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters: 397419\n",
      "Total Unique Used Characters: 67\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Characters: {len(raw_text)}\")\n",
    "print(f\"Total Unique Used Characters: {len(char_array)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete lookup table for all the characters and IDs is shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " '\\r': 1,\n",
       " ' ': 2,\n",
       " '!': 3,\n",
       " '\"': 4,\n",
       " '#': 5,\n",
       " '$': 6,\n",
       " '%': 7,\n",
       " '&': 8,\n",
       " \"'\": 9,\n",
       " '(': 10,\n",
       " ')': 11,\n",
       " '*': 12,\n",
       " ',': 13,\n",
       " '-': 14,\n",
       " '.': 15,\n",
       " '/': 16,\n",
       " '0': 17,\n",
       " '1': 18,\n",
       " '2': 19,\n",
       " '3': 20,\n",
       " '4': 21,\n",
       " '5': 22,\n",
       " '6': 23,\n",
       " '7': 24,\n",
       " '8': 25,\n",
       " '9': 26,\n",
       " ':': 27,\n",
       " ';': 28,\n",
       " '?': 29,\n",
       " '@': 30,\n",
       " '[': 31,\n",
       " ']': 32,\n",
       " '_': 33,\n",
       " 'a': 34,\n",
       " 'b': 35,\n",
       " 'c': 36,\n",
       " 'd': 37,\n",
       " 'e': 38,\n",
       " 'f': 39,\n",
       " 'g': 40,\n",
       " 'h': 41,\n",
       " 'i': 42,\n",
       " 'j': 43,\n",
       " 'k': 44,\n",
       " 'l': 45,\n",
       " 'm': 46,\n",
       " 'n': 47,\n",
       " 'o': 48,\n",
       " 'p': 49,\n",
       " 'q': 50,\n",
       " 'r': 51,\n",
       " 's': 52,\n",
       " 't': 53,\n",
       " 'u': 54,\n",
       " 'v': 55,\n",
       " 'w': 56,\n",
       " 'x': 57,\n",
       " 'y': 58,\n",
       " 'z': 59,\n",
       " '©': 60,\n",
       " '°': 61,\n",
       " '·': 62,\n",
       " '»': 63,\n",
       " '¿': 64,\n",
       " 'â': 65,\n",
       " 'ï': 66}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to build the actual sequences.  Just like previous neural networks there will be an $x$ and $y$.  However, for the LSTM, $x$ and $y$ will both be sequences.  The $x$ input will specify the sequences where $y$ are the expected output.  The following code generates all possible sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences:  397319\n"
     ]
    }
   ],
   "source": [
    "raw_x = []\n",
    "raw_y = []\n",
    "\n",
    "for i in range(0, len(raw_text) - CHAR_SEQ_LEN, 1):\n",
    "    seq_input = raw_text[i:i + CHAR_SEQ_LEN]\n",
    "    seq_expected = raw_text[i + CHAR_SEQ_LEN]\n",
    "    raw_x.append([char2idx[ch] for ch in seq_input])\n",
    "    raw_y.append(char2idx[seq_expected])\n",
    "\n",
    "print(\"Total Sequences: \", len(raw_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now convert the *raw_x* and *raw_y* into the actual Numpy arrays that the neural network will be trained on.  The x-sequences will be normalized to between 0 and 1.  The y-sequences will be converted to dummy variables.  The dummy variables are necessary so that the neural network can predict the probability for each character occurring next. For the text completion, the character with the highest probability will be chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.reshape(raw_x, (len(raw_x), CHAR_SEQ_LEN, 1))\n",
    "x = x / float(len(char_array))\n",
    "y = pd.get_dummies(raw_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dummy variables for $y$ are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  54  55  56  57  58  59  60  \\\n",
       "0   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "1   0   0   1   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "2   0   0   0   0   0   0   0   0   0   0  ...   1   0   0   0   0   0   0   \n",
       "3   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "4   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "5   0   0   1   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "6   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "7   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "8   0   0   1   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "9   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "\n",
       "   61  62  65  \n",
       "0   0   0   0  \n",
       "1   0   0   0  \n",
       "2   0   0   0  \n",
       "3   0   0   0  \n",
       "4   0   0   0  \n",
       "5   0   0   0  \n",
       "6   0   0   0  \n",
       "7   0   0   0  \n",
       "8   0   0   0  \n",
       "9   0   0   0  \n",
       "\n",
       "[10 rows x 64 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the neural network.  The primary feature of this neural network is the LSTM layer.  This allows the sequences to be processed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(x.shape[1], x.shape[2])))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train the neural network, we ensure that there is not already a saved copy of the neural network from a previous train.  It can take up to several hours to train this network, depending on how fast your computer is.  If you have a GPU available, please make sure to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = os.path.join('.','dnn','generate_text_char_network.hdf5')\n",
    "\n",
    "if not os.path.exists(model_filename):\n",
    "    model.fit(x, y, epochs=25, batch_size=64)\n",
    "    model.save(model_filename)\n",
    "else:\n",
    "    model.load_weights(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the neural network is trained, it is saved.\n",
    "\n",
    "Now that the neural network is trained, we are ready to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Attempt #0, starting point:\n",
      "dder was banging to and fro, and\n",
      "the whole ship creaking, groaning, and jumping like a manufactory.\n",
      "\n",
      "Generating text (character by character)...\n",
      " and\n",
      "the serer of the seie the saie oi the saae of the sooc and she saie\n",
      "sooe thre the saie oi the saae of the sooc and she tase thre the sase\n",
      "and sooe of the sooc and she sase thre the sooc of the sooc and she\n",
      "cooter of the saae of the sooc and she tase thre the sooc of the sale\n",
      "and sooe of the sooc and she sase thre the sooc of the sooc and she\n",
      "sooe of the sooc and she sase thre the sooc of the sooc and she sase\n",
      "so the saad on the saad of the sooc and she tase thre the seie thet\n",
      "wase t\n",
      "******************************\n",
      "Attempt #1, starting point:\n",
      "rselves to!--you needn't stand up for\n",
      "mr. hawkins; _he'll_ excuse you, you may lay to that. and so,\n",
      "\n",
      "Generating text (character by character)...\n",
      "te to geve\n",
      "and soin oo the sooc of the sooc of the sooc of the sooc and see the\n",
      "dort oa the shop wi have tee the siop was she whrle oo the saad of the\n",
      "soon. and the seme thre the serere oa the saie oirele was she tase\n",
      "sooein the said of the sooc and she tase thre the sooc of the sooc of\n",
      "the sooc and the saie oi the saie oi the saae of the sooc and she sase\n",
      "and shee of the sooc and she saie oo the saae of the sooc and she sase\n",
      "and shee and the sooc of the sooc and she tase thre the sooc of\n",
      "******************************\n",
      "Attempt #2, starting point:\n",
      "ction\n",
      "that i might pass the stream while it was small. the wood was pretty\n",
      "open, and keeping along\n",
      "\n",
      "Generating text (character by character)...\n",
      " the sooc of the sooc and she tase thre the\n",
      "dorrent of the sooc and she saie oi the sooc of the sooc and she saie\n",
      "so the saad of the sooc and she tase thre the sooc of the sooc and\n",
      "there the saie oi the saae of the sooc and she tase thre the saae of\n",
      "their war ano the sooc of the sooc and she sase thre the sooc of the\n",
      "sooe of the sooc and she sase thre the sooc of the sooc and she sase\n",
      "and sooe of the sooc and see the saie oirele whsh a cootere that he\n",
      "cada to the dort of the sooc and she \n",
      "******************************\n",
      "Attempt #3, starting point:\n",
      "stores over the palisade. then, leaving joyce to\n",
      "guard them--one man, to be sure, but with half a d\n",
      "\n",
      "Generating text (character by character)...\n",
      "oot of the saie of\n",
      "the sooc and she saie oo the saae of the sooc and she saie oi the saad\n",
      "and she saie oi the saae of the sooc and she tase thre the sooc of the\n",
      "sooe of the sooc and she sase thre the sooc of the sooc and she sase\n",
      "and sooe of the sooc and see the saie oirele whsh a cootere that he\n",
      "cada to the dort of the sooc and she whree of the sooc of the sooc of\n",
      "the coctor san the sooc of the sooc and she tase thre the sooc of the\n",
      "sooc of the sooc and she saie oo the saad of the sooc a\n",
      "******************************\n",
      "Attempt #4, starting point:\n",
      "st of weather\n",
      "cross the deck. he had a line or two rigged up to help him across the\n",
      "widest spaces-\n",
      "\n",
      "Generating text (character by character)...\n",
      " and the same oimet he was soint oo the sooc of the saar\n",
      "and the sooc of the sooc and seered to the sooc of the sooc and she\n",
      "corter of the saae on the sooc of the sooc. and the seme of the saie of\n",
      "the saae of the saie oi the saae of the sooc and she tase thre the sase\n",
      "and sooe of the sooc and she sase thre the sooc of the sooc and she\n",
      "cooter of the saae of the sooc and she tase thre the sooc of the sale\n",
      "and sooe of the sooc and she sase thre the sooc of the sooc and she\n",
      "sooe of the sooc a\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"******************************\")\n",
    "    # pick a random seed\n",
    "    start = np.random.randint(0, len(raw_x)-1)\n",
    "    current = raw_x[start]\n",
    "    print(f\"Attempt #{i}, starting point:\")\n",
    "    print(''.join([idx2char[x] for x in current]))\n",
    "\n",
    "    print()\n",
    "    print(\"Generating text (character by character)...\")\n",
    "    output = \"\"\n",
    "\n",
    "    for i in range(500):\n",
    "        x = np.reshape(current, (1, len(current), 1)) / float(len(char_array))\n",
    "        prediction = model.predict(x, verbose=0)\n",
    "        idx = np.argmax(prediction)\n",
    "        output += idx2char[idx]\n",
    "        seq_in = [idx2char[v] for v in current]\n",
    "        current.append(idx)\n",
    "        current = current[1:len(current)]\n",
    "\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow-2.0)",
   "language": "python",
   "name": "tensorflow-2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
