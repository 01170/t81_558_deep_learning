{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Class 12: Deep Learning Applications**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tonight we will see how to apply deep learning networks to data science.  There are many applications of deep learning.  However, we will focus primarily upon data science.  For this class we will go beyond simple academic examples and see how to construct an ensemble that could potentially lead to a high score on a Kaggle competition.  We will see how to evaluate the importance of features and several ways to combine models.\n",
    "\n",
    "Tonights topics include:\n",
    "\n",
    "* Log Loss Error\n",
    "* Evaluating Feature Importance\n",
    "* The Biological Response Data Set\n",
    "* Neural Network Bagging\n",
    "* Nueral Network Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions from Previous Classes\n",
    "\n",
    "The following are utility functions from previous classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df,name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name,x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the origional column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df,name,target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x)==str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name,tv)\n",
    "        df[name2] = l\n",
    "    \n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df,name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df,name,mean=None,sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name]-mean)/sd\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df,target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    \n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        return df.as_matrix(result).astype(np.float32),df.as_matrix([target]).astype(np.int32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32),df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "# Regression chart, we will see more of this chart in the next class.\n",
    "def chart_regression(pred,y):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y_test.flatten()})\n",
    "    t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# Get a new directory to hold checkpoints from a neural network.  This allows the neural network to be\n",
    "# loaded later.  If the erase param is set to true, the contents of the directory will be cleared.\n",
    "def get_model_dir(name,erase):\n",
    "    base_path = os.path.join(\".\",\"dnn\")\n",
    "    model_dir = os.path.join(base_path,name)\n",
    "    os.makedirs(model_dir,exist_ok=True)\n",
    "    if erase and len(model_dir)>4 and os.path.isdir(model_dir):\n",
    "        shutil.rmtree(model_dir,ignore_errors=True) # be careful, this deletes everything below the specified path\n",
    "    return model_dir\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name]-df[name].mean())>=(sd*df[name].std()))]\n",
    "    df.drop(drop_rows,axis=0,inplace=True)\n",
    "    \n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low =-1, normalized_high =1, \n",
    "                         data_low=None, data_high=None):\n",
    "    \n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "    \n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "                * (normalized_high - normalized_low) + normalized_low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogLoss Error\n",
    "\n",
    "Log loss is an error metric that is often used in place of accuracy for classification.  Log loss allows for \"partial credit\" when a miss classification occurs.  For example, a model might be used to classify A, B and C.  The correct answer might be A, however if the classification network chose B as having the highest probability, then accuracy gives the neural network no credit for this classification.  \n",
    "\n",
    "However, with log loss, the probability of the correct answer is added to the score.  For example, the correct answer might be A, but if the neural network only predicted .8 probability of A being correct, then the value -log(.8) is added.\n",
    "\n",
    "$$ logloss = -\\frac{1}{N}\\sum^N_{i=1}\\sum^M_{j=1}y_{ij} \\log(\\hat{y}_{ij}) $$\n",
    "\n",
    "The following table shows the logloss scores that correspond to the average predicted accuracy for the correct item. The **pred** column specifies the average probability for the correct class.  The **logloss** column specifies the log loss for that probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>logloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.000000e-01</td>\n",
       "      <td>0.105361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.000000e-01</td>\n",
       "      <td>0.223144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.000000e-01</td>\n",
       "      <td>0.356675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.000000e-01</td>\n",
       "      <td>0.510826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.000000e-01</td>\n",
       "      <td>0.916291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.000000e-01</td>\n",
       "      <td>1.203973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>1.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>2.302585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.500000e-02</td>\n",
       "      <td>2.590267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.000000e-02</td>\n",
       "      <td>2.995732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.500000e-02</td>\n",
       "      <td>3.688879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>18.420681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pred    logloss\n",
       "0   1.000000e+00  -0.000000\n",
       "1   9.000000e-01   0.105361\n",
       "2   8.000000e-01   0.223144\n",
       "3   7.000000e-01   0.356675\n",
       "4   6.000000e-01   0.510826\n",
       "5   5.000000e-01   0.693147\n",
       "6   4.000000e-01   0.916291\n",
       "7   3.000000e-01   1.203973\n",
       "8   2.000000e-01   1.609438\n",
       "9   1.000000e-01   2.302585\n",
       "10  7.500000e-02   2.590267\n",
       "11  5.000000e-02   2.995732\n",
       "12  2.500000e-02   3.688879\n",
       "13  1.000000e-08  18.420681"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "loss = [1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.075, 0.05, 0.025, 1e-8 ]\n",
    "\n",
    "df = pd.DataFrame({'pred':loss, 'logloss': -np.log(loss)},columns=['pred','logloss'])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows the opposit.  For a given logloss, what is the average probability for the correct class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logloss</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.904837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.818731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.740818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.670320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.606531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.548812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.496585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.449329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.406570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.367879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.223130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.135335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.5</td>\n",
       "      <td>0.082085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.049787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.5</td>\n",
       "      <td>0.030197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.018316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    logloss      pred\n",
       "0       0.1  0.904837\n",
       "1       0.2  0.818731\n",
       "2       0.3  0.740818\n",
       "3       0.4  0.670320\n",
       "4       0.5  0.606531\n",
       "5       0.6  0.548812\n",
       "6       0.7  0.496585\n",
       "7       0.8  0.449329\n",
       "8       0.9  0.406570\n",
       "9       1.0  0.367879\n",
       "10      1.5  0.223130\n",
       "11      2.0  0.135335\n",
       "12      2.5  0.082085\n",
       "13      3.0  0.049787\n",
       "14      3.5  0.030197\n",
       "15      4.0  0.018316"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "loss = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.5, 2, 2.5, 3, 3.5, 4 ]\n",
    "\n",
    "df = pd.DataFrame({'logloss':loss, 'pred': np.exp(np.negative(loss))},\n",
    "                  columns=['logloss','pred'])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluating Feature Importance\n",
    "\n",
    "Feature importance tells us how important each of the features (from the feature/import vector are to the prediction of a neural network, or other model.  There are many different ways to evaluate feature importance for neural networks.  The following paper presents a very good (and readable) overview of the various means of evaluating the importance of neural network inputs/features.\n",
    "\n",
    "Olden, J. D., Joy, M. K., & Death, R. G. (2004). [An accurate comparison of methods for quantifying variable importance in artificial neural networks using simulated data](http://depts.washington.edu/oldenlab/wordpress/wp-content/uploads/2013/03/EcologicalModelling_2004.pdf). *Ecological Modelling*, 178(3), 389-397.\n",
    "\n",
    "In summary, the following methods are available to neural networks:\n",
    "\n",
    "* Connection Weights Algorithm\n",
    "* Partial Derivatives\n",
    "* Input Perturbation\n",
    "* Sensitivity Analysis\n",
    "* Forward Stepwise Addition \n",
    "* Improved Stepwise Selection 1\n",
    "* Backward Stepwise Elimination\n",
    "* Improved Stepwise Selection\n",
    "\n",
    "For this class we will use the **Input Perturbation** feature ranking algorithm.  This algorithm will work with any regression or classification network.  implementation of the input perturbation algorithm for scikit-learn is given in the next section. This algorithm is implemented in a function below that will work with any scikit-learn model.\n",
    "\n",
    "This algorithm was introduced by [Breiman](https://en.wikipedia.org/wiki/Leo_Breiman) in his seminal paper on random forests.  Although he presented this algorithm in conjunction with random forests, it is model-independent and appropriate for any supervised learning model.  This algorithm, known as the input perturbation algorithm, works by evaluating a trained model’s accuracy with each of the inputs individually shuffled from a data set.  Shuffling an input causes it to become useless—effectively removing it from the model. More important inputs will produce a less accurate score when they are removed by shuffling them. This process makes sense, because important features will contribute to the accuracy of the model.\n",
    "\n",
    "The provided algorithm will use logloss to evaluate a classification problem and RMSE for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def mlogloss(y_test, preds):\n",
    "    epsilon = 1e-15\n",
    "    sum = 0\n",
    "    for row in zip(preds,y_test):\n",
    "        x = row[0][row[1]]\n",
    "        x = max(epsilon,x)\n",
    "        x = min(1-epsilon,x)\n",
    "        sum+=math.log(x)\n",
    "    return( (-1/len(preds))*sum)\n",
    "\n",
    "def perturbation_rank(model, x, y, names, regression):\n",
    "    errors = []\n",
    "\n",
    "    for i in range(x.shape[1]):\n",
    "        hold = np.array(x[:, i])\n",
    "        np.random.shuffle(x[:, i])\n",
    "        \n",
    "        if regression:\n",
    "            # The following code is only needed until Google fixes SKCOMPAT\n",
    "            # pred = model.predict(x)\n",
    "            pred = list(model.predict(x_test, as_iterable=True))\n",
    "            error = metrics.mean_squared_error(y, pred)\n",
    "        else:\n",
    "            # The following code is only needed until Google fixes SKCOMPAT\n",
    "            # pred = model.predict_proba(x)\n",
    "            pred = list(model.predict_proba(x_test, as_iterable=True))\n",
    "            error = mlogloss(y, pred)\n",
    "            \n",
    "        errors.append(error)\n",
    "        x[:, i] = hold\n",
    "        \n",
    "    max_error = np.max(errors)\n",
    "    importance = [e/max_error for e in errors]\n",
    "   \n",
    "    data = {'name':names,'error':errors,'importance':importance}\n",
    "    result = pd.DataFrame(data, columns = ['name','error','importance'])\n",
    "    result.sort_values(by=['importance'], ascending=[0], inplace=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Input Perturbation Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x10b0effd0>, 'save_checkpoints_steps': None, '_environment': 'local', '_evaluation_master': '', '_is_chief': True, 'tf_random_seed': None, 'save_summary_steps': 100, 'keep_checkpoint_max': 5, '_num_ps_replicas': 0, 'save_checkpoints_secs': 1, '_master': '', '_task_id': 0, 'tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", 'keep_checkpoint_every_n_hours': 10000, '_task_type': None}\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:322 in __init__.: BaseMonitor.__init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:315 in fit.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:315 in fit.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:315 in fit.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_0:fraction_of_zero_values is illegal; using dnn/hiddenlayer_0_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_0:activation is illegal; using dnn/hiddenlayer_0_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_1:fraction_of_zero_values is illegal; using dnn/hiddenlayer_1_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_1:activation is illegal; using dnn/hiddenlayer_1_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_2:fraction_of_zero_values is illegal; using dnn/hiddenlayer_2_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_2:activation is illegal; using dnn/hiddenlayer_2_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/logits:fraction_of_zero_values is illegal; using dnn/logits_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/logits:activation is illegal; using dnn/logits_activation instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:loss = 0.938449, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./dnn/iris/model.ckpt.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:loss = 0.216566, step = 101\n",
      "INFO:tensorflow:global_step/sec: 135.35\n",
      "INFO:tensorflow:loss = 0.134559, step = 201\n",
      "INFO:tensorflow:global_step/sec: 560.812\n",
      "INFO:tensorflow:Saving checkpoints for 238 into ./dnn/iris/model.ckpt.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:loss = 0.100306, step = 301\n",
      "INFO:tensorflow:global_step/sec: 140.914\n",
      "INFO:tensorflow:loss = 0.0811362, step = 401\n",
      "INFO:tensorflow:global_step/sec: 366.664\n",
      "INFO:tensorflow:Saving checkpoints for 438 into ./dnn/iris/model.ckpt.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:323 in evaluate.: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:323 in evaluate.: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:323 in evaluate.: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_0:fraction_of_zero_values is illegal; using dnn/hiddenlayer_0_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_0:activation is illegal; using dnn/hiddenlayer_0_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_1:fraction_of_zero_values is illegal; using dnn/hiddenlayer_1_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_1:activation is illegal; using dnn/hiddenlayer_1_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_2:fraction_of_zero_values is illegal; using dnn/hiddenlayer_2_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_2:activation is illegal; using dnn/hiddenlayer_2_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/logits:fraction_of_zero_values is illegal; using dnn/logits_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/logits:activation is illegal; using dnn/logits_activation instead.\n",
      "INFO:tensorflow:Restored model from ./dnn/iris\n",
      "INFO:tensorflow:Eval steps [0,inf) for training step 438.\n",
      "INFO:tensorflow:Input iterator is exhausted.\n",
      "INFO:tensorflow:Saving evaluation summary for step 438: accuracy = 0.947368, loss = 0.138699\n",
      "INFO:tensorflow:Validation (step 500): accuracy = 0.947368, loss = 0.138699, global_step = 438\n",
      "INFO:tensorflow:loss = 0.0683095, step = 501\n",
      "INFO:tensorflow:global_step/sec: 48.4866\n",
      "INFO:tensorflow:Saving checkpoints for 501 into ./dnn/iris/model.ckpt.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:loss = 0.0594967, step = 601\n",
      "INFO:tensorflow:global_step/sec: 135.536\n",
      "INFO:tensorflow:loss = 0.0530703, step = 701\n",
      "INFO:tensorflow:global_step/sec: 751.721\n",
      "INFO:tensorflow:loss = 0.0481746, step = 801\n",
      "INFO:tensorflow:global_step/sec: 753.301\n",
      "INFO:tensorflow:Saving checkpoints for 801 into ./dnn/iris/model.ckpt.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:loss = 0.0442843, step = 901\n",
      "INFO:tensorflow:global_step/sec: 156.06\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:323 in evaluate.: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:323 in evaluate.: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:323 in evaluate.: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_0:fraction_of_zero_values is illegal; using dnn/hiddenlayer_0_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_0:activation is illegal; using dnn/hiddenlayer_0_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_1:fraction_of_zero_values is illegal; using dnn/hiddenlayer_1_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_1:activation is illegal; using dnn/hiddenlayer_1_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_2:fraction_of_zero_values is illegal; using dnn/hiddenlayer_2_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_2:activation is illegal; using dnn/hiddenlayer_2_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/logits:fraction_of_zero_values is illegal; using dnn/logits_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/logits:activation is illegal; using dnn/logits_activation instead.\n",
      "INFO:tensorflow:Restored model from ./dnn/iris\n",
      "INFO:tensorflow:Eval steps [0,inf) for training step 801.\n",
      "INFO:tensorflow:Input iterator is exhausted.\n",
      "INFO:tensorflow:Saving evaluation summary for step 801: accuracy = 0.921053, loss = 0.141667\n",
      "INFO:tensorflow:Validation (step 1000): accuracy = 0.921053, loss = 0.141667, global_step = 801\n",
      "INFO:tensorflow:Stopping. Best step: 500 with loss = 0.13869909942150116.\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into ./dnn/iris/model.ckpt.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:Loss for final step: 0.0411022.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.learn.python.learn.estimators.dnn.DNNClassifier at 0x115120b38>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classification ranking\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.learn as learn\n",
    "import numpy as np\n",
    "from tensorflow.contrib.learn.python.learn.metric_spec import MetricSpec\n",
    "\n",
    "# Set the desired TensorFlow output level for this example\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "path = \"./data/\"\n",
    "    \n",
    "filename = os.path.join(path,\"iris.csv\")    \n",
    "df = pd.read_csv(filename,na_values=['NA','?'])\n",
    "\n",
    "# Encode feature vector\n",
    "encode_numeric_zscore(df,'petal_w')\n",
    "encode_numeric_zscore(df,'petal_l')\n",
    "encode_numeric_zscore(df,'sepal_w')\n",
    "encode_numeric_zscore(df,'sepal_l')\n",
    "species = encode_text_index(df,\"species\")\n",
    "num_classes = len(species)\n",
    "\n",
    "# Create x & y for training\n",
    "\n",
    "# Create the x-side (feature vectors) of the training\n",
    "x, y = to_xy(df,'species')\n",
    "    \n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=45)\n",
    "\n",
    "# Get/clear a directory to store the neural network to\n",
    "model_dir = get_model_dir('iris',True)\n",
    "\n",
    "# Create a deep neural network with 3 hidden layers of 10, 20, 5\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x.shape[0])]\n",
    "classifier = learn.DNNClassifier(\n",
    "    model_dir= model_dir,\n",
    "    config=tf.contrib.learn.RunConfig(save_checkpoints_secs=1),\n",
    "    hidden_units=[10, 20, 5], n_classes=num_classes, feature_columns=feature_columns)\n",
    "\n",
    "# Might be needed in future versions of \"TensorFlow Learn\"\n",
    "#classifier = learn.SKCompat(classifier) # For Sklearn compatibility\n",
    "\n",
    "# Early stopping\n",
    "validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    every_n_steps=500,\n",
    "    #metrics=validation_metrics,\n",
    "    early_stopping_metric=\"loss\",\n",
    "    early_stopping_metric_minimize=True,\n",
    "    early_stopping_rounds=50)\n",
    "    \n",
    "# Fit/train neural network\n",
    "classifier.fit(x_train, y_train,monitors=[validation_monitor],steps=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>petal_w</td>\n",
       "      <td>1.166125</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>petal_l</td>\n",
       "      <td>1.145329</td>\n",
       "      <td>0.982166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sepal_w</td>\n",
       "      <td>0.397985</td>\n",
       "      <td>0.341289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sepal_l</td>\n",
       "      <td>0.154799</td>\n",
       "      <td>0.132747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name     error  importance\n",
       "3  petal_w  1.166125    1.000000\n",
       "2  petal_l  1.145329    0.982166\n",
       "1  sepal_w  0.397985    0.341289\n",
       "0  sepal_l  0.154799    0.132747"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the desired TensorFlow output level for this example\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Rank the features\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = df.columns.values[0:-1] # x column names\n",
    "rank = perturbation_rank(classifier, x_test, y_test, names, False)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Input Perturbation Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x112bd2eb8>, 'save_checkpoints_steps': None, '_environment': 'local', '_evaluation_master': '', '_is_chief': True, 'tf_random_seed': None, 'save_summary_steps': 100, 'keep_checkpoint_max': 5, '_num_ps_replicas': 0, 'save_checkpoints_secs': 1, '_master': '', '_task_id': 0, 'tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", 'keep_checkpoint_every_n_hours': 10000, '_task_type': None}\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:322 in __init__.: BaseMonitor.__init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "WARNING:tensorflow:From <ipython-input-5-1d3e0effffb2>:58 in <module>.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-5-1d3e0effffb2>:58 in <module>.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_0:fraction_of_zero_values is illegal; using dnn/hiddenlayer_0_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_0:activation is illegal; using dnn/hiddenlayer_0_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_1:fraction_of_zero_values is illegal; using dnn/hiddenlayer_1_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_1:activation is illegal; using dnn/hiddenlayer_1_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_2:fraction_of_zero_values is illegal; using dnn/hiddenlayer_2_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_2:activation is illegal; using dnn/hiddenlayer_2_activation instead.\n",
      "INFO:tensorflow:Summary name logits:fraction_of_zero_values is illegal; using logits_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name logits:activation is illegal; using logits_activation instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:loss = 468.304, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./dnn/mpg/model.ckpt.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:loss = 14.7444, step = 101\n",
      "INFO:tensorflow:global_step/sec: 132.999\n",
      "INFO:tensorflow:Saving checkpoints for 176 into ./dnn/mpg/model.ckpt.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:loss = 13.2569, step = 201\n",
      "INFO:tensorflow:global_step/sec: 120.974\n",
      "INFO:tensorflow:loss = 12.6428, step = 301\n",
      "INFO:tensorflow:global_step/sec: 445.825\n",
      "INFO:tensorflow:Saving checkpoints for 385 into ./dnn/mpg/model.ckpt.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:loss = 12.1614, step = 401\n",
      "INFO:tensorflow:global_step/sec: 165.77\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:712 in every_n_step_end.: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:712 in every_n_step_end.: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:712 in every_n_step_end.: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_0:fraction_of_zero_values is illegal; using dnn/hiddenlayer_0_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_0:activation is illegal; using dnn/hiddenlayer_0_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_1:fraction_of_zero_values is illegal; using dnn/hiddenlayer_1_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_1:activation is illegal; using dnn/hiddenlayer_1_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_2:fraction_of_zero_values is illegal; using dnn/hiddenlayer_2_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_2:activation is illegal; using dnn/hiddenlayer_2_activation instead.\n",
      "INFO:tensorflow:Summary name logits:fraction_of_zero_values is illegal; using logits_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name logits:activation is illegal; using logits_activation instead.\n",
      "INFO:tensorflow:Restored model from ./dnn/mpg\n",
      "INFO:tensorflow:Eval steps [0,inf) for training step 385.\n",
      "INFO:tensorflow:Input iterator is exhausted.\n",
      "INFO:tensorflow:Saving evaluation summary for step 385: loss = 8.31167\n",
      "INFO:tensorflow:Validation (step 500): loss = 8.31167, global_step = 385\n",
      "INFO:tensorflow:loss = 11.7679, step = 501\n",
      "INFO:tensorflow:global_step/sec: 82.7539\n",
      "INFO:tensorflow:Saving checkpoints for 501 into ./dnn/mpg/model.ckpt.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:loss = 11.7258, step = 601\n",
      "INFO:tensorflow:global_step/sec: 157.641\n",
      "INFO:tensorflow:loss = 11.4179, step = 701\n",
      "INFO:tensorflow:global_step/sec: 507.883\n",
      "INFO:tensorflow:Saving checkpoints for 793 into ./dnn/mpg/model.ckpt.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:loss = 11.3731, step = 801\n",
      "INFO:tensorflow:global_step/sec: 179.05\n",
      "INFO:tensorflow:loss = 11.2102, step = 901\n",
      "INFO:tensorflow:global_step/sec: 419.14\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:712 in every_n_step_end.: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:712 in every_n_step_end.: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:712 in every_n_step_end.: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_0:fraction_of_zero_values is illegal; using dnn/hiddenlayer_0_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_0:activation is illegal; using dnn/hiddenlayer_0_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_1:fraction_of_zero_values is illegal; using dnn/hiddenlayer_1_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_1:activation is illegal; using dnn/hiddenlayer_1_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_2:fraction_of_zero_values is illegal; using dnn/hiddenlayer_2_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_2:activation is illegal; using dnn/hiddenlayer_2_activation instead.\n",
      "INFO:tensorflow:Summary name logits:fraction_of_zero_values is illegal; using logits_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name logits:activation is illegal; using logits_activation instead.\n",
      "INFO:tensorflow:Restored model from ./dnn/mpg\n",
      "INFO:tensorflow:Eval steps [0,inf) for training step 793.\n",
      "INFO:tensorflow:Input iterator is exhausted.\n",
      "INFO:tensorflow:Saving evaluation summary for step 793: loss = 7.69862\n",
      "INFO:tensorflow:Validation (step 1000): loss = 7.69862, global_step = 793\n",
      "INFO:tensorflow:loss = 11.6016, step = 1001\n",
      "INFO:tensorflow:global_step/sec: 122.604\n",
      "INFO:tensorflow:Saving checkpoints for 1001 into ./dnn/mpg/model.ckpt.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:loss = 11.1814, step = 1101\n",
      "INFO:tensorflow:global_step/sec: 126.213\n",
      "INFO:tensorflow:Saving checkpoints for 1160 into ./dnn/mpg/model.ckpt.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:loss = 11.1183, step = 1201\n",
      "INFO:tensorflow:global_step/sec: 147.12\n",
      "INFO:tensorflow:loss = 10.9127, step = 1301\n",
      "INFO:tensorflow:global_step/sec: 527.565\n",
      "INFO:tensorflow:loss = 10.9046, step = 1401\n",
      "INFO:tensorflow:global_step/sec: 464.33\n",
      "INFO:tensorflow:Saving checkpoints for 1466 into ./dnn/mpg/model.ckpt.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:712 in every_n_step_end.: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:712 in every_n_step_end.: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:712 in every_n_step_end.: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_0:fraction_of_zero_values is illegal; using dnn/hiddenlayer_0_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_0:activation is illegal; using dnn/hiddenlayer_0_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_1:fraction_of_zero_values is illegal; using dnn/hiddenlayer_1_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_1:activation is illegal; using dnn/hiddenlayer_1_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_2:fraction_of_zero_values is illegal; using dnn/hiddenlayer_2_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_2:activation is illegal; using dnn/hiddenlayer_2_activation instead.\n",
      "INFO:tensorflow:Summary name logits:fraction_of_zero_values is illegal; using logits_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name logits:activation is illegal; using logits_activation instead.\n",
      "INFO:tensorflow:Restored model from ./dnn/mpg\n",
      "INFO:tensorflow:Eval steps [0,inf) for training step 1466.\n",
      "INFO:tensorflow:Input iterator is exhausted.\n",
      "INFO:tensorflow:Saving evaluation summary for step 1466: loss = 7.70984\n",
      "INFO:tensorflow:Validation (step 1500): loss = 7.70984, global_step = 1466\n",
      "INFO:tensorflow:Stopping. Best step: 1000 with loss = 7.6986236572265625.\n",
      "INFO:tensorflow:Saving checkpoints for 1500 into ./dnn/mpg/model.ckpt.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:Loss for final step: 10.8929.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNRegressor(feature_columns=[_RealValuedColumn(column_name='', dimension=398, default_value=None, dtype=tf.float32, normalizer=None)], optimizer=None, hidden_units=[50, 25, 10], dropout=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.contrib.learn as learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "# Set the desired TensorFlow output level for this example\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "filename_read = os.path.join(path,\"auto-mpg.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "# create feature vector\n",
    "missing_median(df, 'horsepower')\n",
    "df.drop('name',1,inplace=True)\n",
    "encode_numeric_zscore(df, 'horsepower')\n",
    "encode_numeric_zscore(df, 'weight')\n",
    "encode_numeric_zscore(df, 'cylinders')\n",
    "encode_numeric_zscore(df, 'displacement')\n",
    "encode_numeric_zscore(df, 'acceleration')\n",
    "encode_text_dummy(df, 'origin')\n",
    "\n",
    "# Encode to a 2D matrix for training\n",
    "x,y = to_xy(df,'mpg')\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Get/clear a directory to store the neural network to\n",
    "model_dir = get_model_dir('mpg',True)\n",
    "\n",
    "# Create a deep neural network with 3 hidden layers of 50, 25, 10\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x.shape[0])]\n",
    "regressor = learn.DNNRegressor(\n",
    "    model_dir= model_dir,\n",
    "    config=tf.contrib.learn.RunConfig(save_checkpoints_secs=1),\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=[50, 25, 10])\n",
    "\n",
    "# Might be needed in future versions of \"TensorFlow Learn\"\n",
    "#classifier = learn.SKCompat(classifier) # For Sklearn compatibility\n",
    "\n",
    "# Early stopping\n",
    "validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    every_n_steps=500,\n",
    "    early_stopping_metric=\"loss\",\n",
    "    early_stopping_metric_minimize=True,\n",
    "    early_stopping_rounds=50)\n",
    "    \n",
    "# Fit/train neural network\n",
    "regressor.fit(x_train, y_train,monitors=[validation_monitor],steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>horsepower</td>\n",
       "      <td>25.487295</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weight</td>\n",
       "      <td>23.591404</td>\n",
       "      <td>0.925614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>year</td>\n",
       "      <td>13.226979</td>\n",
       "      <td>0.518964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>displacement</td>\n",
       "      <td>10.212932</td>\n",
       "      <td>0.400707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acceleration</td>\n",
       "      <td>8.685765</td>\n",
       "      <td>0.340788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>origin-1</td>\n",
       "      <td>8.395874</td>\n",
       "      <td>0.329414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>origin-2</td>\n",
       "      <td>7.903455</td>\n",
       "      <td>0.310094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cylinders</td>\n",
       "      <td>7.881567</td>\n",
       "      <td>0.309235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>origin-3</td>\n",
       "      <td>7.494909</td>\n",
       "      <td>0.294064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name      error  importance\n",
       "2    horsepower  25.487295    1.000000\n",
       "3        weight  23.591404    0.925614\n",
       "5          year  13.226979    0.518964\n",
       "1  displacement  10.212932    0.400707\n",
       "4  acceleration   8.685765    0.340788\n",
       "6      origin-1   8.395874    0.329414\n",
       "7      origin-2   7.903455    0.310094\n",
       "0     cylinders   7.881567    0.309235\n",
       "8      origin-3   7.494909    0.294064"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the desired TensorFlow output level for this example\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Rank the features\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = df.columns.values[1:] # x column names\n",
    "rank = perturbation_rank(regressor, x_test, y_test, names, True)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Biological Response Data Set\n",
    "\n",
    "* [Biological Response Dataset at Kaggle](https://www.kaggle.com/c/bioresponse)\n",
    "* [1st place interview for Boehringer Ingelheim Biological Response](http://blog.kaggle.com/2012/07/05/1st-place-interview-for-boehringer-ingelheim-biological-response/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./data/bio_train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f7a760509af5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mfilename_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"bio_test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mfilename_submit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"bio_submit.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NA'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'?'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NA'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'?'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    643\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4175)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8440)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'./data/bio_train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.learn as skflow\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.cross_validation import KFold\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "filename_train = os.path.join(path,\"bio_train.csv\")\n",
    "filename_test = os.path.join(path,\"bio_test.csv\")\n",
    "filename_submit = os.path.join(path,\"bio_submit.csv\")\n",
    "df_train = pd.read_csv(filename_train,na_values=['NA','?'])\n",
    "df_test = pd.read_csv(filename_test,na_values=['NA','?'])\n",
    "\n",
    "encode_text_index(df_train,'Activity')\n",
    "\n",
    "#display(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biological Response with Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/learn/python/learn/io/data_feeder.py:281: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  out.itemset((i, self.y[sample]), 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #49, avg. train loss: 0.71293, avg. val loss: 0.70956\n",
      "Step #100, epoch #1, avg. train loss: 0.68720, avg. val loss: 0.67496\n",
      "Step #150, epoch #1, avg. train loss: 0.65776, avg. val loss: 0.65117\n",
      "Step #200, epoch #2, avg. train loss: 0.64972, avg. val loss: 0.64277\n",
      "Step #250, epoch #2, avg. train loss: 0.62097, avg. val loss: 0.62835\n",
      "Step #300, epoch #3, avg. train loss: 0.59859, avg. val loss: 0.61697\n",
      "Step #350, epoch #3, avg. train loss: 0.60311, avg. val loss: 0.60590\n",
      "Step #400, epoch #4, avg. train loss: 0.56464, avg. val loss: 0.60644\n",
      "Step #450, epoch #5, avg. train loss: 0.56250, avg. val loss: 0.59649\n",
      "Step #500, epoch #5, avg. train loss: 0.55042, avg. val loss: 0.58440\n",
      "Step #550, epoch #6, avg. train loss: 0.54160, avg. val loss: 0.58837\n",
      "Step #600, epoch #6, avg. train loss: 0.53489, avg. val loss: 0.56726\n",
      "Step #650, epoch #7, avg. train loss: 0.52663, avg. val loss: 0.58412\n",
      "Step #700, epoch #7, avg. train loss: 0.51687, avg. val loss: 0.57393\n",
      "Step #750, epoch #8, avg. train loss: 0.51722, avg. val loss: 0.58200\n",
      "Step #800, epoch #9, avg. train loss: 0.48581, avg. val loss: 0.56962\n",
      "Step #850, epoch #9, avg. train loss: 0.49913, avg. val loss: 0.58082\n",
      "Step #900, epoch #10, avg. train loss: 0.49679, avg. val loss: 0.56997\n",
      "Step #950, epoch #10, avg. train loss: 0.47982, avg. val loss: 0.56918\n",
      "Step #1000, epoch #11, avg. train loss: 0.49133, avg. val loss: 0.57388\n",
      "Step #1050, epoch #11, avg. train loss: 0.46980, avg. val loss: 0.58057\n",
      "Step #1100, epoch #12, avg. train loss: 0.45433, avg. val loss: 0.58381\n",
      "Step #1150, epoch #13, avg. train loss: 0.45127, avg. val loss: 0.58846\n",
      "Step #1200, epoch #13, avg. train loss: 0.47991, avg. val loss: 0.57903\n",
      "Step #1250, epoch #14, avg. train loss: 0.46112, avg. val loss: 0.57883\n",
      "Step #1300, epoch #14, avg. train loss: 0.44343, avg. val loss: 0.58056\n",
      "Step #1350, epoch #15, avg. train loss: 0.42739, avg. val loss: 0.58738\n",
      "Step #1400, epoch #15, avg. train loss: 0.47484, avg. val loss: 0.57388\n",
      "Step #1450, epoch #16, avg. train loss: 0.42763, avg. val loss: 0.58445\n",
      "Step #1500, epoch #17, avg. train loss: 0.44327, avg. val loss: 0.57376\n",
      "Step #1550, epoch #17, avg. train loss: 0.40853, avg. val loss: 0.59978\n",
      "Step #1600, epoch #18, avg. train loss: 0.41910, avg. val loss: 0.58004\n",
      "Step #1650, epoch #18, avg. train loss: 0.40590, avg. val loss: 0.61385\n",
      "Step #1700, epoch #19, avg. train loss: 0.42820, avg. val loss: 0.60594\n",
      "Step #1750, epoch #19, avg. train loss: 0.41307, avg. val loss: 0.58408\n",
      "Step #1800, epoch #20, avg. train loss: 0.37722, avg. val loss: 0.61067\n",
      "Step #1850, epoch #21, avg. train loss: 0.39656, avg. val loss: 0.62544\n",
      "Step #1900, epoch #21, avg. train loss: 0.38560, avg. val loss: 0.61233\n",
      "Step #1950, epoch #22, avg. train loss: 0.38652, avg. val loss: 0.63749\n",
      "Step #2000, epoch #22, avg. train loss: 0.40150, avg. val loss: 0.59898\n",
      "Step #2050, epoch #23, avg. train loss: 0.39894, avg. val loss: 0.60660\n",
      "Step #2100, epoch #23, avg. train loss: 0.36725, avg. val loss: 0.63532\n",
      "Step #2150, epoch #24, avg. train loss: 0.36846, avg. val loss: 0.63694\n",
      "Step #2200, epoch #25, avg. train loss: 0.39123, avg. val loss: 0.63728\n",
      "Step #2250, epoch #25, avg. train loss: 0.33432, avg. val loss: 0.65220\n",
      "Step #2300, epoch #26, avg. train loss: 0.37439, avg. val loss: 0.63239\n",
      "Step #2350, epoch #26, avg. train loss: 0.37683, avg. val loss: 0.66446\n",
      "Step #2400, epoch #27, avg. train loss: 0.36200, avg. val loss: 0.63233\n",
      "Step #2450, epoch #27, avg. train loss: 0.35016, avg. val loss: 0.67892\n",
      "Step #2500, epoch #28, avg. train loss: 0.34503, avg. val loss: 0.67347\n",
      "Step #2550, epoch #28, avg. train loss: 0.36795, avg. val loss: 0.67297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping. Best step:\n",
      " step 1572 with loss 0.5185275077819824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation logloss: 0.5593276151913225\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import tensorflow.contrib.learn as skflow\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "def dropout_model_classification(X, y):\n",
    "    \"\"\"This is DNN with 10, 20, 10 hidden layers, and dropout of 0.5 probability.\"\"\"\n",
    "    layers = skflow.ops.dnn(X, [500, 250, 100, 50], dropout=0.5)\n",
    "    return skflow.models.logistic_regression(layers, y)\n",
    "\n",
    "# Encode feature vector\n",
    "x, y = to_xy(df_train,'Activity')\n",
    "x_submit = df_test.as_matrix().astype(np.float32)\n",
    "num_classes = 2\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=42) \n",
    "\n",
    "# Create a deep neural network with 3 hidden layers of 10, 20, 10\n",
    "classifier = skflow.TensorFlowEstimator(model_fn=dropout_model_classification, \n",
    "                                        n_classes=num_classes, steps=10000)\n",
    "\n",
    "# Early stopping\n",
    "early_stop = skflow.monitors.ValidationMonitor(x_test, y_test,\n",
    "    early_stopping_rounds=1000, print_steps=50, n_classes=num_classes)\n",
    "    \n",
    "# Fit/train neural network\n",
    "classifier.fit(x_train, y_train, monitor=early_stop)\n",
    "\n",
    "\n",
    "pred = classifier.predict_proba(x_test)\n",
    "pred = pred[:,1]\n",
    "print(\"Validation logloss: {}\".format(sklearn.metrics.log_loss(y_test,pred)))\n",
    "\n",
    "\n",
    "pred_submit = classifier.predict_proba(x_submit)\n",
    "pred_submit = pred_submit[:,1]\n",
    "submit_df = pd.DataFrame({'MoleculeId':[x+1 for x in range(len(pred_submit))],'PredictedProbability':pred_submit})\n",
    "submit_df.to_csv(filename_submit, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:38: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>D26</td>\n",
       "      <td>0.632833</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>D50</td>\n",
       "      <td>0.570915</td>\n",
       "      <td>0.902158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>D960</td>\n",
       "      <td>0.567925</td>\n",
       "      <td>0.897433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>D1110</td>\n",
       "      <td>0.566142</td>\n",
       "      <td>0.894616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>D193</td>\n",
       "      <td>0.565461</td>\n",
       "      <td>0.893539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>D1021</td>\n",
       "      <td>0.565296</td>\n",
       "      <td>0.893279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>D200</td>\n",
       "      <td>0.565017</td>\n",
       "      <td>0.892838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>D1092</td>\n",
       "      <td>0.564899</td>\n",
       "      <td>0.892650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>D1158</td>\n",
       "      <td>0.564392</td>\n",
       "      <td>0.891850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>D997</td>\n",
       "      <td>0.564098</td>\n",
       "      <td>0.891385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>D1157</td>\n",
       "      <td>0.564027</td>\n",
       "      <td>0.891273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>D1070</td>\n",
       "      <td>0.563420</td>\n",
       "      <td>0.890314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>D1009</td>\n",
       "      <td>0.563344</td>\n",
       "      <td>0.890195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>D1005</td>\n",
       "      <td>0.563220</td>\n",
       "      <td>0.889998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>D1152</td>\n",
       "      <td>0.563152</td>\n",
       "      <td>0.889890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>D1007</td>\n",
       "      <td>0.563127</td>\n",
       "      <td>0.889850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>D958</td>\n",
       "      <td>0.563089</td>\n",
       "      <td>0.889791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>D1035</td>\n",
       "      <td>0.563022</td>\n",
       "      <td>0.889686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>D1036</td>\n",
       "      <td>0.562977</td>\n",
       "      <td>0.889614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>D1193</td>\n",
       "      <td>0.562974</td>\n",
       "      <td>0.889610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>D1401</td>\n",
       "      <td>0.562922</td>\n",
       "      <td>0.889526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>D1187</td>\n",
       "      <td>0.562847</td>\n",
       "      <td>0.889409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>D991</td>\n",
       "      <td>0.562700</td>\n",
       "      <td>0.889176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>D1115</td>\n",
       "      <td>0.562697</td>\n",
       "      <td>0.889171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>D1058</td>\n",
       "      <td>0.562655</td>\n",
       "      <td>0.889105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>D995</td>\n",
       "      <td>0.562640</td>\n",
       "      <td>0.889081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>D1173</td>\n",
       "      <td>0.562466</td>\n",
       "      <td>0.888807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>D1202</td>\n",
       "      <td>0.562286</td>\n",
       "      <td>0.888522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>D1417</td>\n",
       "      <td>0.562166</td>\n",
       "      <td>0.888333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>D1012</td>\n",
       "      <td>0.562104</td>\n",
       "      <td>0.888235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>D1026</td>\n",
       "      <td>0.556973</td>\n",
       "      <td>0.880127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>D1262</td>\n",
       "      <td>0.556972</td>\n",
       "      <td>0.880125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>D1441</td>\n",
       "      <td>0.556944</td>\n",
       "      <td>0.880081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>D1060</td>\n",
       "      <td>0.556924</td>\n",
       "      <td>0.880049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>D992</td>\n",
       "      <td>0.556912</td>\n",
       "      <td>0.880029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>D1001</td>\n",
       "      <td>0.556897</td>\n",
       "      <td>0.880007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>D978</td>\n",
       "      <td>0.556895</td>\n",
       "      <td>0.880004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>D1063</td>\n",
       "      <td>0.556852</td>\n",
       "      <td>0.879935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>D1031</td>\n",
       "      <td>0.556823</td>\n",
       "      <td>0.879890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>D1059</td>\n",
       "      <td>0.556744</td>\n",
       "      <td>0.879765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>D1151</td>\n",
       "      <td>0.556735</td>\n",
       "      <td>0.879751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>D1285</td>\n",
       "      <td>0.556734</td>\n",
       "      <td>0.879749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>D1293</td>\n",
       "      <td>0.556585</td>\n",
       "      <td>0.879513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>D1198</td>\n",
       "      <td>0.556501</td>\n",
       "      <td>0.879381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>D1050</td>\n",
       "      <td>0.556484</td>\n",
       "      <td>0.879354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>D1332</td>\n",
       "      <td>0.556280</td>\n",
       "      <td>0.879032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>D1199</td>\n",
       "      <td>0.556229</td>\n",
       "      <td>0.878951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>D1106</td>\n",
       "      <td>0.556221</td>\n",
       "      <td>0.878939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>D1284</td>\n",
       "      <td>0.556153</td>\n",
       "      <td>0.878830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>D1143</td>\n",
       "      <td>0.556152</td>\n",
       "      <td>0.878828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>D999</td>\n",
       "      <td>0.556015</td>\n",
       "      <td>0.878612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>D1304</td>\n",
       "      <td>0.555886</td>\n",
       "      <td>0.878408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>D1406</td>\n",
       "      <td>0.555884</td>\n",
       "      <td>0.878406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>D1433</td>\n",
       "      <td>0.555882</td>\n",
       "      <td>0.878402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>D1011</td>\n",
       "      <td>0.555851</td>\n",
       "      <td>0.878353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>D985</td>\n",
       "      <td>0.555311</td>\n",
       "      <td>0.877500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>D1095</td>\n",
       "      <td>0.555257</td>\n",
       "      <td>0.877415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>D1038</td>\n",
       "      <td>0.554892</td>\n",
       "      <td>0.876838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>D1337</td>\n",
       "      <td>0.553926</td>\n",
       "      <td>0.875311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>D1068</td>\n",
       "      <td>0.553815</td>\n",
       "      <td>0.875136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1776 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       name     error  importance\n",
       "26      D26  0.632833    1.000000\n",
       "50      D50  0.570915    0.902158\n",
       "960    D960  0.567925    0.897433\n",
       "1110  D1110  0.566142    0.894616\n",
       "193    D193  0.565461    0.893539\n",
       "1021  D1021  0.565296    0.893279\n",
       "200    D200  0.565017    0.892838\n",
       "1092  D1092  0.564899    0.892650\n",
       "1158  D1158  0.564392    0.891850\n",
       "997    D997  0.564098    0.891385\n",
       "1157  D1157  0.564027    0.891273\n",
       "1070  D1070  0.563420    0.890314\n",
       "1009  D1009  0.563344    0.890195\n",
       "1005  D1005  0.563220    0.889998\n",
       "1152  D1152  0.563152    0.889890\n",
       "1007  D1007  0.563127    0.889850\n",
       "958    D958  0.563089    0.889791\n",
       "1035  D1035  0.563022    0.889686\n",
       "1036  D1036  0.562977    0.889614\n",
       "1193  D1193  0.562974    0.889610\n",
       "1401  D1401  0.562922    0.889526\n",
       "1187  D1187  0.562847    0.889409\n",
       "991    D991  0.562700    0.889176\n",
       "1115  D1115  0.562697    0.889171\n",
       "1058  D1058  0.562655    0.889105\n",
       "995    D995  0.562640    0.889081\n",
       "1173  D1173  0.562466    0.888807\n",
       "1202  D1202  0.562286    0.888522\n",
       "1417  D1417  0.562166    0.888333\n",
       "1012  D1012  0.562104    0.888235\n",
       "...     ...       ...         ...\n",
       "1026  D1026  0.556973    0.880127\n",
       "1262  D1262  0.556972    0.880125\n",
       "1441  D1441  0.556944    0.880081\n",
       "1060  D1060  0.556924    0.880049\n",
       "992    D992  0.556912    0.880029\n",
       "1001  D1001  0.556897    0.880007\n",
       "978    D978  0.556895    0.880004\n",
       "1063  D1063  0.556852    0.879935\n",
       "1031  D1031  0.556823    0.879890\n",
       "1059  D1059  0.556744    0.879765\n",
       "1151  D1151  0.556735    0.879751\n",
       "1285  D1285  0.556734    0.879749\n",
       "1293  D1293  0.556585    0.879513\n",
       "1198  D1198  0.556501    0.879381\n",
       "1050  D1050  0.556484    0.879354\n",
       "1332  D1332  0.556280    0.879032\n",
       "1199  D1199  0.556229    0.878951\n",
       "1106  D1106  0.556221    0.878939\n",
       "1284  D1284  0.556153    0.878830\n",
       "1143  D1143  0.556152    0.878828\n",
       "999    D999  0.556015    0.878612\n",
       "1304  D1304  0.555886    0.878408\n",
       "1406  D1406  0.555884    0.878406\n",
       "1433  D1433  0.555882    0.878402\n",
       "1011  D1011  0.555851    0.878353\n",
       "985    D985  0.555311    0.877500\n",
       "1095  D1095  0.555257    0.877415\n",
       "1038  D1038  0.554892    0.876838\n",
       "1337  D1337  0.553926    0.875311\n",
       "1068  D1068  0.553815    0.875136\n",
       "\n",
       "[1776 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rank the features\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = df_train.columns.values[0:-1] # x column names\n",
    "rank = perturbation_rank(classifier, x_test, y_test, names, False)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biological Response with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:9: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insample logloss: 0.1255907712846015\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn\n",
    "\n",
    "\n",
    "x, y = to_xy(df_train,'Activity')\n",
    "x_test = df_test.as_matrix().astype(np.float32)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(x, y)\n",
    "pred = rf.predict_proba(x_test)\n",
    "pred = pred[:,1]\n",
    "pred_insample = rf.predict_proba(x)\n",
    "pred_insample = pred_insample[:,1]\n",
    "\n",
    "submit_df = pd.DataFrame({'MoleculeId':[x+1 for x in range(len(pred))],'PredictedProbability':pred})\n",
    "submit_df.to_csv(filename_submit, index=False)\n",
    "print(\"Insample logloss: {}\".format(sklearn.metrics.log_loss(y,pred_insample)))\n",
    "#display(submit_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Bagging\n",
    "\n",
    "Neural networks will typically achieve better results when they are bagged.  Bagging a neural network is a process where the same neural network is trained over and over and the results are averaged together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow.contrib.learn as skflow\n",
    "\n",
    "PATH = \"./data/\"\n",
    "SHUFFLE = False\n",
    "FOLDS = 10\n",
    "\n",
    "def mlogloss(y_test, preds):\n",
    "    epsilon = 1e-15\n",
    "    sum = 0\n",
    "    for row in zip(preds,y_test):\n",
    "        x = row[0][row[1]]\n",
    "        x = max(epsilon,x)\n",
    "        x = min(1-epsilon,x)\n",
    "        sum+=math.log(x)\n",
    "    return( (-1/len(preds))*sum)\n",
    "\n",
    "def stretch(y):\n",
    "    return (y - y.min()) / (y.max() - y.min())\n",
    "\n",
    "\n",
    "def blend_ensemble(x, y, x_submit):\n",
    "\n",
    "    folds = list(StratifiedKFold(y, FOLDS))\n",
    "\n",
    "    models = [\n",
    "        skflow.TensorFlowDNNClassifier(hidden_units=[100, 50, 25, 5], n_classes=2, steps=1000),\n",
    "        skflow.TensorFlowDNNClassifier(hidden_units=[100, 50, 25, 5], n_classes=2, steps=500),\n",
    "        skflow.TensorFlowDNNClassifier(hidden_units=[200, 100, 50, 25], n_classes=2, steps=1000),\n",
    "        skflow.TensorFlowDNNClassifier(hidden_units=[200, 100, 50, 25], n_classes=2, steps=500),\n",
    "        skflow.TensorFlowDNNClassifier(hidden_units=[50, 25, 5], n_classes=2, steps=500)]\n",
    "\n",
    "    dataset_blend_train = np.zeros((x.shape[0], len(models)))\n",
    "    dataset_blend_test = np.zeros((x_submit.shape[0], len(models)))\n",
    "\n",
    "    for j, model in enumerate(models):\n",
    "        print(\"Model: {} : {}\".format(j, model) )\n",
    "        fold_sums = np.zeros((x_submit.shape[0], len(folds)))\n",
    "        total_loss = 0\n",
    "        for i, (train, test) in enumerate(folds):\n",
    "            x_train = x[train]\n",
    "            y_train = y[train]\n",
    "            x_test = x[test]\n",
    "            y_test = y[test]\n",
    "            model.fit(x_train, y_train)\n",
    "            pred = model.predict_proba(x_test)\n",
    "            dataset_blend_train[test, j] = pred[:, 1]\n",
    "            fold_sums[:, i] = model.predict_proba(x_submit)[:, 1]\n",
    "            loss = mlogloss(y_test, pred)\n",
    "            total_loss+=loss\n",
    "            print(\"Fold #{}: loss={}\".format(i,loss))\n",
    "        print(\"{}: Mean loss={}\".format(model.__class__.__name__,total_loss/len(folds)))\n",
    "        dataset_blend_test[:, j] = fold_sums.mean(1)\n",
    "\n",
    "    print()\n",
    "    print(\"Blending models.\")\n",
    "    blend = LogisticRegression()\n",
    "    blend.fit(dataset_blend_train, y)\n",
    "    return blend.predict_proba(dataset_blend_test)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    np.random.seed(42)  # seed to shuffle the train set\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    filename_train = os.path.join(PATH, \"bio_train.csv\")\n",
    "    df_train = pd.read_csv(filename_train, na_values=['NA', '?'])\n",
    "\n",
    "    filename_submit = os.path.join(PATH, \"bio_test.csv\")\n",
    "    df_submit = pd.read_csv(filename_submit, na_values=['NA', '?'])\n",
    "\n",
    "    predictors = list(df_train.columns.values)\n",
    "    predictors.remove('Activity')\n",
    "    x = df_train.as_matrix(predictors)\n",
    "    y = df_train['Activity']\n",
    "    x_submit = df_submit.as_matrix()\n",
    "\n",
    "    if SHUFFLE:\n",
    "        idx = np.random.permutation(y.size)\n",
    "        x = x[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    submit_data = blend_ensemble(x, y, x_submit)\n",
    "    submit_data = stretch(submit_data)\n",
    "\n",
    "    ####################\n",
    "    # Build submit file\n",
    "    ####################\n",
    "    ids = [id+1 for id in range(submit_data.shape[0])]\n",
    "    submit_filename = os.path.join(PATH, \"bio_submit.csv\")\n",
    "    submit_df = pd.DataFrame({'MoleculeId': ids, 'PredictedProbability': submit_data[:, 1]},\n",
    "                             columns=['MoleculeId','PredictedProbability'])\n",
    "    submit_df.to_csv(submit_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Ensemble\n",
    "\n",
    "A neural network ensemble combines neural network predictions with other models.  The exact blend of all of these models is determined by logistic regression.  The following code performs this blend for a classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Model: 0 : TensorFlowDNNClassifier(batch_size=32, class_weight=None, clip_gradients=5.0,\n",
      "            config=None, continue_training=False, dropout=None,\n",
      "            hidden_units=[100, 50, 25, 5], learning_rate=0.1, n_classes=2,\n",
      "            optimizer='Adagrad', steps=1000, verbose=1)\n",
      "Step #99, avg. train loss: 0.66611\n",
      "Step #200, epoch #1, avg. train loss: 0.56297\n",
      "Step #300, epoch #2, avg. train loss: 0.52502\n",
      "Step #400, epoch #3, avg. train loss: 0.47896\n",
      "Step #500, epoch #4, avg. train loss: 0.45476\n",
      "Step #600, epoch #5, avg. train loss: 0.43957\n",
      "Step #700, epoch #6, avg. train loss: 0.41687\n",
      "Step #800, epoch #7, avg. train loss: 0.39842\n",
      "Step #900, epoch #8, avg. train loss: 0.36307\n",
      "Step #1000, epoch #9, avg. train loss: 0.37075\n",
      "Fold #0: loss=0.511710197288865\n",
      "Step #99, avg. train loss: 0.64651\n",
      "Step #200, epoch #1, avg. train loss: 0.56006\n",
      "Step #300, epoch #2, avg. train loss: 0.51776\n",
      "Step #400, epoch #3, avg. train loss: 0.48333\n",
      "Step #500, epoch #4, avg. train loss: 0.45770\n",
      "Step #600, epoch #5, avg. train loss: 0.43619\n",
      "Step #700, epoch #6, avg. train loss: 0.41256\n",
      "Step #800, epoch #7, avg. train loss: 0.40976\n",
      "Step #900, epoch #8, avg. train loss: 0.38654\n",
      "Step #1000, epoch #9, avg. train loss: 0.38070\n",
      "Fold #1: loss=0.4529770292414818\n",
      "Step #99, avg. train loss: 0.64191\n",
      "Step #200, epoch #1, avg. train loss: 0.55085\n",
      "Step #300, epoch #2, avg. train loss: 0.49990\n",
      "Step #400, epoch #3, avg. train loss: 0.48252\n",
      "Step #500, epoch #4, avg. train loss: 0.45656\n",
      "Step #600, epoch #5, avg. train loss: 0.43025\n",
      "Step #700, epoch #6, avg. train loss: 0.41194\n",
      "Step #800, epoch #7, avg. train loss: 0.39984\n",
      "Step #900, epoch #8, avg. train loss: 0.37599\n",
      "Step #1000, epoch #9, avg. train loss: 0.36624\n",
      "Fold #2: loss=0.5703335135416453\n",
      "Step #99, avg. train loss: 0.65627\n",
      "Step #200, epoch #1, avg. train loss: 0.56519\n",
      "Step #300, epoch #2, avg. train loss: 0.51699\n",
      "Step #400, epoch #3, avg. train loss: 0.48890\n",
      "Step #500, epoch #4, avg. train loss: 0.46332\n",
      "Step #600, epoch #5, avg. train loss: 0.44273\n",
      "Step #700, epoch #6, avg. train loss: 0.43146\n",
      "Step #800, epoch #7, avg. train loss: 0.40715\n",
      "Step #900, epoch #8, avg. train loss: 0.37742\n",
      "Step #1000, epoch #9, avg. train loss: 0.36323\n",
      "Fold #3: loss=0.47423787556265884\n",
      "Step #99, avg. train loss: 0.64852\n",
      "Step #200, epoch #1, avg. train loss: 0.55008\n",
      "Step #300, epoch #2, avg. train loss: 0.51285\n",
      "Step #400, epoch #3, avg. train loss: 0.46731\n",
      "Step #500, epoch #4, avg. train loss: 0.45884\n",
      "Step #600, epoch #5, avg. train loss: 0.43326\n",
      "Step #700, epoch #6, avg. train loss: 0.42539\n",
      "Step #800, epoch #7, avg. train loss: 0.39198\n",
      "Step #900, epoch #8, avg. train loss: 0.38467\n",
      "Step #1000, epoch #9, avg. train loss: 0.35605\n",
      "Fold #4: loss=0.529972735911898\n",
      "Step #99, avg. train loss: 0.65988\n",
      "Step #200, epoch #1, avg. train loss: 0.56384\n",
      "Step #300, epoch #2, avg. train loss: 0.51065\n",
      "Step #400, epoch #3, avg. train loss: 0.46858\n",
      "Step #500, epoch #4, avg. train loss: 0.44578\n",
      "Step #600, epoch #5, avg. train loss: 0.43700\n",
      "Step #700, epoch #6, avg. train loss: 0.41821\n",
      "Step #800, epoch #7, avg. train loss: 0.39525\n",
      "Step #900, epoch #8, avg. train loss: 0.37196\n",
      "Step #1000, epoch #9, avg. train loss: 0.35992\n",
      "Fold #5: loss=0.6099523198018239\n",
      "Step #99, avg. train loss: 0.65619\n",
      "Step #200, epoch #1, avg. train loss: 0.56114\n",
      "Step #300, epoch #2, avg. train loss: 0.52046\n",
      "Step #400, epoch #3, avg. train loss: 0.47379\n",
      "Step #500, epoch #4, avg. train loss: 0.45870\n",
      "Step #600, epoch #5, avg. train loss: 0.45057\n",
      "Step #700, epoch #6, avg. train loss: 0.42079\n",
      "Step #800, epoch #7, avg. train loss: 0.40659\n",
      "Step #900, epoch #8, avg. train loss: 0.38330\n",
      "Step #1000, epoch #9, avg. train loss: 0.36381\n",
      "Fold #6: loss=0.4716374990738397\n",
      "Step #99, avg. train loss: 0.64352\n",
      "Step #200, epoch #1, avg. train loss: 0.54955\n",
      "Step #300, epoch #2, avg. train loss: 0.50862\n",
      "Step #400, epoch #3, avg. train loss: 0.46483\n",
      "Step #500, epoch #4, avg. train loss: 0.44949\n",
      "Step #600, epoch #5, avg. train loss: 0.45279\n",
      "Step #700, epoch #6, avg. train loss: 0.40592\n",
      "Step #800, epoch #7, avg. train loss: 0.40375\n",
      "Step #900, epoch #8, avg. train loss: 0.38168\n",
      "Step #1000, epoch #9, avg. train loss: 0.37432\n",
      "Fold #7: loss=0.533348825125513\n",
      "Step #99, avg. train loss: 0.65841\n",
      "Step #200, epoch #1, avg. train loss: 0.54818\n",
      "Step #300, epoch #2, avg. train loss: 0.50682\n",
      "Step #400, epoch #3, avg. train loss: 0.46721\n",
      "Step #500, epoch #4, avg. train loss: 0.45458\n",
      "Step #600, epoch #5, avg. train loss: 0.45240\n",
      "Step #700, epoch #6, avg. train loss: 0.41547\n",
      "Step #800, epoch #7, avg. train loss: 0.41160\n",
      "Step #900, epoch #8, avg. train loss: 0.37235\n",
      "Step #1000, epoch #9, avg. train loss: 0.37450\n",
      "Fold #8: loss=0.4690331759426469\n",
      "Step #99, avg. train loss: 0.64664\n",
      "Step #200, epoch #1, avg. train loss: 0.56628\n",
      "Step #300, epoch #2, avg. train loss: 0.52381\n",
      "Step #400, epoch #3, avg. train loss: 0.47079\n",
      "Step #500, epoch #4, avg. train loss: 0.45238\n",
      "Step #600, epoch #5, avg. train loss: 0.45895\n",
      "Step #700, epoch #6, avg. train loss: 0.42676\n",
      "Step #800, epoch #7, avg. train loss: 0.41127\n",
      "Step #900, epoch #8, avg. train loss: 0.37519\n",
      "Step #1000, epoch #9, avg. train loss: 0.37630\n",
      "Fold #9: loss=0.6143471605311859\n",
      "TensorFlowDNNClassifier: Mean loss=0.5237550332021559\n",
      "Model: 1 : KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "Fold #0: loss=3.606678388314123\n",
      "Fold #1: loss=2.2197228940978317\n",
      "Fold #2: loss=3.6717523663107237\n",
      "Fold #3: loss=2.5045156203944594\n",
      "Fold #4: loss=4.443553550438037\n",
      "Fold #5: loss=4.410524301688227\n",
      "Fold #6: loss=3.400455469543658\n",
      "Fold #7: loss=3.0885474338547683\n",
      "Fold #8: loss=2.1219335323249253\n",
      "Fold #9: loss=3.0613772690497245\n",
      "KNeighborsClassifier: Mean loss=3.2529060826016476\n",
      "Model: 2 : RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fold #0: loss=0.4656951886598634\n",
      "Fold #1: loss=0.42616738372828994\n",
      "Fold #2: loss=0.5553310672072804\n",
      "Fold #3: loss=0.4158882703480888\n",
      "Fold #4: loss=0.47961750744493736\n",
      "Fold #5: loss=0.4818691257035154\n",
      "Fold #6: loss=0.4077640145387344\n",
      "Fold #7: loss=0.4765886929423487\n",
      "Fold #8: loss=0.4517642970846355\n",
      "Fold #9: loss=0.4680612294852455\n",
      "RandomForestClassifier: Mean loss=0.46287467771429397\n",
      "Model: 3 : RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fold #0: loss=0.4517180829333204\n",
      "Fold #1: loss=0.4288138331194263\n",
      "Fold #2: loss=0.48381057653259435\n",
      "Fold #3: loss=0.4182666851649727\n",
      "Fold #4: loss=0.48153896166957655\n",
      "Fold #5: loss=0.4980753099535686\n",
      "Fold #6: loss=0.40137715472145985\n",
      "Fold #7: loss=0.45485344988166004\n",
      "Fold #8: loss=0.4468679233769468\n",
      "Fold #9: loss=0.46952543963188276\n",
      "RandomForestClassifier: Mean loss=0.4534847416985408\n",
      "Model: 4 : ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Fold #0: loss=0.45496751079363495\n",
      "Fold #1: loss=0.4221988146584036\n",
      "Fold #2: loss=0.5864779075767735\n",
      "Fold #3: loss=0.4135387147555288\n",
      "Fold #4: loss=0.49165543640777337\n",
      "Fold #5: loss=0.5826853127129209\n",
      "Fold #6: loss=0.4992293347545809\n",
      "Fold #7: loss=0.5696776278324083\n",
      "Fold #8: loss=0.5377009614825886\n",
      "Fold #9: loss=0.6233110429105153\n",
      "ExtraTreesClassifier: Mean loss=0.5181442663885129\n",
      "Model: 5 : ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Fold #0: loss=0.44825346440152214\n",
      "Fold #1: loss=0.40736044850901787\n",
      "Fold #2: loss=0.5775460796610716\n",
      "Fold #3: loss=0.42126274280011555\n",
      "Fold #4: loss=0.4949968282719872\n",
      "Fold #5: loss=0.5074636989761913\n",
      "Fold #6: loss=0.4176899941799664\n",
      "Fold #7: loss=0.6431685288404098\n",
      "Fold #8: loss=0.5444145998368657\n",
      "Fold #9: loss=0.46770812815232515\n",
      "ExtraTreesClassifier: Mean loss=0.4929864513629473\n",
      "Model: 6 : GradientBoostingClassifier(init=None, learning_rate=0.05, loss='deviance',\n",
      "              max_depth=6, max_features=None, max_leaf_nodes=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "              presort='auto', random_state=None, subsample=0.5, verbose=0,\n",
      "              warm_start=False)\n",
      "Fold #0: loss=0.4821080713706618\n",
      "Fold #1: loss=0.4637928880214942\n",
      "Fold #2: loss=0.48596775087640226\n",
      "Fold #3: loss=0.4440448889244344\n",
      "Fold #4: loss=0.49922732908471634\n",
      "Fold #5: loss=0.4919547963242652\n",
      "Fold #6: loss=0.44912745206339083\n",
      "Fold #7: loss=0.465104570795673\n",
      "Fold #8: loss=0.4528440085905339\n",
      "Fold #9: loss=0.4699129824000123\n",
      "GradientBoostingClassifier: Mean loss=0.47040847384515844\n",
      "\n",
      "Blending models.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow.contrib.learn as skflow\n",
    "\n",
    "PATH = \"./data/\"\n",
    "SHUFFLE = False\n",
    "FOLDS = 10\n",
    "\n",
    "def mlogloss(y_test, preds):\n",
    "    epsilon = 1e-15\n",
    "    sum = 0\n",
    "    for row in zip(preds,y_test):\n",
    "        x = row[0][row[1]]\n",
    "        x = max(epsilon,x)\n",
    "        x = min(1-epsilon,x)\n",
    "        sum+=math.log(x)\n",
    "    return( (-1/len(preds))*sum)\n",
    "\n",
    "def stretch(y):\n",
    "    return (y - y.min()) / (y.max() - y.min())\n",
    "\n",
    "\n",
    "def blend_ensemble(x, y, x_submit):\n",
    "\n",
    "    folds = list(StratifiedKFold(y, FOLDS))\n",
    "\n",
    "    models = [\n",
    "        skflow.TensorFlowDNNClassifier(hidden_units=[100, 50, 25, 5], n_classes=2, steps=1000),\n",
    "        KNeighborsClassifier(n_neighbors=3),\n",
    "        RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "        RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50)]\n",
    "\n",
    "    dataset_blend_train = np.zeros((x.shape[0], len(models)))\n",
    "    dataset_blend_test = np.zeros((x_submit.shape[0], len(models)))\n",
    "\n",
    "    for j, model in enumerate(models):\n",
    "        print(\"Model: {} : {}\".format(j, model) )\n",
    "        fold_sums = np.zeros((x_submit.shape[0], len(folds)))\n",
    "        total_loss = 0\n",
    "        for i, (train, test) in enumerate(folds):\n",
    "            x_train = x[train]\n",
    "            y_train = y[train]\n",
    "            x_test = x[test]\n",
    "            y_test = y[test]\n",
    "            model.fit(x_train, y_train)\n",
    "            pred = model.predict_proba(x_test)\n",
    "            dataset_blend_train[test, j] = pred[:, 1]\n",
    "            fold_sums[:, i] = model.predict_proba(x_submit)[:, 1]\n",
    "            loss = mlogloss(y_test, pred)\n",
    "            total_loss+=loss\n",
    "            print(\"Fold #{}: loss={}\".format(i,loss))\n",
    "        print(\"{}: Mean loss={}\".format(model.__class__.__name__,total_loss/len(folds)))\n",
    "        dataset_blend_test[:, j] = fold_sums.mean(1)\n",
    "\n",
    "    print()\n",
    "    print(\"Blending models.\")\n",
    "    blend = LogisticRegression()\n",
    "    blend.fit(dataset_blend_train, y)\n",
    "    return blend.predict_proba(dataset_blend_test)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    np.random.seed(42)  # seed to shuffle the train set\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    filename_train = os.path.join(PATH, \"bio_train.csv\")\n",
    "    df_train = pd.read_csv(filename_train, na_values=['NA', '?'])\n",
    "\n",
    "    filename_submit = os.path.join(PATH, \"bio_test.csv\")\n",
    "    df_submit = pd.read_csv(filename_submit, na_values=['NA', '?'])\n",
    "\n",
    "    predictors = list(df_train.columns.values)\n",
    "    predictors.remove('Activity')\n",
    "    x = df_train.as_matrix(predictors)\n",
    "    y = df_train['Activity']\n",
    "    x_submit = df_submit.as_matrix()\n",
    "\n",
    "    if SHUFFLE:\n",
    "        idx = np.random.permutation(y.size)\n",
    "        x = x[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    submit_data = blend_ensemble(x, y, x_submit)\n",
    "    submit_data = stretch(submit_data)\n",
    "\n",
    "    ####################\n",
    "    # Build submit file\n",
    "    ####################\n",
    "    ids = [id+1 for id in range(submit_data.shape[0])]\n",
    "    submit_filename = os.path.join(PATH, \"bio_submit.csv\")\n",
    "    submit_df = pd.DataFrame({'MoleculeId': ids, 'PredictedProbability': submit_data[:, 1]},\n",
    "                             columns=['MoleculeId','PredictedProbability'])\n",
    "    submit_df.to_csv(submit_filename, index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
