{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Class 12: Deep Learning Applications**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tonight we will see how to apply deep learning networks to data science.  There are many applications of deep learning.  However, we will focus primarily upon data science.  For this class we will go beyond simple academic examples and see how to construct an ensemble that could potentially lead to a high score on a Kaggle competition.  We will see how to evaluate the importance of features and several ways to combine models.\n",
    "\n",
    "Tonights topics include:\n",
    "\n",
    "* Log Loss Error\n",
    "* Evaluating Feature Importance\n",
    "* The Biological Response Data Set\n",
    "* Neural Network Bagging\n",
    "* Nueral Network Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions\n",
    "\n",
    "These are exactly the same feature vector encoding functions from [Class 3](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class3_training.ipynb).  They must be defined for this class as well.  For more information, refer to class 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogLoss Error\n",
    "\n",
    "Log loss is an error metric that is often used in place of accuracy for classification.  Log loss allows for \"partial credit\" when a miss classification occurs.  For example, a model might be used to classify A, B and C.  The correct answer might be A, however if the classification network chose B as having the highest probability, then accuracy gives the neural network no credit for this classification.  \n",
    "\n",
    "However, with log loss, the probability of the correct answer is added to the score.  For example, the correct answer might be A, but if the neural network only predicted .8 probability of A being correct, then the value -log(.8) is added.\n",
    "\n",
    "$$ logloss = -\\frac{1}{N}\\sum^N_{i=1}\\sum^M_{j=1}y_{ij} \\log(\\hat{y}_{ij}) $$\n",
    "\n",
    "The following table shows the logloss scores that correspond to the average predicted accuracy for the correct item. The **pred** column specifies the average probability for the correct class.  The **logloss** column specifies the log loss for that probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>logloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.000000e-01</td>\n",
       "      <td>0.105361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.000000e-01</td>\n",
       "      <td>0.223144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.000000e-01</td>\n",
       "      <td>0.356675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.000000e-01</td>\n",
       "      <td>0.510826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.000000e-01</td>\n",
       "      <td>0.916291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.000000e-01</td>\n",
       "      <td>1.203973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>1.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>2.302585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.500000e-02</td>\n",
       "      <td>2.590267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.000000e-02</td>\n",
       "      <td>2.995732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.500000e-02</td>\n",
       "      <td>3.688879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>18.420681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pred    logloss\n",
       "0   1.000000e+00  -0.000000\n",
       "1   9.000000e-01   0.105361\n",
       "2   8.000000e-01   0.223144\n",
       "3   7.000000e-01   0.356675\n",
       "4   6.000000e-01   0.510826\n",
       "5   5.000000e-01   0.693147\n",
       "6   4.000000e-01   0.916291\n",
       "7   3.000000e-01   1.203973\n",
       "8   2.000000e-01   1.609438\n",
       "9   1.000000e-01   2.302585\n",
       "10  7.500000e-02   2.590267\n",
       "11  5.000000e-02   2.995732\n",
       "12  2.500000e-02   3.688879\n",
       "13  1.000000e-08  18.420681"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "loss = [1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.075, 0.05, 0.025, 1e-8 ]\n",
    "\n",
    "df = pd.DataFrame({'pred':loss, 'logloss': -np.log(loss)},columns=['pred','logloss'])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows the opposit.  For a given logloss, what is the average probability for the correct class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logloss</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.904837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.818731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.740818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.670320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.606531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.548812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.496585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.449329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.406570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.367879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.223130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.135335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.5</td>\n",
       "      <td>0.082085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.049787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.5</td>\n",
       "      <td>0.030197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.018316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    logloss      pred\n",
       "0       0.1  0.904837\n",
       "1       0.2  0.818731\n",
       "2       0.3  0.740818\n",
       "3       0.4  0.670320\n",
       "4       0.5  0.606531\n",
       "5       0.6  0.548812\n",
       "6       0.7  0.496585\n",
       "7       0.8  0.449329\n",
       "8       0.9  0.406570\n",
       "9       1.0  0.367879\n",
       "10      1.5  0.223130\n",
       "11      2.0  0.135335\n",
       "12      2.5  0.082085\n",
       "13      3.0  0.049787\n",
       "14      3.5  0.030197\n",
       "15      4.0  0.018316"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "loss = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.5, 2, 2.5, 3, 3.5, 4 ]\n",
    "\n",
    "df = pd.DataFrame({'logloss':loss, 'pred': np.exp(np.negative(loss))},\n",
    "                  columns=['logloss','pred'])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluating Feature Importance\n",
    "\n",
    "Feature importance tells us how important each of the features (from the feature/import vector are to the prediction of a neural network, or other model.  There are many different ways to evaluate feature importance for neural networks.  The following paper presents a very good (and readable) overview of the various means of evaluating the importance of neural network inputs/features.\n",
    "\n",
    "Olden, J. D., Joy, M. K., & Death, R. G. (2004). [An accurate comparison of methods for quantifying variable importance in artificial neural networks using simulated data](http://depts.washington.edu/oldenlab/wordpress/wp-content/uploads/2013/03/EcologicalModelling_2004.pdf). *Ecological Modelling*, 178(3), 389-397.\n",
    "\n",
    "In summary, the following methods are available to neural networks:\n",
    "\n",
    "* Connection Weights Algorithm\n",
    "* Partial Derivatives\n",
    "* Input Perturbation\n",
    "* Sensitivity Analysis\n",
    "* Forward Stepwise Addition \n",
    "* Improved Stepwise Selection 1\n",
    "* Backward Stepwise Elimination\n",
    "* Improved Stepwise Selection\n",
    "\n",
    "For this class we will use the **Input Perturbation** feature ranking algorithm.  This algorithm will work with any regression or classification network.  implementation of the input perturbation algorithm for scikit-learn is given in the next section. This algorithm is implemented in a function below that will work with any scikit-learn model.\n",
    "\n",
    "This algorithm was introduced by [Breiman](https://en.wikipedia.org/wiki/Leo_Breiman) in his seminal paper on random forests.  Although he presented this algorithm in conjunction with random forests, it is model-independent and appropriate for any supervised learning model.  This algorithm, known as the input perturbation algorithm, works by evaluating a trained model’s accuracy with each of the inputs individually shuffled from a data set.  Shuffling an input causes it to become useless—effectively removing it from the model. More important inputs will produce a less accurate score when they are removed by shuffling them. This process makes sense, because important features will contribute to the accuracy of the model.  The TensorFlow version of this algorithm is taken from the following paper.\n",
    "\n",
    "Heaton, J., McElwee, S., & Cannady, J. (May 2017). Early stabilizing feature importance for TensorFlow deep neural networks. In *International Joint Conference on Neural Networks (IJCNN 2017)* (accepted for publication). IEEE.\n",
    "\n",
    "This algorithm will use logloss to evaluate a classification problem and RMSE for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn import metrics\n",
    "\n",
    "def perturbation_rank(model, x, y, names, regression):\n",
    "    errors = []\n",
    "\n",
    "    for i in range(x.shape[1]):\n",
    "        hold = np.array(x[:, i])\n",
    "        np.random.shuffle(x[:, i])\n",
    "        \n",
    "        if regression:\n",
    "            pred = model.predict(x)\n",
    "            error = metrics.mean_squared_error(y, pred)\n",
    "        else:\n",
    "            pred = model.predict_proba(x)\n",
    "            error = metrics.log_loss(y, pred)\n",
    "            \n",
    "        errors.append(error)\n",
    "        x[:, i] = hold\n",
    "        \n",
    "    max_error = np.max(errors)\n",
    "    importance = [e/max_error for e in errors]\n",
    "\n",
    "    data = {'name':names,'error':errors,'importance':importance}\n",
    "    result = pd.DataFrame(data, columns = ['name','error','importance'])\n",
    "    result.sort_values(by=['importance'], ascending=[0], inplace=True)\n",
    "    result.reset_index(inplace=True, drop=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Input Perturbation Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00600: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe99ab95cc0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "path = \"./data/\"\n",
    "    \n",
    "filename = os.path.join(path,\"iris.csv\")    \n",
    "df = pd.read_csv(filename,na_values=['NA','?'])\n",
    "\n",
    "species = encode_text_index(df,\"species\")\n",
    "x,y = to_xy(df,\"species\")\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "\n",
    "model.fit(x,y,validation_data=(x_test,y_test),callbacks=[monitor],verbose=0,epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/38 [========================>.....] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>petal_w</td>\n",
       "      <td>1.961095</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>petal_l</td>\n",
       "      <td>1.444562</td>\n",
       "      <td>0.736610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sepal_l</td>\n",
       "      <td>0.130144</td>\n",
       "      <td>0.066363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sepal_w</td>\n",
       "      <td>0.075281</td>\n",
       "      <td>0.038387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name     error  importance\n",
       "0  petal_w  1.961095    1.000000\n",
       "1  petal_l  1.444562    0.736610\n",
       "2  sepal_l  0.130144    0.066363\n",
       "3  sepal_w  0.075281    0.038387"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rank the features\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = list(df.columns) # x+y column names\n",
    "names.remove(\"species\") # remove the target(y)\n",
    "rank = perturbation_rank(model, x_test, y_test, names, False)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Input Perturbation Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00257: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe9983e0470>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank MPG fields\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "# Set the desired TensorFlow output level for this example\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "filename_read = os.path.join(path,\"auto-mpg.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "# create feature vector\n",
    "missing_median(df, 'horsepower')\n",
    "df.drop('name',1,inplace=True)\n",
    "encode_numeric_zscore(df, 'horsepower')\n",
    "encode_numeric_zscore(df, 'weight')\n",
    "encode_numeric_zscore(df, 'cylinders')\n",
    "encode_numeric_zscore(df, 'displacement')\n",
    "encode_numeric_zscore(df, 'acceleration')\n",
    "encode_text_dummy(df, 'origin')\n",
    "\n",
    "# Encode to a 2D matrix for training\n",
    "x,y = to_xy(df,'mpg')\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.20, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "model.fit(x,y,validation_data=(x_test,y_test),callbacks=[monitor],verbose=0,epochs=1000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weight</td>\n",
       "      <td>17.446842</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>year</td>\n",
       "      <td>16.235704</td>\n",
       "      <td>0.930581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>horsepower</td>\n",
       "      <td>15.936447</td>\n",
       "      <td>0.913429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cylinders</td>\n",
       "      <td>12.498433</td>\n",
       "      <td>0.716372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>origin-3</td>\n",
       "      <td>11.255796</td>\n",
       "      <td>0.645148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>acceleration</td>\n",
       "      <td>11.129115</td>\n",
       "      <td>0.637887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>displacement</td>\n",
       "      <td>10.780478</td>\n",
       "      <td>0.617904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>origin-1</td>\n",
       "      <td>10.268312</td>\n",
       "      <td>0.588548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>origin-2</td>\n",
       "      <td>10.093933</td>\n",
       "      <td>0.578554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name      error  importance\n",
       "0        weight  17.446842    1.000000\n",
       "1          year  16.235704    0.930581\n",
       "2    horsepower  15.936447    0.913429\n",
       "3     cylinders  12.498433    0.716372\n",
       "4      origin-3  11.255796    0.645148\n",
       "5  acceleration  11.129115    0.637887\n",
       "6  displacement  10.780478    0.617904\n",
       "7      origin-1  10.268312    0.588548\n",
       "8      origin-2  10.093933    0.578554"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rank the features\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = list(df.columns) # x+y column names\n",
    "names.remove(\"mpg\") # remove the target(y)\n",
    "rank = perturbation_rank(model, x_test, y_test, names, True)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Biological Response Data Set\n",
    "\n",
    "* [Biological Response Dataset at Kaggle](https://www.kaggle.com/c/bioresponse)\n",
    "* [1st place interview for Boehringer Ingelheim Biological Response](http://blog.kaggle.com/2012/07/05/1st-place-interview-for-boehringer-ingelheim-biological-response/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.learn as skflow\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import KFold\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "filename_train = os.path.join(path,\"bio_train.csv\")\n",
    "filename_test = os.path.join(path,\"bio_test.csv\")\n",
    "filename_submit = os.path.join(path,\"bio_submit.csv\")\n",
    "df_train = pd.read_csv(filename_train,na_values=['NA','?'])\n",
    "df_test = pd.read_csv(filename_test,na_values=['NA','?'])\n",
    "\n",
    "activity_classes = encode_text_index(df_train,'Activity')\n",
    "\n",
    "#display(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3751, 1777)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biological Response with Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting/Training...\n",
      "Epoch 00009: early stopping\n",
      "Fitting done...\n",
      "Validation logloss: 0.554332795172248\n",
      "Validation accuracy score: 0.7611940298507462\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.contrib.learn as skflow\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "# Set the desired TensorFlow output level for this example\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Encode feature vector\n",
    "x, y = to_xy(df_train,'Activity')\n",
    "x_submit = df_test.as_matrix().astype(np.float32)\n",
    "num_classes = len(activity_classes)\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=42) \n",
    "\n",
    "print(\"Fitting/Training...\")\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=0,epochs=1000)\n",
    "print(\"Fitting done...\")\n",
    "\n",
    "# Give logloss error\n",
    "pred = model.predict(x_test)\n",
    "pred2 = np.argmax(pred,axis=1)\n",
    "pred = pred[:,1]\n",
    "# Clip so that min is never exactly 0, max never 1\n",
    "pred = np.clip(pred,a_min=1e-6,a_max=(1-1e-6)) \n",
    "print(\"Validation logloss: {}\".format(sklearn.metrics.log_loss(y_test,pred)))\n",
    "\n",
    "# Evaluate success using accuracy\n",
    "pred_submit = pred.copy()\n",
    "y_test = y_test[:,1]\n",
    "score = metrics.accuracy_score(y_test, pred2)\n",
    "print(\"Validation accuracy score: {}\".format(score))\n",
    "\n",
    "# Build real submit file\n",
    "pred_submit = model.predict(x_submit)\n",
    "pred_submit = pred_submit[:,1]\n",
    "\n",
    "# Clip so that min is never exactly 0, max never 1\n",
    "pred = np.clip(pred,a_min=1e-6,a_max=(1-1e-6)) \n",
    "submit_df = pd.DataFrame({'MoleculeId':[x+1 for x in range(len(pred_submit))],'PredictedProbability':pred_submit})\n",
    "submit_df.to_csv(filename_submit, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Features/Columns are Important \n",
    "\n",
    "The following uses perturbation ranking to evaluate the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/938 [>.............................] - ETA: 0s - ETA: 0s - ETA: 0s - ETA: 0s - ETA: 0s - ETA: 0s - ETA: 0s - ETA: 0s - ETA: 0s - ETA: 0s - ETA: 0s - ETA: 0s - ETA: 0s - ETA: 0s - ETA: 0s - ETA: 0s - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D27</td>\n",
       "      <td>0.613003</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D51</td>\n",
       "      <td>0.565310</td>\n",
       "      <td>0.922197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D1012</td>\n",
       "      <td>0.563098</td>\n",
       "      <td>0.918589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D1188</td>\n",
       "      <td>0.561603</td>\n",
       "      <td>0.916150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D958</td>\n",
       "      <td>0.561371</td>\n",
       "      <td>0.915771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>D1049</td>\n",
       "      <td>0.561338</td>\n",
       "      <td>0.915718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>D1167</td>\n",
       "      <td>0.560920</td>\n",
       "      <td>0.915036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>D1059</td>\n",
       "      <td>0.560599</td>\n",
       "      <td>0.914512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>D994</td>\n",
       "      <td>0.560178</td>\n",
       "      <td>0.913826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>D1402</td>\n",
       "      <td>0.560009</td>\n",
       "      <td>0.913550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>D1271</td>\n",
       "      <td>0.559874</td>\n",
       "      <td>0.913330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>D1022</td>\n",
       "      <td>0.559298</td>\n",
       "      <td>0.912391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>D961</td>\n",
       "      <td>0.559155</td>\n",
       "      <td>0.912157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>D1067</td>\n",
       "      <td>0.559154</td>\n",
       "      <td>0.912155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>D1093</td>\n",
       "      <td>0.559064</td>\n",
       "      <td>0.912008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>D1203</td>\n",
       "      <td>0.558951</td>\n",
       "      <td>0.911824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>D998</td>\n",
       "      <td>0.558931</td>\n",
       "      <td>0.911791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>D1115</td>\n",
       "      <td>0.558921</td>\n",
       "      <td>0.911775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>D1241</td>\n",
       "      <td>0.558639</td>\n",
       "      <td>0.911315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>D1037</td>\n",
       "      <td>0.558636</td>\n",
       "      <td>0.911310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>D992</td>\n",
       "      <td>0.558609</td>\n",
       "      <td>0.911266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>D119</td>\n",
       "      <td>0.558352</td>\n",
       "      <td>0.910847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>D1385</td>\n",
       "      <td>0.558165</td>\n",
       "      <td>0.910541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>D1164</td>\n",
       "      <td>0.558149</td>\n",
       "      <td>0.910515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>D1352</td>\n",
       "      <td>0.557703</td>\n",
       "      <td>0.909788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>D1032</td>\n",
       "      <td>0.557697</td>\n",
       "      <td>0.909779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>D1055</td>\n",
       "      <td>0.557672</td>\n",
       "      <td>0.909737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>D1159</td>\n",
       "      <td>0.557610</td>\n",
       "      <td>0.909637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>D1142</td>\n",
       "      <td>0.557609</td>\n",
       "      <td>0.909635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>D199</td>\n",
       "      <td>0.557458</td>\n",
       "      <td>0.909388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>D1290</td>\n",
       "      <td>0.552261</td>\n",
       "      <td>0.900911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>D1075</td>\n",
       "      <td>0.552249</td>\n",
       "      <td>0.900891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>D1421</td>\n",
       "      <td>0.552242</td>\n",
       "      <td>0.900879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>D1454</td>\n",
       "      <td>0.552229</td>\n",
       "      <td>0.900859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>D1327</td>\n",
       "      <td>0.552226</td>\n",
       "      <td>0.900853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>D981</td>\n",
       "      <td>0.552206</td>\n",
       "      <td>0.900821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>D1391</td>\n",
       "      <td>0.552170</td>\n",
       "      <td>0.900763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>D1338</td>\n",
       "      <td>0.552116</td>\n",
       "      <td>0.900674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754</th>\n",
       "      <td>D1195</td>\n",
       "      <td>0.552079</td>\n",
       "      <td>0.900613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755</th>\n",
       "      <td>D1288</td>\n",
       "      <td>0.551997</td>\n",
       "      <td>0.900480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>D1209</td>\n",
       "      <td>0.551953</td>\n",
       "      <td>0.900408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>D1415</td>\n",
       "      <td>0.551911</td>\n",
       "      <td>0.900340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>D1144</td>\n",
       "      <td>0.551869</td>\n",
       "      <td>0.900270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>D1202</td>\n",
       "      <td>0.551773</td>\n",
       "      <td>0.900114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>D1095</td>\n",
       "      <td>0.551761</td>\n",
       "      <td>0.900094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>D1062</td>\n",
       "      <td>0.551709</td>\n",
       "      <td>0.900010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>D1002</td>\n",
       "      <td>0.551700</td>\n",
       "      <td>0.899995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>D1294</td>\n",
       "      <td>0.551662</td>\n",
       "      <td>0.899933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>D997</td>\n",
       "      <td>0.551597</td>\n",
       "      <td>0.899827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>D1007</td>\n",
       "      <td>0.551550</td>\n",
       "      <td>0.899751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>D1423</td>\n",
       "      <td>0.551508</td>\n",
       "      <td>0.899683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>D1154</td>\n",
       "      <td>0.551483</td>\n",
       "      <td>0.899642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>D1083</td>\n",
       "      <td>0.551407</td>\n",
       "      <td>0.899518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>D101</td>\n",
       "      <td>0.551246</td>\n",
       "      <td>0.899254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>D1205</td>\n",
       "      <td>0.551113</td>\n",
       "      <td>0.899037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>D1285</td>\n",
       "      <td>0.550985</td>\n",
       "      <td>0.898828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>D1043</td>\n",
       "      <td>0.550628</td>\n",
       "      <td>0.898247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>D1427</td>\n",
       "      <td>0.550579</td>\n",
       "      <td>0.898167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>D1219</td>\n",
       "      <td>0.548833</td>\n",
       "      <td>0.895319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>D964</td>\n",
       "      <td>0.548552</td>\n",
       "      <td>0.894861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1776 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       name     error  importance\n",
       "0       D27  0.613003    1.000000\n",
       "1       D51  0.565310    0.922197\n",
       "2     D1012  0.563098    0.918589\n",
       "3     D1188  0.561603    0.916150\n",
       "4      D958  0.561371    0.915771\n",
       "5     D1049  0.561338    0.915718\n",
       "6     D1167  0.560920    0.915036\n",
       "7     D1059  0.560599    0.914512\n",
       "8      D994  0.560178    0.913826\n",
       "9     D1402  0.560009    0.913550\n",
       "10    D1271  0.559874    0.913330\n",
       "11    D1022  0.559298    0.912391\n",
       "12     D961  0.559155    0.912157\n",
       "13    D1067  0.559154    0.912155\n",
       "14    D1093  0.559064    0.912008\n",
       "15    D1203  0.558951    0.911824\n",
       "16     D998  0.558931    0.911791\n",
       "17    D1115  0.558921    0.911775\n",
       "18    D1241  0.558639    0.911315\n",
       "19    D1037  0.558636    0.911310\n",
       "20     D992  0.558609    0.911266\n",
       "21     D119  0.558352    0.910847\n",
       "22    D1385  0.558165    0.910541\n",
       "23    D1164  0.558149    0.910515\n",
       "24    D1352  0.557703    0.909788\n",
       "25    D1032  0.557697    0.909779\n",
       "26    D1055  0.557672    0.909737\n",
       "27    D1159  0.557610    0.909637\n",
       "28    D1142  0.557609    0.909635\n",
       "29     D199  0.557458    0.909388\n",
       "...     ...       ...         ...\n",
       "1746  D1290  0.552261    0.900911\n",
       "1747  D1075  0.552249    0.900891\n",
       "1748  D1421  0.552242    0.900879\n",
       "1749  D1454  0.552229    0.900859\n",
       "1750  D1327  0.552226    0.900853\n",
       "1751   D981  0.552206    0.900821\n",
       "1752  D1391  0.552170    0.900763\n",
       "1753  D1338  0.552116    0.900674\n",
       "1754  D1195  0.552079    0.900613\n",
       "1755  D1288  0.551997    0.900480\n",
       "1756  D1209  0.551953    0.900408\n",
       "1757  D1415  0.551911    0.900340\n",
       "1758  D1144  0.551869    0.900270\n",
       "1759  D1202  0.551773    0.900114\n",
       "1760  D1095  0.551761    0.900094\n",
       "1761  D1062  0.551709    0.900010\n",
       "1762  D1002  0.551700    0.899995\n",
       "1763  D1294  0.551662    0.899933\n",
       "1764   D997  0.551597    0.899827\n",
       "1765  D1007  0.551550    0.899751\n",
       "1766  D1423  0.551508    0.899683\n",
       "1767  D1154  0.551483    0.899642\n",
       "1768  D1083  0.551407    0.899518\n",
       "1769   D101  0.551246    0.899254\n",
       "1770  D1205  0.551113    0.899037\n",
       "1771  D1285  0.550985    0.898828\n",
       "1772  D1043  0.550628    0.898247\n",
       "1773  D1427  0.550579    0.898167\n",
       "1774  D1219  0.548833    0.895319\n",
       "1775   D964  0.548552    0.894861\n",
       "\n",
       "[1776 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rank the features\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = list(df_train.columns) # x+y column names\n",
    "names.remove(\"Activity\") # remove the target(y)\n",
    "rank = perturbation_rank(model, x_test, y_test, names, False)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biological Response with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insample logloss: 0.12606115016833866\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn\n",
    "\n",
    "\n",
    "x, y = to_xy(df_train,'Activity')\n",
    "y = np.argmax(y,axis=1)\n",
    "x_test = df_test.as_matrix().astype(np.float32)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(x, y)\n",
    "pred = rf.predict_proba(x_test)\n",
    "pred = pred[:,1]\n",
    "pred_insample = rf.predict_proba(x)\n",
    "pred_insample = pred_insample[:,1]\n",
    "\n",
    "submit_df = pd.DataFrame({'MoleculeId':[x+1 for x in range(len(pred))],'PredictedProbability':pred})\n",
    "submit_df.to_csv(filename_submit, index=False)\n",
    "print(\"Insample logloss: {}\".format(sklearn.metrics.log_loss(y,pred_insample)))\n",
    "#display(submit_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D27</td>\n",
       "      <td>0.164608</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D78</td>\n",
       "      <td>0.150168</td>\n",
       "      <td>0.912276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D469</td>\n",
       "      <td>0.149858</td>\n",
       "      <td>0.910395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D66</td>\n",
       "      <td>0.148075</td>\n",
       "      <td>0.899560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D5</td>\n",
       "      <td>0.144928</td>\n",
       "      <td>0.880444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>D88</td>\n",
       "      <td>0.143171</td>\n",
       "      <td>0.869770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>D20</td>\n",
       "      <td>0.142081</td>\n",
       "      <td>0.863149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>D87</td>\n",
       "      <td>0.141662</td>\n",
       "      <td>0.860605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>D95</td>\n",
       "      <td>0.141030</td>\n",
       "      <td>0.856762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>D747</td>\n",
       "      <td>0.140913</td>\n",
       "      <td>0.856053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>D2</td>\n",
       "      <td>0.140609</td>\n",
       "      <td>0.854204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>D7</td>\n",
       "      <td>0.140206</td>\n",
       "      <td>0.851757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>D8</td>\n",
       "      <td>0.139760</td>\n",
       "      <td>0.849050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>D17</td>\n",
       "      <td>0.139308</td>\n",
       "      <td>0.846302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>D107</td>\n",
       "      <td>0.139267</td>\n",
       "      <td>0.846051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>D21</td>\n",
       "      <td>0.138604</td>\n",
       "      <td>0.842022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>D106</td>\n",
       "      <td>0.138145</td>\n",
       "      <td>0.839237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>D14</td>\n",
       "      <td>0.137727</td>\n",
       "      <td>0.836699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>D204</td>\n",
       "      <td>0.137699</td>\n",
       "      <td>0.836530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>D660</td>\n",
       "      <td>0.137665</td>\n",
       "      <td>0.836323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>D18</td>\n",
       "      <td>0.137566</td>\n",
       "      <td>0.835720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>D89</td>\n",
       "      <td>0.137414</td>\n",
       "      <td>0.834797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>D104</td>\n",
       "      <td>0.137376</td>\n",
       "      <td>0.834564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>D146</td>\n",
       "      <td>0.137079</td>\n",
       "      <td>0.832763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>D10</td>\n",
       "      <td>0.137053</td>\n",
       "      <td>0.832601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>D64</td>\n",
       "      <td>0.137013</td>\n",
       "      <td>0.832357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>D131</td>\n",
       "      <td>0.136960</td>\n",
       "      <td>0.832036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>D19</td>\n",
       "      <td>0.136580</td>\n",
       "      <td>0.829726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>D105</td>\n",
       "      <td>0.136488</td>\n",
       "      <td>0.829167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>D1036</td>\n",
       "      <td>0.136470</td>\n",
       "      <td>0.829061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>D397</td>\n",
       "      <td>0.126061</td>\n",
       "      <td>0.765827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>D485</td>\n",
       "      <td>0.126061</td>\n",
       "      <td>0.765827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>D371</td>\n",
       "      <td>0.126061</td>\n",
       "      <td>0.765827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>D369</td>\n",
       "      <td>0.126061</td>\n",
       "      <td>0.765827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>D1487</td>\n",
       "      <td>0.126061</td>\n",
       "      <td>0.765827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>D488</td>\n",
       "      <td>0.126061</td>\n",
       "      <td>0.765827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>D354</td>\n",
       "      <td>0.126061</td>\n",
       "      <td>0.765827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>D1470</td>\n",
       "      <td>0.126061</td>\n",
       "      <td>0.765827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754</th>\n",
       "      <td>D1474</td>\n",
       "      <td>0.126061</td>\n",
       "      <td>0.765827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755</th>\n",
       "      <td>D1478</td>\n",
       "      <td>0.126061</td>\n",
       "      <td>0.765827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>D1485</td>\n",
       "      <td>0.126061</td>\n",
       "      <td>0.765827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>D357</td>\n",
       "      <td>0.126061</td>\n",
       "      <td>0.765827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>D1498</td>\n",
       "      <td>0.126061</td>\n",
       "      <td>0.765827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>D1494</td>\n",
       "      <td>0.126061</td>\n",
       "      <td>0.765827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>D1663</td>\n",
       "      <td>0.126061</td>\n",
       "      <td>0.765826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>D272</td>\n",
       "      <td>0.126061</td>\n",
       "      <td>0.765826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>D870</td>\n",
       "      <td>0.126061</td>\n",
       "      <td>0.765826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>D1574</td>\n",
       "      <td>0.126061</td>\n",
       "      <td>0.765825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>D278</td>\n",
       "      <td>0.126061</td>\n",
       "      <td>0.765825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>D1458</td>\n",
       "      <td>0.126061</td>\n",
       "      <td>0.765824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>D581</td>\n",
       "      <td>0.126061</td>\n",
       "      <td>0.765823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>D511</td>\n",
       "      <td>0.126060</td>\n",
       "      <td>0.765821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>D780</td>\n",
       "      <td>0.126060</td>\n",
       "      <td>0.765818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>D1642</td>\n",
       "      <td>0.126060</td>\n",
       "      <td>0.765817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>D509</td>\n",
       "      <td>0.126059</td>\n",
       "      <td>0.765813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>D308</td>\n",
       "      <td>0.126059</td>\n",
       "      <td>0.765812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>D1641</td>\n",
       "      <td>0.126059</td>\n",
       "      <td>0.765811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>D1704</td>\n",
       "      <td>0.126058</td>\n",
       "      <td>0.765809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>D1713</td>\n",
       "      <td>0.126058</td>\n",
       "      <td>0.765808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>D385</td>\n",
       "      <td>0.126058</td>\n",
       "      <td>0.765807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1776 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       name     error  importance\n",
       "0       D27  0.164608    1.000000\n",
       "1       D78  0.150168    0.912276\n",
       "2      D469  0.149858    0.910395\n",
       "3       D66  0.148075    0.899560\n",
       "4        D5  0.144928    0.880444\n",
       "5       D88  0.143171    0.869770\n",
       "6       D20  0.142081    0.863149\n",
       "7       D87  0.141662    0.860605\n",
       "8       D95  0.141030    0.856762\n",
       "9      D747  0.140913    0.856053\n",
       "10       D2  0.140609    0.854204\n",
       "11       D7  0.140206    0.851757\n",
       "12       D8  0.139760    0.849050\n",
       "13      D17  0.139308    0.846302\n",
       "14     D107  0.139267    0.846051\n",
       "15      D21  0.138604    0.842022\n",
       "16     D106  0.138145    0.839237\n",
       "17      D14  0.137727    0.836699\n",
       "18     D204  0.137699    0.836530\n",
       "19     D660  0.137665    0.836323\n",
       "20      D18  0.137566    0.835720\n",
       "21      D89  0.137414    0.834797\n",
       "22     D104  0.137376    0.834564\n",
       "23     D146  0.137079    0.832763\n",
       "24      D10  0.137053    0.832601\n",
       "25      D64  0.137013    0.832357\n",
       "26     D131  0.136960    0.832036\n",
       "27      D19  0.136580    0.829726\n",
       "28     D105  0.136488    0.829167\n",
       "29    D1036  0.136470    0.829061\n",
       "...     ...       ...         ...\n",
       "1746   D397  0.126061    0.765827\n",
       "1747   D485  0.126061    0.765827\n",
       "1748   D371  0.126061    0.765827\n",
       "1749   D369  0.126061    0.765827\n",
       "1750  D1487  0.126061    0.765827\n",
       "1751   D488  0.126061    0.765827\n",
       "1752   D354  0.126061    0.765827\n",
       "1753  D1470  0.126061    0.765827\n",
       "1754  D1474  0.126061    0.765827\n",
       "1755  D1478  0.126061    0.765827\n",
       "1756  D1485  0.126061    0.765827\n",
       "1757   D357  0.126061    0.765827\n",
       "1758  D1498  0.126061    0.765827\n",
       "1759  D1494  0.126061    0.765827\n",
       "1760  D1663  0.126061    0.765826\n",
       "1761   D272  0.126061    0.765826\n",
       "1762   D870  0.126061    0.765826\n",
       "1763  D1574  0.126061    0.765825\n",
       "1764   D278  0.126061    0.765825\n",
       "1765  D1458  0.126061    0.765824\n",
       "1766   D581  0.126061    0.765823\n",
       "1767   D511  0.126060    0.765821\n",
       "1768   D780  0.126060    0.765818\n",
       "1769  D1642  0.126060    0.765817\n",
       "1770   D509  0.126059    0.765813\n",
       "1771   D308  0.126059    0.765812\n",
       "1772  D1641  0.126059    0.765811\n",
       "1773  D1704  0.126058    0.765809\n",
       "1774  D1713  0.126058    0.765808\n",
       "1775   D385  0.126058    0.765807\n",
       "\n",
       "[1776 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rank the features\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = list(df_train.columns) # x+y column names\n",
    "names.remove(\"Activity\") # remove the target(y)\n",
    "rank = perturbation_rank(rf, x, y, names, False)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Bagging\n",
    "\n",
    "Neural networks will typically achieve better results when they are bagged.  Bagging a neural network is a process where the same neural network is trained over and over and the results are averaged together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Model: 0 : <keras.wrappers.scikit_learn.KerasClassifier object at 0x7fe99a8962e8>\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6160     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5035     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4649     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4302     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4139     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3921     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3889     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3670     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3668     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3423     \n",
      "2016/2501 [=======================>......] - ETA: 0sFold #0: loss=0.5487004665827624\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6371     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5346     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4832     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4528     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4294     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4222     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4039     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3921     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3855     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3745     \n",
      "1888/2501 [=====================>........] - ETA: 0sFold #1: loss=0.48166648848338395\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6515     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5463     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4851     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4504     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4315     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4147     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3991     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3844     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3729     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3646     \n",
      "1952/2501 [======================>.......] - ETA: 0sFold #2: loss=0.5787311878185909\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6639     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5599     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4932     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4617     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4362     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4244     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4107     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4007     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3934     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3901     \n",
      "1920/2501 [======================>.......] - ETA: 0sFold #3: loss=0.48769577270851083\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.6323     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5383     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4787     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4482     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4232     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4064     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3961     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3827     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3658     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3590     \n",
      "1888/2501 [=====================>........] - ETA: 0sFold #4: loss=0.5609770002075718\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.6610     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5575     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4892     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4545     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4312     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4172     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4045     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3971     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3842     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3792     \n",
      "1888/2501 [=====================>........] - ETA: 0sFold #5: loss=0.6653757042706742\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.6137     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5064     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4616     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4338     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4203     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3969     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3868     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3787     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3624     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3549     \n",
      "1856/2501 [=====================>........] - ETA: 0sFold #6: loss=0.4998111961594711\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.6421     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.5375     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4800     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4564     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4291     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4111     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4020     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3891     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3806     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3659     \n",
      "1792/2501 [====================>.........] - ETA: 0sFold #7: loss=0.5024692149606323\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.6059     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4927     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4497     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4301     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4042     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3911     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3677     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3599     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3522     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3351     \n",
      "1984/2501 [======================>.......] - ETA: 0sFold #8: loss=0.5180004297221857\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.6329     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.5172     \n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3377/3377 [==============================] - 0s - loss: 0.4654     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4405     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4150     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3972     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3807     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3708     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3528     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3452     \n",
      "2016/2501 [=======================>......] - ETA: 0sFold #9: loss=0.5691933364539885\n",
      "KerasClassifier: Mean loss=0.5412620797367772\n",
      "Model: 1 : <keras.wrappers.scikit_learn.KerasClassifier object at 0x7fe97fc6b630>\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5914     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4887     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4366     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4115     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3872     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3612     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3410     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3259     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3118     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.2854     \n",
      "1952/2501 [======================>.......] - ETA: 0sFold #0: loss=0.5991805174101954\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6145     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5069     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4562     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4287     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4072     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3870     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3752     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3555     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3404     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3230     \n",
      "1952/2501 [======================>.......] - ETA: 0sFold #1: loss=0.4701714933468222\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5882     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4807     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4493     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4122     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3954     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3792     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3682     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3465     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3401     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3286     \n",
      "1984/2501 [======================>.......] - ETA: 0sFold #2: loss=0.62807002440046\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6056     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5011     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4640     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4375     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4154     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3988     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3841     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3729     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3631     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3547     \n",
      "1952/2501 [======================>.......] - ETA: 0sFold #3: loss=0.4867631485541681\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.6400     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5212     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4655     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4378     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4201     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4016     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3874     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3793     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3648     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3602     \n",
      "1824/2501 [====================>.........] - ETA: 0sFold #4: loss=0.5619218991955363\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5887     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4719     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4303     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4028     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3933     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3692     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3525     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3363     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3194     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3169     \n",
      "1888/2501 [=====================>........] - ETA: 0sFold #5: loss=0.6824605919589666\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.6106     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5086     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4552     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4282     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4113     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3914     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3768     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3707     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3454     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3269     \n",
      "1888/2501 [=====================>........] - ETA: 0sFold #6: loss=0.4635486421261313\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.6260     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.5125     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4612     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4406     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4209     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4060     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4008     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3844     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3772     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3668     \n",
      "1888/2501 [=====================>........] - ETA: 0sFold #7: loss=0.5158144028372387\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.6060     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4975     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4547     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4229     \n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3377/3377 [==============================] - 0s - loss: 0.4005     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3751     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3617     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3461     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3313     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3224     \n",
      "1888/2501 [=====================>........] - ETA: 0sFold #8: loss=0.5073889618154452\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.5931     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4779     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4376     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4073     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3828     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3647     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3492     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3229     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3106     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.2895     \n",
      "1888/2501 [=====================>........] - ETA: 0sFold #9: loss=0.607970871842984\n",
      "KerasClassifier: Mean loss=0.5523290553487947\n",
      "Model: 2 : <keras.wrappers.scikit_learn.KerasClassifier object at 0x7fe97fc6b668>\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5935     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4921     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4487     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4141     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3993     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3725     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3545     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3347     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3109     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3071     \n",
      "1728/2501 [===================>..........] - ETA: 0s ETAFold #0: loss=0.5635462774459565\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6087     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5070     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4621     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4361     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4161     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3949     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3804     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3659     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3503     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3351     \n",
      "1792/2501 [====================>.........] - ETA: 0sFold #1: loss=0.4671718313321161\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5949     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4901     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4358     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4074     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3794     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3649     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3434     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3222     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3105     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.2988     \n",
      "1760/2501 [====================>.........] - ETA: 0sFold #2: loss=0.6407005508532486\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6410     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5174     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4679     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4371     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4292     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4106     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3988     - ETA: 0s - loss: 0.\n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3841     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3802     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3725     \n",
      "1760/2501 [====================>.........] - ETA: 0sFold #3: loss=0.4982745907472799\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5833     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4707     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4237     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4061     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3695     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3415     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3267     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.2985     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.2781     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.2619     \n",
      "1760/2501 [====================>.........] - ETA: 0sFold #4: loss=0.6400998005708534\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.6430     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5148     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4647     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4318     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4128     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3935     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3874     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3775     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3633     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3559     \n",
      "1728/2501 [===================>..........] - ETA: 0sFold #5: loss=0.6922605134975682\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5861     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4847     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4419     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4131     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3829     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3667     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3409     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3284     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3145     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.2864     \n",
      "1760/2501 [====================>.........] - ETA: 0sFold #6: loss=0.4837244673316681\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.6075     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.5048     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4575     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4255     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4063     \n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3377/3377 [==============================] - 0s - loss: 0.3881     - ETA: 0s - loss: 0.383\n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3667     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3537     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3383     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3223     \n",
      "1760/2501 [====================>.........] - ETA: 0sFold #7: loss=0.5879131621346576\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.5981     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4875     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4481     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4141     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3925     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3718     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3631     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3351     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3109     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.2938     \n",
      "1696/2501 [===================>..........] - ETA: 0sFold #8: loss=0.5358429047403205\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.6024     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4933     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4516     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4176     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3990     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3777     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3658     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3471     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3338     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3212     \n",
      "1728/2501 [===================>..........] - ETA: 0sFold #9: loss=0.6646257641913158\n",
      "KerasClassifier: Mean loss=0.5774159862844984\n",
      "\n",
      "Blending models.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "PATH = \"./data/\"\n",
    "SHUFFLE = False\n",
    "FOLDS = 10\n",
    "\n",
    "def build_ann(input_size,classes,neurons):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=input_size, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Dense(classes,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def mlogloss(y_test, preds):\n",
    "    epsilon = 1e-15\n",
    "    sum = 0\n",
    "    for row in zip(preds,y_test):\n",
    "        x = row[0][row[1]]\n",
    "        x = max(epsilon,x)\n",
    "        x = min(1-epsilon,x)\n",
    "        sum+=math.log(x)\n",
    "    return( (-1/len(preds))*sum)\n",
    "\n",
    "def stretch(y):\n",
    "    return (y - y.min()) / (y.max() - y.min())\n",
    "\n",
    "\n",
    "def blend_ensemble(x, y, x_submit):\n",
    "\n",
    "    folds = list(StratifiedKFold(y, FOLDS))\n",
    "    feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x.shape[0])]\n",
    "\n",
    "    models = [\n",
    "        KerasClassifier(build_fn=build_ann,neurons=10,input_size=x.shape[1],classes=2),\n",
    "        KerasClassifier(build_fn=build_ann,neurons=20,input_size=x.shape[1],classes=2),\n",
    "        KerasClassifier(build_fn=build_ann,neurons=30,input_size=x.shape[1],classes=2)\n",
    "        ]\n",
    "\n",
    "    dataset_blend_train = np.zeros((x.shape[0], len(models)))\n",
    "    dataset_blend_test = np.zeros((x_submit.shape[0], len(models)))\n",
    "\n",
    "    for j, model in enumerate(models):\n",
    "        print(\"Model: {} : {}\".format(j, model) )\n",
    "        fold_sums = np.zeros((x_submit.shape[0], len(folds)))\n",
    "        total_loss = 0\n",
    "        for i, (train, test) in enumerate(folds):\n",
    "            x_train = x[train]\n",
    "            y_train = y[train]\n",
    "            x_test = x[test]\n",
    "            y_test = y[test]\n",
    "            model.fit(x_train, y_train)\n",
    "            pred = np.array(model.predict_proba(x_test))\n",
    "            # pred = model.predict_proba(x_test)\n",
    "            dataset_blend_train[test, j] = pred[:, 1]\n",
    "            pred2 = np.array(model.predict_proba(x_submit))\n",
    "            #fold_sums[:, i] = model.predict_proba(x_submit)[:, 1]\n",
    "            fold_sums[:, i] = pred2[:, 1]\n",
    "            loss = mlogloss(y_test, pred)\n",
    "            total_loss+=loss\n",
    "            print(\"Fold #{}: loss={}\".format(i,loss))\n",
    "        print(\"{}: Mean loss={}\".format(model.__class__.__name__,total_loss/len(folds)))\n",
    "        dataset_blend_test[:, j] = fold_sums.mean(1)\n",
    "\n",
    "    print()\n",
    "    print(\"Blending models.\")\n",
    "    blend = LogisticRegression()\n",
    "    blend.fit(dataset_blend_train, y)\n",
    "    return blend.predict_proba(dataset_blend_test)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    np.random.seed(42)  # seed to shuffle the train set\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    filename_train = os.path.join(PATH, \"bio_train.csv\")\n",
    "    df_train = pd.read_csv(filename_train, na_values=['NA', '?'])\n",
    "\n",
    "    filename_submit = os.path.join(PATH, \"bio_test.csv\")\n",
    "    df_submit = pd.read_csv(filename_submit, na_values=['NA', '?'])\n",
    "\n",
    "    predictors = list(df_train.columns.values)\n",
    "    predictors.remove('Activity')\n",
    "    x = df_train.as_matrix(predictors)\n",
    "    y = df_train['Activity']\n",
    "    x_submit = df_submit.as_matrix()\n",
    "\n",
    "    if SHUFFLE:\n",
    "        idx = np.random.permutation(y.size)\n",
    "        x = x[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    submit_data = blend_ensemble(x, y, x_submit)\n",
    "    submit_data = stretch(submit_data)\n",
    "\n",
    "    ####################\n",
    "    # Build submit file\n",
    "    ####################\n",
    "    ids = [id+1 for id in range(submit_data.shape[0])]\n",
    "    submit_filename = os.path.join(PATH, \"bio_submit.csv\")\n",
    "    submit_df = pd.DataFrame({'MoleculeId': ids, 'PredictedProbability': submit_data[:, 1]},\n",
    "                             columns=['MoleculeId','PredictedProbability'])\n",
    "    submit_df.to_csv(submit_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Ensemble\n",
    "\n",
    "A neural network ensemble combines neural network predictions with other models.  The exact blend of all of these models is determined by logistic regression.  The following code performs this blend for a classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Model: 0 : <keras.wrappers.scikit_learn.KerasClassifier object at 0x7fe97fc6b550>\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6029     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4847     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4463     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4167     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3976     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3729     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3623     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3372     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3407     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3043     \n",
      "1824/2501 [====================>.........] - ETA: 0sFold #0: loss=0.5699500593962061\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6164     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5101     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4709     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4399     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4186     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4129     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3948     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3795     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3719     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3623     \n",
      "1824/2501 [====================>.........] - ETA: 0sFold #1: loss=0.4875375405023912\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6390     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5300     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4701     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4367     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4208     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4032     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3874     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3716     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3579     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3467     \n",
      "1824/2501 [====================>.........] - ETA: 0sFold #2: loss=0.5891018534275058\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6617     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5545     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4888     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4575     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4316     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4193     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4063     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3955     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3891     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3834     \n",
      "1792/2501 [====================>.........] - ETA: 0sFold #3: loss=0.4921888669934585\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.6089     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5148     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4554     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4323     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4025     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3884     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3748     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3624     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3395     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3364     \n",
      "1760/2501 [====================>.........] - ETA: 0sFold #4: loss=0.5812659871878084\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.6354     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5189     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4608     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4347     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4137     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4030     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3928     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3824     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3708     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3618     \n",
      "1792/2501 [====================>.........] - ETA: 0sFold #5: loss=0.7002197389846028\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.6050     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4964     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4495     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4159     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3966     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3719     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3575     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3483     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3236     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3151     \n",
      "1824/2501 [====================>.........] - ETA: 0sFold #6: loss=0.4827185212223421\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.6228     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.5169     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4648     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4447     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4204     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3995     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3900     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3748     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3645     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3483     \n",
      "1760/2501 [====================>.........] - ETA: 0sFold #7: loss=0.5131647059333817\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.5926     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4840     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4388     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4190     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3892     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3735     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3471     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3378     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3148     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3002     \n",
      "1728/2501 [===================>..........] - ETA: 0sFold #8: loss=0.5428594131186842\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.6020     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4883     \n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3377/3377 [==============================] - 0s - loss: 0.4414     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4176     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3858     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3648     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3423     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3398     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3076     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.2984     \n",
      "1760/2501 [====================>.........] - ETA: 0sFold #9: loss=0.6261977498960891\n",
      "KerasClassifier: Mean loss=0.5585204436662469\n",
      "Model: 1 : KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "Fold #0: loss=3.606678388314123\n",
      "Fold #1: loss=2.2197228940978317\n",
      "Fold #2: loss=3.6717523663107237\n",
      "Fold #3: loss=2.5045156203944594\n",
      "Fold #4: loss=4.443553550438037\n",
      "Fold #5: loss=4.410524301688227\n",
      "Fold #6: loss=3.400455469543658\n",
      "Fold #7: loss=3.0885474338547683\n",
      "Fold #8: loss=2.1219335323249253\n",
      "Fold #9: loss=3.0613772690497245\n",
      "KNeighborsClassifier: Mean loss=3.2529060826016476\n",
      "Model: 2 : RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fold #0: loss=0.45859387026544035\n",
      "Fold #1: loss=0.43226994423427495\n",
      "Fold #2: loss=0.4762818884046444\n",
      "Fold #3: loss=0.43338810350051965\n",
      "Fold #4: loss=0.47502753371869\n",
      "Fold #5: loss=0.492658174605396\n",
      "Fold #6: loss=0.4015988631740329\n",
      "Fold #7: loss=0.4717953689427381\n",
      "Fold #8: loss=0.4517105069522408\n",
      "Fold #9: loss=0.46971357575792544\n",
      "RandomForestClassifier: Mean loss=0.45630378295559015\n",
      "Model: 3 : RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fold #0: loss=0.4501929416405849\n",
      "Fold #1: loss=0.43527263084022183\n",
      "Fold #2: loss=0.4698774736941605\n",
      "Fold #3: loss=0.41784016293793536\n",
      "Fold #4: loss=0.470862719005535\n",
      "Fold #5: loss=0.5007009099421501\n",
      "Fold #6: loss=0.40496551263998337\n",
      "Fold #7: loss=0.4688833748405199\n",
      "Fold #8: loss=0.45798694087339165\n",
      "Fold #9: loss=0.45974118174298945\n",
      "RandomForestClassifier: Mean loss=0.45363238481574725\n",
      "Model: 4 : ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Fold #0: loss=0.45131247798132806\n",
      "Fold #1: loss=0.4230980001099943\n",
      "Fold #2: loss=0.586187893928544\n",
      "Fold #3: loss=0.42553475374872446\n",
      "Fold #4: loss=0.4964472681005012\n",
      "Fold #5: loss=0.494729595836481\n",
      "Fold #6: loss=0.41487748895885895\n",
      "Fold #7: loss=0.48506699974271766\n",
      "Fold #8: loss=0.4595462862215204\n",
      "Fold #9: loss=0.4673771268612419\n",
      "ExtraTreesClassifier: Mean loss=0.47041778914899124\n",
      "Model: 5 : ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Fold #0: loss=0.44916718203909234\n",
      "Fold #1: loss=0.4141554387911032\n",
      "Fold #2: loss=0.5897250522492203\n",
      "Fold #3: loss=0.4234716042493701\n",
      "Fold #4: loss=0.49267251836808157\n",
      "Fold #5: loss=0.5029294613500651\n",
      "Fold #6: loss=0.4138860223390099\n",
      "Fold #7: loss=0.6423108659854284\n",
      "Fold #8: loss=0.5382143341356395\n",
      "Fold #9: loss=0.6301460999525225\n",
      "ExtraTreesClassifier: Mean loss=0.5096678579459533\n",
      "Model: 6 : GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.05, loss='deviance', max_depth=6,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "              presort='auto', random_state=None, subsample=0.5, verbose=0,\n",
      "              warm_start=False)\n",
      "Fold #0: loss=0.48033546988804227\n",
      "Fold #1: loss=0.4659003726481674\n",
      "Fold #2: loss=0.4757100968118204\n",
      "Fold #3: loss=0.44494271863599\n",
      "Fold #4: loss=0.5016905324925709\n",
      "Fold #5: loss=0.4946939556794898\n",
      "Fold #6: loss=0.45002051472618254\n",
      "Fold #7: loss=0.4586389893553193\n",
      "Fold #8: loss=0.45452694136050165\n",
      "Fold #9: loss=0.47425905315384753\n",
      "GradientBoostingClassifier: Mean loss=0.4700718644751932\n",
      "\n",
      "Blending models.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "PATH = \"./data/\"\n",
    "SHUFFLE = False\n",
    "FOLDS = 10\n",
    "\n",
    "def build_ann(input_size,classes,neurons):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=input_size, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Dense(classes,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def mlogloss(y_test, preds):\n",
    "    epsilon = 1e-15\n",
    "    sum = 0\n",
    "    for row in zip(preds,y_test):\n",
    "        x = row[0][row[1]]\n",
    "        x = max(epsilon,x)\n",
    "        x = min(1-epsilon,x)\n",
    "        sum+=math.log(x)\n",
    "    return( (-1/len(preds))*sum)\n",
    "\n",
    "def stretch(y):\n",
    "    return (y - y.min()) / (y.max() - y.min())\n",
    "\n",
    "\n",
    "def blend_ensemble(x, y, x_submit):\n",
    "\n",
    "    folds = list(StratifiedKFold(y, FOLDS))\n",
    "    feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x.shape[0])]\n",
    "\n",
    "    models = [\n",
    "        KerasClassifier(build_fn=build_ann,neurons=20,input_size=x.shape[1],classes=2),\n",
    "        KNeighborsClassifier(n_neighbors=3),\n",
    "        RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "        RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50)]\n",
    "\n",
    "    dataset_blend_train = np.zeros((x.shape[0], len(models)))\n",
    "    dataset_blend_test = np.zeros((x_submit.shape[0], len(models)))\n",
    "\n",
    "    for j, model in enumerate(models):\n",
    "        print(\"Model: {} : {}\".format(j, model) )\n",
    "        fold_sums = np.zeros((x_submit.shape[0], len(folds)))\n",
    "        total_loss = 0\n",
    "        for i, (train, test) in enumerate(folds):\n",
    "            x_train = x[train]\n",
    "            y_train = y[train]\n",
    "            x_test = x[test]\n",
    "            y_test = y[test]\n",
    "            model.fit(x_train, y_train)\n",
    "            pred = np.array(model.predict_proba(x_test))\n",
    "            # pred = model.predict_proba(x_test)\n",
    "            dataset_blend_train[test, j] = pred[:, 1]\n",
    "            pred2 = np.array(model.predict_proba(x_submit))\n",
    "            #fold_sums[:, i] = model.predict_proba(x_submit)[:, 1]\n",
    "            fold_sums[:, i] = pred2[:, 1]\n",
    "            loss = mlogloss(y_test, pred)\n",
    "            total_loss+=loss\n",
    "            print(\"Fold #{}: loss={}\".format(i,loss))\n",
    "        print(\"{}: Mean loss={}\".format(model.__class__.__name__,total_loss/len(folds)))\n",
    "        dataset_blend_test[:, j] = fold_sums.mean(1)\n",
    "\n",
    "    print()\n",
    "    print(\"Blending models.\")\n",
    "    blend = LogisticRegression()\n",
    "    blend.fit(dataset_blend_train, y)\n",
    "    return blend.predict_proba(dataset_blend_test)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    np.random.seed(42)  # seed to shuffle the train set\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    filename_train = os.path.join(PATH, \"bio_train.csv\")\n",
    "    df_train = pd.read_csv(filename_train, na_values=['NA', '?'])\n",
    "\n",
    "    filename_submit = os.path.join(PATH, \"bio_test.csv\")\n",
    "    df_submit = pd.read_csv(filename_submit, na_values=['NA', '?'])\n",
    "\n",
    "    predictors = list(df_train.columns.values)\n",
    "    predictors.remove('Activity')\n",
    "    x = df_train.as_matrix(predictors)\n",
    "    y = df_train['Activity']\n",
    "    x_submit = df_submit.as_matrix()\n",
    "\n",
    "    if SHUFFLE:\n",
    "        idx = np.random.permutation(y.size)\n",
    "        x = x[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    submit_data = blend_ensemble(x, y, x_submit)\n",
    "    submit_data = stretch(submit_data)\n",
    "\n",
    "    ####################\n",
    "    # Build submit file\n",
    "    ####################\n",
    "    ids = [id+1 for id in range(submit_data.shape[0])]\n",
    "    submit_filename = os.path.join(PATH, \"bio_submit.csv\")\n",
    "    submit_df = pd.DataFrame({'MoleculeId': ids, 'PredictedProbability': submit_data[:, 1]},\n",
    "                             columns=['MoleculeId','PredictedProbability'])\n",
    "    submit_df.to_csv(submit_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
