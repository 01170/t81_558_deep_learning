{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Class 12: Deep Learning Applications**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tonight we will see how to apply deep learning networks to data science.  There are many applications of deep learning.  However, we will focus primarily upon data science.  For this class we will go beyond simple academic examples and see how to construct an ensemble that could potentially lead to a high score on a Kaggle competition.  We will see how to evaluate the importance of features and several ways to combine models.\n",
    "\n",
    "Tonights topics include:\n",
    "\n",
    "* Log Loss Error\n",
    "* Evaluating Feature Importance\n",
    "* The Biological Response Data Set\n",
    "* Neural Network Bagging\n",
    "* Nueral Network Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions from Previous Classes\n",
    "\n",
    "The following are utility functions from previous classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/usr/local/lib/python3.4/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df,name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name,x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df,name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df,name,mean=None,sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name]-mean)/sd\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df,target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    print(target_type)\n",
    "    \n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        return df.as_matrix(result).astype(np.float32),df.as_matrix([target]).astype(np.int32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32),df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "# Regression chart, we will see more of this chart in the next class.\n",
    "def chart_regression(pred,y):\n",
    "    t = pd.DataFrame({'pred' : pred.flatten(), 'y' : y_test.flatten()})\n",
    "    t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogLoss Error\n",
    "\n",
    "Log loss is an error metric that is often used in place of accuracy for classification.  Log loss allows for \"partial credit\" when a miss classification occurs.  For example, a model might be used to classify A, B and C.  The correct answer might be A, however if the classification network chose B as having the highest probability, then accuracy gives the neural network no credit for this classification.  \n",
    "\n",
    "However, with log loss, the probability of the correct answer is added to the score.  For example, the correct answer might be A, but if the neural network only predicted .8 probability of A being correct, then the value -log(.8) is added.\n",
    "\n",
    "$$ logloss = -\\frac{1}{N}\\sum^N_{i=1}\\sum^M_{j=1}y_{ij} \\log(\\hat{y}_{ij}) $$\n",
    "\n",
    "The following table shows the logloss scores that correspond to the average predicted accuracy for the correct item. The **pred** column specifies the average probability for the correct class.  The **logloss** column specifies the log loss for that probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>logloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.900</td>\n",
       "      <td>0.105361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.800</td>\n",
       "      <td>0.223144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.700</td>\n",
       "      <td>0.356675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.510826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.400</td>\n",
       "      <td>0.916291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.300</td>\n",
       "      <td>1.203973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.200</td>\n",
       "      <td>1.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.100</td>\n",
       "      <td>2.302585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.075</td>\n",
       "      <td>2.590267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.050</td>\n",
       "      <td>2.995732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.025</td>\n",
       "      <td>3.688879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pred   logloss\n",
       "0   1.000 -0.000000\n",
       "1   0.900  0.105361\n",
       "2   0.800  0.223144\n",
       "3   0.700  0.356675\n",
       "4   0.600  0.510826\n",
       "5   0.500  0.693147\n",
       "6   0.400  0.916291\n",
       "7   0.300  1.203973\n",
       "8   0.200  1.609438\n",
       "9   0.100  2.302585\n",
       "10  0.075  2.590267\n",
       "11  0.050  2.995732\n",
       "12  0.025  3.688879\n",
       "13  0.000       inf"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "loss = [1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.075, 0.05, 0.025, 0.0 ]\n",
    "\n",
    "df = pd.DataFrame({'pred':loss, 'logloss': -np.log(loss)},columns=['pred','logloss'])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows the opposit.  For a given logloss, what is the average probability for the correct class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logloss</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.904837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.818731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.740818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.670320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.606531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.548812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.496585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.449329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.406570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.367879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.223130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.135335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.5</td>\n",
       "      <td>0.082085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.049787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.5</td>\n",
       "      <td>0.030197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.018316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    logloss      pred\n",
       "0       0.1  0.904837\n",
       "1       0.2  0.818731\n",
       "2       0.3  0.740818\n",
       "3       0.4  0.670320\n",
       "4       0.5  0.606531\n",
       "5       0.6  0.548812\n",
       "6       0.7  0.496585\n",
       "7       0.8  0.449329\n",
       "8       0.9  0.406570\n",
       "9       1.0  0.367879\n",
       "10      1.5  0.223130\n",
       "11      2.0  0.135335\n",
       "12      2.5  0.082085\n",
       "13      3.0  0.049787\n",
       "14      3.5  0.030197\n",
       "15      4.0  0.018316"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "loss = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.5, 2, 2.5, 3, 3.5, 4 ]\n",
    "\n",
    "df = pd.DataFrame({'logloss':loss, 'pred': np.exp(np.negative(loss))},\n",
    "                  columns=['logloss','pred'])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluating Feature Importance\n",
    "\n",
    "Feature importance tells us how important each of the features (from the feature/import vector are to the prediction of a neural network, or other model.  There are many different ways to evaluate feature importance for neural networks.  The following paper presents a very good (and readable) overview of the various means of evaluating the importance of neural network inputs/features.\n",
    "\n",
    "Olden, J. D., Joy, M. K., & Death, R. G. (2004). [An accurate comparison of methods for quantifying variable importance in artificial neural networks using simulated data](http://depts.washington.edu/oldenlab/wordpress/wp-content/uploads/2013/03/EcologicalModelling_2004.pdf). *Ecological Modelling*, 178(3), 389-397.\n",
    "\n",
    "In summary, the following methods are available to neural networks:\n",
    "\n",
    "* Connection Weights Algorithm\n",
    "* Partial Derivatives\n",
    "* Input Perturbation\n",
    "* Sensitivity Analysis\n",
    "* Forward Stepwise Addition \n",
    "* Improved Stepwise Selection 1\n",
    "* Backward Stepwise Elimination\n",
    "* Improved Stepwise Selection\n",
    "\n",
    "For this class we will use the **Input Perturbation** feature ranking algorithm.  This algorithm will work with any regression or classification network.  implementation of the input perturbation algorithm for scikit-learn is given in the next section. This algorithm is implemented in a function below that will work with any scikit-learn model.\n",
    "\n",
    "This algorithm was introduced by [Breiman](https://en.wikipedia.org/wiki/Leo_Breiman) in his seminal paper on random forests.  Although he presented this algorithm in conjunction with random forests, it is model-independent and appropriate for any supervised learning model.  This algorithm, known as the input perturbation algorithm, works by evaluating a trained model’s accuracy with each of the inputs individually shuffled from a data set.  Shuffling an input causes it to become useless—effectively removing it from the model. More important inputs will produce a less accurate score when they are removed by shuffling them. This process makes sense, because important features will contribute to the accuracy of the model.\n",
    "\n",
    "The provided algorithm will use logloss to evaluate a classification problem and RMSE for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def mlogloss(y_test, preds):\n",
    "    epsilon = 1e-15\n",
    "    sum = 0\n",
    "    for row in zip(preds,y_test):\n",
    "        x = row[0][row[1]]\n",
    "        x = max(epsilon,x)\n",
    "        x = min(1-epsilon,x)\n",
    "        sum+=math.log(x)\n",
    "    return( (-1/len(preds))*sum)\n",
    "\n",
    "def perturbation_rank(model, x, y, names, regression):\n",
    "    errors = []\n",
    "\n",
    "    for i in range(x.shape[1]):\n",
    "        hold = np.array(x[:, i])\n",
    "        np.random.shuffle(x[:, i])\n",
    "        \n",
    "        if regression:\n",
    "            pred = model.predict(x)\n",
    "            error = metrics.mean_squared_error(y, pred)\n",
    "        else:\n",
    "            pred = model.predict_proba(x)\n",
    "            error = mlogloss(y, pred)\n",
    "            \n",
    "        errors.append(error)\n",
    "        x[:, i] = hold\n",
    "        \n",
    "    max_error = np.max(errors)\n",
    "    importance = [e/max_error for e in errors]\n",
    "   \n",
    "    data = {'name':names,'error':errors,'importance':importance}\n",
    "    result = pd.DataFrame(data, columns = ['name','error','importance'])\n",
    "    result.sort(['importance'], ascending=[0], inplace=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Input Perturbation Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/learn/python/learn/io/data_feeder.py:281: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  out.itemset((i, self.y[sample]), 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #50, epoch #12, avg. train loss: 0.39461, avg. val loss: 0.41250\n",
      "Step #100, epoch #25, avg. train loss: 0.07423, avg. val loss: 0.16584\n",
      "Step #150, epoch #37, avg. train loss: 0.03887, avg. val loss: 0.16293\n",
      "Step #200, epoch #50, avg. train loss: 0.03189, avg. val loss: 0.17975\n",
      "Step #250, epoch #62, avg. train loss: 0.01956, avg. val loss: 0.18672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping. Best step:\n",
      " step 73 with loss 0.07940922677516937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorFlowDNNClassifier(batch_size=32, class_weight=None, clip_gradients=5.0,\n",
       "            config=None, continue_training=False, dropout=None,\n",
       "            hidden_units=[20, 10, 5], learning_rate=0.1, n_classes=3,\n",
       "            optimizer='Adagrad', steps=10000, verbose=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classification ranking\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import tensorflow.contrib.learn as skflow\n",
    "import numpy as np\n",
    "\n",
    "path = \"./data/\"\n",
    "    \n",
    "filename = os.path.join(path,\"iris.csv\")    \n",
    "df = pd.read_csv(filename,na_values=['NA','?'])\n",
    "\n",
    "# Encode feature vector\n",
    "encode_numeric_zscore(df,'petal_w')\n",
    "encode_numeric_zscore(df,'petal_l')\n",
    "encode_numeric_zscore(df,'sepal_w')\n",
    "encode_numeric_zscore(df,'sepal_l')\n",
    "species = encode_text_index(df,\"species\")\n",
    "num_classes = len(species)\n",
    "\n",
    "# Create x & y for training\n",
    "\n",
    "# Create the x-side (feature vectors) of the training\n",
    "x, y = to_xy(df,'species')\n",
    "    \n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=45) \n",
    "    # as much as I would like to use 42, it gives a perfect result, and a boring confusion matrix!\n",
    "        \n",
    "# Create a deep neural network with 3 hidden layers of 10, 20, 10\n",
    "classifier = skflow.TensorFlowDNNClassifier(hidden_units=[20, 10, 5], n_classes=num_classes,\n",
    "    steps=10000)\n",
    "\n",
    "# Early stopping\n",
    "early_stop = skflow.monitors.ValidationMonitor(x_test, y_test,\n",
    "    early_stopping_rounds=200, print_steps=50, n_classes=num_classes)\n",
    "    \n",
    "# Fit/train neural network\n",
    "classifier.fit(x_train, y_train, monitor=early_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:38: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>petal_w</td>\n",
       "      <td>2.827385</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>petal_l</td>\n",
       "      <td>2.365922</td>\n",
       "      <td>0.836788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sepal_l</td>\n",
       "      <td>0.617823</td>\n",
       "      <td>0.218514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sepal_w</td>\n",
       "      <td>0.366238</td>\n",
       "      <td>0.129533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name     error  importance\n",
       "3  petal_w  2.827385    1.000000\n",
       "2  petal_l  2.365922    0.836788\n",
       "0  sepal_l  0.617823    0.218514\n",
       "1  sepal_w  0.366238    0.129533"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rank the features\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = df.columns.values[0:-1] # x column names\n",
    "rank = perturbation_rank(classifier, x_test, y_test, names, False)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Input Perturbation Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "Step #50, epoch #5, avg. train loss: 91.35452, avg. val loss: 81.57780\n",
      "Step #100, epoch #10, avg. train loss: 22.42863, avg. val loss: 16.40538\n",
      "Step #150, epoch #15, avg. train loss: 13.33392, avg. val loss: 9.34911\n",
      "Step #200, epoch #20, avg. train loss: 10.46424, avg. val loss: 7.41646\n",
      "Step #250, epoch #25, avg. train loss: 9.70131, avg. val loss: 7.06513\n",
      "Step #300, epoch #30, avg. train loss: 9.29365, avg. val loss: 6.48175\n",
      "Step #350, epoch #35, avg. train loss: 7.95944, avg. val loss: 5.66703\n",
      "Step #400, epoch #40, avg. train loss: 7.88686, avg. val loss: 5.73117\n",
      "Step #450, epoch #45, avg. train loss: 7.72119, avg. val loss: 5.31690\n",
      "Step #500, epoch #50, avg. train loss: 7.46781, avg. val loss: 5.38290\n",
      "Step #550, epoch #55, avg. train loss: 7.22598, avg. val loss: 5.39639\n",
      "Step #600, epoch #60, avg. train loss: 7.10224, avg. val loss: 5.14146\n",
      "Step #650, epoch #65, avg. train loss: 6.95857, avg. val loss: 4.65309\n",
      "Step #700, epoch #70, avg. train loss: 6.71094, avg. val loss: 4.78885\n",
      "Step #750, epoch #75, avg. train loss: 6.72612, avg. val loss: 4.68753\n",
      "Step #800, epoch #80, avg. train loss: 6.67923, avg. val loss: 4.64102\n",
      "Step #850, epoch #85, avg. train loss: 6.80301, avg. val loss: 4.74877\n",
      "Step #900, epoch #90, avg. train loss: 6.46789, avg. val loss: 4.46814\n",
      "Step #950, epoch #95, avg. train loss: 6.49462, avg. val loss: 4.37168\n",
      "Step #1000, epoch #100, avg. train loss: 6.28528, avg. val loss: 4.38087\n",
      "Step #1050, epoch #105, avg. train loss: 6.35701, avg. val loss: 4.41127\n",
      "Step #1100, epoch #110, avg. train loss: 6.43468, avg. val loss: 4.42055\n",
      "Step #1150, epoch #115, avg. train loss: 6.36032, avg. val loss: 4.34383\n",
      "Step #1200, epoch #120, avg. train loss: 6.16866, avg. val loss: 4.19689\n",
      "Step #1250, epoch #125, avg. train loss: 6.11910, avg. val loss: 4.26469\n",
      "Step #1300, epoch #130, avg. train loss: 6.00381, avg. val loss: 4.31075\n",
      "Step #1350, epoch #135, avg. train loss: 6.14456, avg. val loss: 4.27796\n",
      "Step #1400, epoch #140, avg. train loss: 5.98823, avg. val loss: 4.18407\n",
      "Step #1450, epoch #145, avg. train loss: 6.10478, avg. val loss: 4.09698\n",
      "Step #1500, epoch #150, avg. train loss: 6.10619, avg. val loss: 4.26873\n",
      "Step #1550, epoch #155, avg. train loss: 5.82874, avg. val loss: 4.16855\n",
      "Step #1600, epoch #160, avg. train loss: 5.86975, avg. val loss: 4.06271\n",
      "Step #1650, epoch #165, avg. train loss: 5.80431, avg. val loss: 4.01623\n",
      "Step #1700, epoch #170, avg. train loss: 5.94144, avg. val loss: 4.10316\n",
      "Step #1750, epoch #175, avg. train loss: 5.91832, avg. val loss: 4.00636\n",
      "Step #1800, epoch #180, avg. train loss: 5.84610, avg. val loss: 4.00934\n",
      "Step #1850, epoch #185, avg. train loss: 5.78820, avg. val loss: 3.98281\n",
      "Step #1900, epoch #190, avg. train loss: 5.79928, avg. val loss: 4.06623\n",
      "Step #1950, epoch #195, avg. train loss: 5.83923, avg. val loss: 4.01967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping. Best step:\n",
      " step 1755 with loss 3.594547986984253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorFlowDNNRegressor(batch_size=32, clip_gradients=5.0, config=None,\n",
       "            continue_training=False, dropout=None,\n",
       "            hidden_units=[50, 25, 10], learning_rate=0.1, n_classes=0,\n",
       "            optimizer='Adagrad', steps=5000, verbose=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regression ranking\n",
    "\n",
    "import tensorflow.contrib.learn as skflow\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "filename_read = os.path.join(path,\"auto-mpg.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "# create feature vector\n",
    "missing_median(df, 'horsepower')\n",
    "df.drop('name',1,inplace=True)\n",
    "encode_numeric_zscore(df, 'horsepower')\n",
    "encode_numeric_zscore(df, 'weight')\n",
    "encode_numeric_zscore(df, 'cylinders')\n",
    "encode_numeric_zscore(df, 'displacement')\n",
    "encode_numeric_zscore(df, 'acceleration')\n",
    "encode_text_dummy(df, 'origin')\n",
    "\n",
    "# Encode to a 2D matrix for training\n",
    "x,y = to_xy(df,'mpg')\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Create a deep neural network with 3 hidden layers of 50, 25, 10\n",
    "regressor = skflow.TensorFlowDNNRegressor(hidden_units=[50, 25, 10], steps=5000)\n",
    "\n",
    "# Early stopping\n",
    "early_stop = skflow.monitors.ValidationMonitor(x_test, y_test,\n",
    "    early_stopping_rounds=200, print_steps=50)\n",
    "\n",
    "# Fit/train neural network\n",
    "regressor.fit(x_train, y_train, monitor=early_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:38: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>horsepower</td>\n",
       "      <td>25.248680</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weight</td>\n",
       "      <td>20.607851</td>\n",
       "      <td>0.816195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>year</td>\n",
       "      <td>12.813540</td>\n",
       "      <td>0.507493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>displacement</td>\n",
       "      <td>11.658216</td>\n",
       "      <td>0.461736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acceleration</td>\n",
       "      <td>8.377670</td>\n",
       "      <td>0.331806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>origin-1</td>\n",
       "      <td>8.324597</td>\n",
       "      <td>0.329704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>origin-2</td>\n",
       "      <td>8.188123</td>\n",
       "      <td>0.324299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>origin-3</td>\n",
       "      <td>7.299152</td>\n",
       "      <td>0.289090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cylinders</td>\n",
       "      <td>6.833241</td>\n",
       "      <td>0.270638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name      error  importance\n",
       "2    horsepower  25.248680    1.000000\n",
       "3        weight  20.607851    0.816195\n",
       "5          year  12.813540    0.507493\n",
       "1  displacement  11.658216    0.461736\n",
       "4  acceleration   8.377670    0.331806\n",
       "6      origin-1   8.324597    0.329704\n",
       "7      origin-2   8.188123    0.324299\n",
       "8      origin-3   7.299152    0.289090\n",
       "0     cylinders   6.833241    0.270638"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rank the features\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = df.columns.values[1:] # x column names\n",
    "rank = perturbation_rank(regressor, x_test, y_test, names, True)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Biological Response Data Set\n",
    "\n",
    "* [Biological Response Dataset at Kaggle](https://www.kaggle.com/c/bioresponse)\n",
    "* [1st place interview for Boehringer Ingelheim Biological Response](http://blog.kaggle.com/2012/07/05/1st-place-interview-for-boehringer-ingelheim-biological-response/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.contrib.learn as skflow\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.cross_validation import KFold\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "filename_train = os.path.join(path,\"bio_train.csv\")\n",
    "filename_test = os.path.join(path,\"bio_test.csv\")\n",
    "filename_submit = os.path.join(path,\"bio_submit.csv\")\n",
    "df_train = pd.read_csv(filename_train,na_values=['NA','?'])\n",
    "df_test = pd.read_csv(filename_test,na_values=['NA','?'])\n",
    "\n",
    "encode_text_index(df_train,'Activity')\n",
    "\n",
    "#display(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biological Response with Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/learn/python/learn/io/data_feeder.py:281: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  out.itemset((i, self.y[sample]), 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #49, avg. train loss: 0.71293, avg. val loss: 0.70956\n",
      "Step #100, epoch #1, avg. train loss: 0.68720, avg. val loss: 0.67496\n",
      "Step #150, epoch #1, avg. train loss: 0.65776, avg. val loss: 0.65117\n",
      "Step #200, epoch #2, avg. train loss: 0.64972, avg. val loss: 0.64277\n",
      "Step #250, epoch #2, avg. train loss: 0.62097, avg. val loss: 0.62835\n",
      "Step #300, epoch #3, avg. train loss: 0.59859, avg. val loss: 0.61697\n",
      "Step #350, epoch #3, avg. train loss: 0.60311, avg. val loss: 0.60590\n",
      "Step #400, epoch #4, avg. train loss: 0.56464, avg. val loss: 0.60644\n",
      "Step #450, epoch #5, avg. train loss: 0.56250, avg. val loss: 0.59649\n",
      "Step #500, epoch #5, avg. train loss: 0.55042, avg. val loss: 0.58440\n",
      "Step #550, epoch #6, avg. train loss: 0.54160, avg. val loss: 0.58837\n",
      "Step #600, epoch #6, avg. train loss: 0.53489, avg. val loss: 0.56726\n",
      "Step #650, epoch #7, avg. train loss: 0.52663, avg. val loss: 0.58412\n",
      "Step #700, epoch #7, avg. train loss: 0.51687, avg. val loss: 0.57393\n",
      "Step #750, epoch #8, avg. train loss: 0.51722, avg. val loss: 0.58200\n",
      "Step #800, epoch #9, avg. train loss: 0.48581, avg. val loss: 0.56962\n",
      "Step #850, epoch #9, avg. train loss: 0.49913, avg. val loss: 0.58082\n",
      "Step #900, epoch #10, avg. train loss: 0.49679, avg. val loss: 0.56997\n",
      "Step #950, epoch #10, avg. train loss: 0.47982, avg. val loss: 0.56918\n",
      "Step #1000, epoch #11, avg. train loss: 0.49133, avg. val loss: 0.57388\n",
      "Step #1050, epoch #11, avg. train loss: 0.46980, avg. val loss: 0.58057\n",
      "Step #1100, epoch #12, avg. train loss: 0.45433, avg. val loss: 0.58381\n",
      "Step #1150, epoch #13, avg. train loss: 0.45127, avg. val loss: 0.58846\n",
      "Step #1200, epoch #13, avg. train loss: 0.47991, avg. val loss: 0.57903\n",
      "Step #1250, epoch #14, avg. train loss: 0.46112, avg. val loss: 0.57883\n",
      "Step #1300, epoch #14, avg. train loss: 0.44343, avg. val loss: 0.58056\n",
      "Step #1350, epoch #15, avg. train loss: 0.42739, avg. val loss: 0.58738\n",
      "Step #1400, epoch #15, avg. train loss: 0.47484, avg. val loss: 0.57388\n",
      "Step #1450, epoch #16, avg. train loss: 0.42763, avg. val loss: 0.58445\n",
      "Step #1500, epoch #17, avg. train loss: 0.44327, avg. val loss: 0.57376\n",
      "Step #1550, epoch #17, avg. train loss: 0.40853, avg. val loss: 0.59978\n",
      "Step #1600, epoch #18, avg. train loss: 0.41910, avg. val loss: 0.58004\n",
      "Step #1650, epoch #18, avg. train loss: 0.40590, avg. val loss: 0.61385\n",
      "Step #1700, epoch #19, avg. train loss: 0.42820, avg. val loss: 0.60594\n",
      "Step #1750, epoch #19, avg. train loss: 0.41307, avg. val loss: 0.58408\n",
      "Step #1800, epoch #20, avg. train loss: 0.37722, avg. val loss: 0.61067\n",
      "Step #1850, epoch #21, avg. train loss: 0.39656, avg. val loss: 0.62544\n",
      "Step #1900, epoch #21, avg. train loss: 0.38560, avg. val loss: 0.61233\n",
      "Step #1950, epoch #22, avg. train loss: 0.38652, avg. val loss: 0.63749\n",
      "Step #2000, epoch #22, avg. train loss: 0.40150, avg. val loss: 0.59898\n",
      "Step #2050, epoch #23, avg. train loss: 0.39894, avg. val loss: 0.60660\n",
      "Step #2100, epoch #23, avg. train loss: 0.36725, avg. val loss: 0.63532\n",
      "Step #2150, epoch #24, avg. train loss: 0.36846, avg. val loss: 0.63694\n",
      "Step #2200, epoch #25, avg. train loss: 0.39123, avg. val loss: 0.63728\n",
      "Step #2250, epoch #25, avg. train loss: 0.33432, avg. val loss: 0.65220\n",
      "Step #2300, epoch #26, avg. train loss: 0.37439, avg. val loss: 0.63239\n",
      "Step #2350, epoch #26, avg. train loss: 0.37683, avg. val loss: 0.66446\n",
      "Step #2400, epoch #27, avg. train loss: 0.36200, avg. val loss: 0.63233\n",
      "Step #2450, epoch #27, avg. train loss: 0.35016, avg. val loss: 0.67892\n",
      "Step #2500, epoch #28, avg. train loss: 0.34503, avg. val loss: 0.67347\n",
      "Step #2550, epoch #28, avg. train loss: 0.36795, avg. val loss: 0.67297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping. Best step:\n",
      " step 1572 with loss 0.5185275077819824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation logloss: 0.5593276151913225\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import tensorflow.contrib.learn as skflow\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "def dropout_model_classification(X, y):\n",
    "    \"\"\"This is DNN with 10, 20, 10 hidden layers, and dropout of 0.5 probability.\"\"\"\n",
    "    layers = skflow.ops.dnn(X, [500, 250, 100, 50], dropout=0.5)\n",
    "    return skflow.models.logistic_regression(layers, y)\n",
    "\n",
    "# Encode feature vector\n",
    "x, y = to_xy(df_train,'Activity')\n",
    "x_submit = df_test.as_matrix().astype(np.float32)\n",
    "num_classes = 2\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=42) \n",
    "\n",
    "# Create a deep neural network with 3 hidden layers of 10, 20, 10\n",
    "classifier = skflow.TensorFlowEstimator(model_fn=dropout_model_classification, \n",
    "                                        n_classes=num_classes, steps=10000)\n",
    "\n",
    "# Early stopping\n",
    "early_stop = skflow.monitors.ValidationMonitor(x_test, y_test,\n",
    "    early_stopping_rounds=1000, print_steps=50, n_classes=num_classes)\n",
    "    \n",
    "# Fit/train neural network\n",
    "classifier.fit(x_train, y_train, monitor=early_stop)\n",
    "\n",
    "\n",
    "pred = classifier.predict_proba(x_test)\n",
    "pred = pred[:,1]\n",
    "print(\"Validation logloss: {}\".format(sklearn.metrics.log_loss(y_test,pred)))\n",
    "\n",
    "\n",
    "pred_submit = classifier.predict_proba(x_submit)\n",
    "pred_submit = pred_submit[:,1]\n",
    "submit_df = pd.DataFrame({'MoleculeId':[x+1 for x in range(len(pred_submit))],'PredictedProbability':pred_submit})\n",
    "submit_df.to_csv(filename_submit, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:38: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>D26</td>\n",
       "      <td>0.632833</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>D50</td>\n",
       "      <td>0.570915</td>\n",
       "      <td>0.902158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>D960</td>\n",
       "      <td>0.567925</td>\n",
       "      <td>0.897433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>D1110</td>\n",
       "      <td>0.566142</td>\n",
       "      <td>0.894616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>D193</td>\n",
       "      <td>0.565461</td>\n",
       "      <td>0.893539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>D1021</td>\n",
       "      <td>0.565296</td>\n",
       "      <td>0.893279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>D200</td>\n",
       "      <td>0.565017</td>\n",
       "      <td>0.892838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>D1092</td>\n",
       "      <td>0.564899</td>\n",
       "      <td>0.892650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>D1158</td>\n",
       "      <td>0.564392</td>\n",
       "      <td>0.891850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>D997</td>\n",
       "      <td>0.564098</td>\n",
       "      <td>0.891385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>D1157</td>\n",
       "      <td>0.564027</td>\n",
       "      <td>0.891273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>D1070</td>\n",
       "      <td>0.563420</td>\n",
       "      <td>0.890314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>D1009</td>\n",
       "      <td>0.563344</td>\n",
       "      <td>0.890195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>D1005</td>\n",
       "      <td>0.563220</td>\n",
       "      <td>0.889998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>D1152</td>\n",
       "      <td>0.563152</td>\n",
       "      <td>0.889890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>D1007</td>\n",
       "      <td>0.563127</td>\n",
       "      <td>0.889850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>D958</td>\n",
       "      <td>0.563089</td>\n",
       "      <td>0.889791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>D1035</td>\n",
       "      <td>0.563022</td>\n",
       "      <td>0.889686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>D1036</td>\n",
       "      <td>0.562977</td>\n",
       "      <td>0.889614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>D1193</td>\n",
       "      <td>0.562974</td>\n",
       "      <td>0.889610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>D1401</td>\n",
       "      <td>0.562922</td>\n",
       "      <td>0.889526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>D1187</td>\n",
       "      <td>0.562847</td>\n",
       "      <td>0.889409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>D991</td>\n",
       "      <td>0.562700</td>\n",
       "      <td>0.889176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>D1115</td>\n",
       "      <td>0.562697</td>\n",
       "      <td>0.889171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>D1058</td>\n",
       "      <td>0.562655</td>\n",
       "      <td>0.889105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>D995</td>\n",
       "      <td>0.562640</td>\n",
       "      <td>0.889081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>D1173</td>\n",
       "      <td>0.562466</td>\n",
       "      <td>0.888807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>D1202</td>\n",
       "      <td>0.562286</td>\n",
       "      <td>0.888522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>D1417</td>\n",
       "      <td>0.562166</td>\n",
       "      <td>0.888333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>D1012</td>\n",
       "      <td>0.562104</td>\n",
       "      <td>0.888235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>D1026</td>\n",
       "      <td>0.556973</td>\n",
       "      <td>0.880127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>D1262</td>\n",
       "      <td>0.556972</td>\n",
       "      <td>0.880125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>D1441</td>\n",
       "      <td>0.556944</td>\n",
       "      <td>0.880081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>D1060</td>\n",
       "      <td>0.556924</td>\n",
       "      <td>0.880049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>D992</td>\n",
       "      <td>0.556912</td>\n",
       "      <td>0.880029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>D1001</td>\n",
       "      <td>0.556897</td>\n",
       "      <td>0.880007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>D978</td>\n",
       "      <td>0.556895</td>\n",
       "      <td>0.880004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>D1063</td>\n",
       "      <td>0.556852</td>\n",
       "      <td>0.879935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>D1031</td>\n",
       "      <td>0.556823</td>\n",
       "      <td>0.879890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>D1059</td>\n",
       "      <td>0.556744</td>\n",
       "      <td>0.879765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>D1151</td>\n",
       "      <td>0.556735</td>\n",
       "      <td>0.879751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>D1285</td>\n",
       "      <td>0.556734</td>\n",
       "      <td>0.879749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>D1293</td>\n",
       "      <td>0.556585</td>\n",
       "      <td>0.879513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>D1198</td>\n",
       "      <td>0.556501</td>\n",
       "      <td>0.879381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>D1050</td>\n",
       "      <td>0.556484</td>\n",
       "      <td>0.879354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>D1332</td>\n",
       "      <td>0.556280</td>\n",
       "      <td>0.879032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>D1199</td>\n",
       "      <td>0.556229</td>\n",
       "      <td>0.878951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>D1106</td>\n",
       "      <td>0.556221</td>\n",
       "      <td>0.878939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>D1284</td>\n",
       "      <td>0.556153</td>\n",
       "      <td>0.878830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>D1143</td>\n",
       "      <td>0.556152</td>\n",
       "      <td>0.878828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>D999</td>\n",
       "      <td>0.556015</td>\n",
       "      <td>0.878612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>D1304</td>\n",
       "      <td>0.555886</td>\n",
       "      <td>0.878408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>D1406</td>\n",
       "      <td>0.555884</td>\n",
       "      <td>0.878406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>D1433</td>\n",
       "      <td>0.555882</td>\n",
       "      <td>0.878402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>D1011</td>\n",
       "      <td>0.555851</td>\n",
       "      <td>0.878353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>D985</td>\n",
       "      <td>0.555311</td>\n",
       "      <td>0.877500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>D1095</td>\n",
       "      <td>0.555257</td>\n",
       "      <td>0.877415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>D1038</td>\n",
       "      <td>0.554892</td>\n",
       "      <td>0.876838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>D1337</td>\n",
       "      <td>0.553926</td>\n",
       "      <td>0.875311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>D1068</td>\n",
       "      <td>0.553815</td>\n",
       "      <td>0.875136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1776 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       name     error  importance\n",
       "26      D26  0.632833    1.000000\n",
       "50      D50  0.570915    0.902158\n",
       "960    D960  0.567925    0.897433\n",
       "1110  D1110  0.566142    0.894616\n",
       "193    D193  0.565461    0.893539\n",
       "1021  D1021  0.565296    0.893279\n",
       "200    D200  0.565017    0.892838\n",
       "1092  D1092  0.564899    0.892650\n",
       "1158  D1158  0.564392    0.891850\n",
       "997    D997  0.564098    0.891385\n",
       "1157  D1157  0.564027    0.891273\n",
       "1070  D1070  0.563420    0.890314\n",
       "1009  D1009  0.563344    0.890195\n",
       "1005  D1005  0.563220    0.889998\n",
       "1152  D1152  0.563152    0.889890\n",
       "1007  D1007  0.563127    0.889850\n",
       "958    D958  0.563089    0.889791\n",
       "1035  D1035  0.563022    0.889686\n",
       "1036  D1036  0.562977    0.889614\n",
       "1193  D1193  0.562974    0.889610\n",
       "1401  D1401  0.562922    0.889526\n",
       "1187  D1187  0.562847    0.889409\n",
       "991    D991  0.562700    0.889176\n",
       "1115  D1115  0.562697    0.889171\n",
       "1058  D1058  0.562655    0.889105\n",
       "995    D995  0.562640    0.889081\n",
       "1173  D1173  0.562466    0.888807\n",
       "1202  D1202  0.562286    0.888522\n",
       "1417  D1417  0.562166    0.888333\n",
       "1012  D1012  0.562104    0.888235\n",
       "...     ...       ...         ...\n",
       "1026  D1026  0.556973    0.880127\n",
       "1262  D1262  0.556972    0.880125\n",
       "1441  D1441  0.556944    0.880081\n",
       "1060  D1060  0.556924    0.880049\n",
       "992    D992  0.556912    0.880029\n",
       "1001  D1001  0.556897    0.880007\n",
       "978    D978  0.556895    0.880004\n",
       "1063  D1063  0.556852    0.879935\n",
       "1031  D1031  0.556823    0.879890\n",
       "1059  D1059  0.556744    0.879765\n",
       "1151  D1151  0.556735    0.879751\n",
       "1285  D1285  0.556734    0.879749\n",
       "1293  D1293  0.556585    0.879513\n",
       "1198  D1198  0.556501    0.879381\n",
       "1050  D1050  0.556484    0.879354\n",
       "1332  D1332  0.556280    0.879032\n",
       "1199  D1199  0.556229    0.878951\n",
       "1106  D1106  0.556221    0.878939\n",
       "1284  D1284  0.556153    0.878830\n",
       "1143  D1143  0.556152    0.878828\n",
       "999    D999  0.556015    0.878612\n",
       "1304  D1304  0.555886    0.878408\n",
       "1406  D1406  0.555884    0.878406\n",
       "1433  D1433  0.555882    0.878402\n",
       "1011  D1011  0.555851    0.878353\n",
       "985    D985  0.555311    0.877500\n",
       "1095  D1095  0.555257    0.877415\n",
       "1038  D1038  0.554892    0.876838\n",
       "1337  D1337  0.553926    0.875311\n",
       "1068  D1068  0.553815    0.875136\n",
       "\n",
       "[1776 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rank the features\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = df_train.columns.values[0:-1] # x column names\n",
    "rank = perturbation_rank(classifier, x_test, y_test, names, False)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biological Response with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:9: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insample logloss: 0.1255907712846015\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn\n",
    "\n",
    "\n",
    "x, y = to_xy(df_train,'Activity')\n",
    "x_test = df_test.as_matrix().astype(np.float32)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(x, y)\n",
    "pred = rf.predict_proba(x_test)\n",
    "pred = pred[:,1]\n",
    "pred_insample = rf.predict_proba(x)\n",
    "pred_insample = pred_insample[:,1]\n",
    "\n",
    "submit_df = pd.DataFrame({'MoleculeId':[x+1 for x in range(len(pred))],'PredictedProbability':pred})\n",
    "submit_df.to_csv(filename_submit, index=False)\n",
    "print(\"Insample logloss: {}\".format(sklearn.metrics.log_loss(y,pred_insample)))\n",
    "#display(submit_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Bagging\n",
    "\n",
    "Neural networks will typically achieve better results when they are bagged.  Bagging a neural network is a process where the same neural network is trained over and over and the results are averaged together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow.contrib.learn as skflow\n",
    "\n",
    "PATH = \"./data/\"\n",
    "SHUFFLE = False\n",
    "FOLDS = 10\n",
    "\n",
    "def mlogloss(y_test, preds):\n",
    "    epsilon = 1e-15\n",
    "    sum = 0\n",
    "    for row in zip(preds,y_test):\n",
    "        x = row[0][row[1]]\n",
    "        x = max(epsilon,x)\n",
    "        x = min(1-epsilon,x)\n",
    "        sum+=math.log(x)\n",
    "    return( (-1/len(preds))*sum)\n",
    "\n",
    "def stretch(y):\n",
    "    return (y - y.min()) / (y.max() - y.min())\n",
    "\n",
    "\n",
    "def blend_ensemble(x, y, x_submit):\n",
    "\n",
    "    folds = list(StratifiedKFold(y, FOLDS))\n",
    "\n",
    "    models = [\n",
    "        skflow.TensorFlowDNNClassifier(hidden_units=[100, 50, 25, 5], n_classes=2, steps=1000),\n",
    "        skflow.TensorFlowDNNClassifier(hidden_units=[100, 50, 25, 5], n_classes=2, steps=500),\n",
    "        skflow.TensorFlowDNNClassifier(hidden_units=[200, 100, 50, 25], n_classes=2, steps=1000),\n",
    "        skflow.TensorFlowDNNClassifier(hidden_units=[200, 100, 50, 25], n_classes=2, steps=500),\n",
    "        skflow.TensorFlowDNNClassifier(hidden_units=[50, 25, 5], n_classes=2, steps=500)]\n",
    "\n",
    "    dataset_blend_train = np.zeros((x.shape[0], len(models)))\n",
    "    dataset_blend_test = np.zeros((x_submit.shape[0], len(models)))\n",
    "\n",
    "    for j, model in enumerate(models):\n",
    "        print(\"Model: {} : {}\".format(j, model) )\n",
    "        fold_sums = np.zeros((x_submit.shape[0], len(folds)))\n",
    "        total_loss = 0\n",
    "        for i, (train, test) in enumerate(folds):\n",
    "            x_train = x[train]\n",
    "            y_train = y[train]\n",
    "            x_test = x[test]\n",
    "            y_test = y[test]\n",
    "            model.fit(x_train, y_train)\n",
    "            pred = model.predict_proba(x_test)\n",
    "            dataset_blend_train[test, j] = pred[:, 1]\n",
    "            fold_sums[:, i] = model.predict_proba(x_submit)[:, 1]\n",
    "            loss = mlogloss(y_test, pred)\n",
    "            total_loss+=loss\n",
    "            print(\"Fold #{}: loss={}\".format(i,loss))\n",
    "        print(\"{}: Mean loss={}\".format(model.__class__.__name__,total_loss/len(folds)))\n",
    "        dataset_blend_test[:, j] = fold_sums.mean(1)\n",
    "\n",
    "    print()\n",
    "    print(\"Blending models.\")\n",
    "    blend = LogisticRegression()\n",
    "    blend.fit(dataset_blend_train, y)\n",
    "    return blend.predict_proba(dataset_blend_test)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    np.random.seed(42)  # seed to shuffle the train set\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    filename_train = os.path.join(PATH, \"bio_train.csv\")\n",
    "    df_train = pd.read_csv(filename_train, na_values=['NA', '?'])\n",
    "\n",
    "    filename_submit = os.path.join(PATH, \"bio_test.csv\")\n",
    "    df_submit = pd.read_csv(filename_submit, na_values=['NA', '?'])\n",
    "\n",
    "    predictors = list(df_train.columns.values)\n",
    "    predictors.remove('Activity')\n",
    "    x = df_train.as_matrix(predictors)\n",
    "    y = df_train['Activity']\n",
    "    x_submit = df_submit.as_matrix()\n",
    "\n",
    "    if SHUFFLE:\n",
    "        idx = np.random.permutation(y.size)\n",
    "        x = x[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    submit_data = blend_ensemble(x, y, x_submit)\n",
    "    submit_data = stretch(submit_data)\n",
    "\n",
    "    ####################\n",
    "    # Build submit file\n",
    "    ####################\n",
    "    ids = [id+1 for id in range(submit_data.shape[0])]\n",
    "    submit_filename = os.path.join(PATH, \"bio_submit.csv\")\n",
    "    submit_df = pd.DataFrame({'MoleculeId': ids, 'PredictedProbability': submit_data[:, 1]},\n",
    "                             columns=['MoleculeId','PredictedProbability'])\n",
    "    submit_df.to_csv(submit_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Ensemble\n",
    "\n",
    "A neural network ensemble combines neural network predictions with other models.  The exact blend of all of these models is determined by logistic regression.  The following code performs this blend for a classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Model: 0 : TensorFlowDNNClassifier(batch_size=32, class_weight=None, clip_gradients=5.0,\n",
      "            config=None, continue_training=False, dropout=None,\n",
      "            hidden_units=[100, 50, 25, 5], learning_rate=0.1, n_classes=2,\n",
      "            optimizer='Adagrad', steps=1000, verbose=1)\n",
      "Step #99, avg. train loss: 0.66611\n",
      "Step #200, epoch #1, avg. train loss: 0.56297\n",
      "Step #300, epoch #2, avg. train loss: 0.52502\n",
      "Step #400, epoch #3, avg. train loss: 0.47896\n",
      "Step #500, epoch #4, avg. train loss: 0.45476\n",
      "Step #600, epoch #5, avg. train loss: 0.43957\n",
      "Step #700, epoch #6, avg. train loss: 0.41687\n",
      "Step #800, epoch #7, avg. train loss: 0.39842\n",
      "Step #900, epoch #8, avg. train loss: 0.36307\n",
      "Step #1000, epoch #9, avg. train loss: 0.37075\n",
      "Fold #0: loss=0.511710197288865\n",
      "Step #99, avg. train loss: 0.64651\n",
      "Step #200, epoch #1, avg. train loss: 0.56006\n",
      "Step #300, epoch #2, avg. train loss: 0.51776\n",
      "Step #400, epoch #3, avg. train loss: 0.48333\n",
      "Step #500, epoch #4, avg. train loss: 0.45770\n",
      "Step #600, epoch #5, avg. train loss: 0.43619\n",
      "Step #700, epoch #6, avg. train loss: 0.41256\n",
      "Step #800, epoch #7, avg. train loss: 0.40976\n",
      "Step #900, epoch #8, avg. train loss: 0.38654\n",
      "Step #1000, epoch #9, avg. train loss: 0.38070\n",
      "Fold #1: loss=0.4529770292414818\n",
      "Step #99, avg. train loss: 0.64191\n",
      "Step #200, epoch #1, avg. train loss: 0.55085\n",
      "Step #300, epoch #2, avg. train loss: 0.49990\n",
      "Step #400, epoch #3, avg. train loss: 0.48252\n",
      "Step #500, epoch #4, avg. train loss: 0.45656\n",
      "Step #600, epoch #5, avg. train loss: 0.43025\n",
      "Step #700, epoch #6, avg. train loss: 0.41194\n",
      "Step #800, epoch #7, avg. train loss: 0.39984\n",
      "Step #900, epoch #8, avg. train loss: 0.37599\n",
      "Step #1000, epoch #9, avg. train loss: 0.36624\n",
      "Fold #2: loss=0.5703335135416453\n",
      "Step #99, avg. train loss: 0.65627\n",
      "Step #200, epoch #1, avg. train loss: 0.56519\n",
      "Step #300, epoch #2, avg. train loss: 0.51699\n",
      "Step #400, epoch #3, avg. train loss: 0.48890\n",
      "Step #500, epoch #4, avg. train loss: 0.46332\n",
      "Step #600, epoch #5, avg. train loss: 0.44273\n",
      "Step #700, epoch #6, avg. train loss: 0.43146\n",
      "Step #800, epoch #7, avg. train loss: 0.40715\n",
      "Step #900, epoch #8, avg. train loss: 0.37742\n",
      "Step #1000, epoch #9, avg. train loss: 0.36323\n",
      "Fold #3: loss=0.47423787556265884\n",
      "Step #99, avg. train loss: 0.64852\n",
      "Step #200, epoch #1, avg. train loss: 0.55008\n",
      "Step #300, epoch #2, avg. train loss: 0.51285\n",
      "Step #400, epoch #3, avg. train loss: 0.46731\n",
      "Step #500, epoch #4, avg. train loss: 0.45884\n",
      "Step #600, epoch #5, avg. train loss: 0.43326\n",
      "Step #700, epoch #6, avg. train loss: 0.42539\n",
      "Step #800, epoch #7, avg. train loss: 0.39198\n",
      "Step #900, epoch #8, avg. train loss: 0.38467\n",
      "Step #1000, epoch #9, avg. train loss: 0.35605\n",
      "Fold #4: loss=0.529972735911898\n",
      "Step #99, avg. train loss: 0.65988\n",
      "Step #200, epoch #1, avg. train loss: 0.56384\n",
      "Step #300, epoch #2, avg. train loss: 0.51065\n",
      "Step #400, epoch #3, avg. train loss: 0.46858\n",
      "Step #500, epoch #4, avg. train loss: 0.44578\n",
      "Step #600, epoch #5, avg. train loss: 0.43700\n",
      "Step #700, epoch #6, avg. train loss: 0.41821\n",
      "Step #800, epoch #7, avg. train loss: 0.39525\n",
      "Step #900, epoch #8, avg. train loss: 0.37196\n",
      "Step #1000, epoch #9, avg. train loss: 0.35992\n",
      "Fold #5: loss=0.6099523198018239\n",
      "Step #99, avg. train loss: 0.65619\n",
      "Step #200, epoch #1, avg. train loss: 0.56114\n",
      "Step #300, epoch #2, avg. train loss: 0.52046\n",
      "Step #400, epoch #3, avg. train loss: 0.47379\n",
      "Step #500, epoch #4, avg. train loss: 0.45870\n",
      "Step #600, epoch #5, avg. train loss: 0.45057\n",
      "Step #700, epoch #6, avg. train loss: 0.42079\n",
      "Step #800, epoch #7, avg. train loss: 0.40659\n",
      "Step #900, epoch #8, avg. train loss: 0.38330\n",
      "Step #1000, epoch #9, avg. train loss: 0.36381\n",
      "Fold #6: loss=0.4716374990738397\n",
      "Step #99, avg. train loss: 0.64352\n",
      "Step #200, epoch #1, avg. train loss: 0.54955\n",
      "Step #300, epoch #2, avg. train loss: 0.50862\n",
      "Step #400, epoch #3, avg. train loss: 0.46483\n",
      "Step #500, epoch #4, avg. train loss: 0.44949\n",
      "Step #600, epoch #5, avg. train loss: 0.45279\n",
      "Step #700, epoch #6, avg. train loss: 0.40592\n",
      "Step #800, epoch #7, avg. train loss: 0.40375\n",
      "Step #900, epoch #8, avg. train loss: 0.38168\n",
      "Step #1000, epoch #9, avg. train loss: 0.37432\n",
      "Fold #7: loss=0.533348825125513\n",
      "Step #99, avg. train loss: 0.65841\n",
      "Step #200, epoch #1, avg. train loss: 0.54818\n",
      "Step #300, epoch #2, avg. train loss: 0.50682\n",
      "Step #400, epoch #3, avg. train loss: 0.46721\n",
      "Step #500, epoch #4, avg. train loss: 0.45458\n",
      "Step #600, epoch #5, avg. train loss: 0.45240\n",
      "Step #700, epoch #6, avg. train loss: 0.41547\n",
      "Step #800, epoch #7, avg. train loss: 0.41160\n",
      "Step #900, epoch #8, avg. train loss: 0.37235\n",
      "Step #1000, epoch #9, avg. train loss: 0.37450\n",
      "Fold #8: loss=0.4690331759426469\n",
      "Step #99, avg. train loss: 0.64664\n",
      "Step #200, epoch #1, avg. train loss: 0.56628\n",
      "Step #300, epoch #2, avg. train loss: 0.52381\n",
      "Step #400, epoch #3, avg. train loss: 0.47079\n",
      "Step #500, epoch #4, avg. train loss: 0.45238\n",
      "Step #600, epoch #5, avg. train loss: 0.45895\n",
      "Step #700, epoch #6, avg. train loss: 0.42676\n",
      "Step #800, epoch #7, avg. train loss: 0.41127\n",
      "Step #900, epoch #8, avg. train loss: 0.37519\n",
      "Step #1000, epoch #9, avg. train loss: 0.37630\n",
      "Fold #9: loss=0.6143471605311859\n",
      "TensorFlowDNNClassifier: Mean loss=0.5237550332021559\n",
      "Model: 1 : KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "Fold #0: loss=3.606678388314123\n",
      "Fold #1: loss=2.2197228940978317\n",
      "Fold #2: loss=3.6717523663107237\n",
      "Fold #3: loss=2.5045156203944594\n",
      "Fold #4: loss=4.443553550438037\n",
      "Fold #5: loss=4.410524301688227\n",
      "Fold #6: loss=3.400455469543658\n",
      "Fold #7: loss=3.0885474338547683\n",
      "Fold #8: loss=2.1219335323249253\n",
      "Fold #9: loss=3.0613772690497245\n",
      "KNeighborsClassifier: Mean loss=3.2529060826016476\n",
      "Model: 2 : RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fold #0: loss=0.4656951886598634\n",
      "Fold #1: loss=0.42616738372828994\n",
      "Fold #2: loss=0.5553310672072804\n",
      "Fold #3: loss=0.4158882703480888\n",
      "Fold #4: loss=0.47961750744493736\n",
      "Fold #5: loss=0.4818691257035154\n",
      "Fold #6: loss=0.4077640145387344\n",
      "Fold #7: loss=0.4765886929423487\n",
      "Fold #8: loss=0.4517642970846355\n",
      "Fold #9: loss=0.4680612294852455\n",
      "RandomForestClassifier: Mean loss=0.46287467771429397\n",
      "Model: 3 : RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fold #0: loss=0.4517180829333204\n",
      "Fold #1: loss=0.4288138331194263\n",
      "Fold #2: loss=0.48381057653259435\n",
      "Fold #3: loss=0.4182666851649727\n",
      "Fold #4: loss=0.48153896166957655\n",
      "Fold #5: loss=0.4980753099535686\n",
      "Fold #6: loss=0.40137715472145985\n",
      "Fold #7: loss=0.45485344988166004\n",
      "Fold #8: loss=0.4468679233769468\n",
      "Fold #9: loss=0.46952543963188276\n",
      "RandomForestClassifier: Mean loss=0.4534847416985408\n",
      "Model: 4 : ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Fold #0: loss=0.45496751079363495\n",
      "Fold #1: loss=0.4221988146584036\n",
      "Fold #2: loss=0.5864779075767735\n",
      "Fold #3: loss=0.4135387147555288\n",
      "Fold #4: loss=0.49165543640777337\n",
      "Fold #5: loss=0.5826853127129209\n",
      "Fold #6: loss=0.4992293347545809\n",
      "Fold #7: loss=0.5696776278324083\n",
      "Fold #8: loss=0.5377009614825886\n",
      "Fold #9: loss=0.6233110429105153\n",
      "ExtraTreesClassifier: Mean loss=0.5181442663885129\n",
      "Model: 5 : ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Fold #0: loss=0.44825346440152214\n",
      "Fold #1: loss=0.40736044850901787\n",
      "Fold #2: loss=0.5775460796610716\n",
      "Fold #3: loss=0.42126274280011555\n",
      "Fold #4: loss=0.4949968282719872\n",
      "Fold #5: loss=0.5074636989761913\n",
      "Fold #6: loss=0.4176899941799664\n",
      "Fold #7: loss=0.6431685288404098\n",
      "Fold #8: loss=0.5444145998368657\n",
      "Fold #9: loss=0.46770812815232515\n",
      "ExtraTreesClassifier: Mean loss=0.4929864513629473\n",
      "Model: 6 : GradientBoostingClassifier(init=None, learning_rate=0.05, loss='deviance',\n",
      "              max_depth=6, max_features=None, max_leaf_nodes=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "              presort='auto', random_state=None, subsample=0.5, verbose=0,\n",
      "              warm_start=False)\n",
      "Fold #0: loss=0.4821080713706618\n",
      "Fold #1: loss=0.4637928880214942\n",
      "Fold #2: loss=0.48596775087640226\n",
      "Fold #3: loss=0.4440448889244344\n",
      "Fold #4: loss=0.49922732908471634\n",
      "Fold #5: loss=0.4919547963242652\n",
      "Fold #6: loss=0.44912745206339083\n",
      "Fold #7: loss=0.465104570795673\n",
      "Fold #8: loss=0.4528440085905339\n",
      "Fold #9: loss=0.4699129824000123\n",
      "GradientBoostingClassifier: Mean loss=0.47040847384515844\n",
      "\n",
      "Blending models.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow.contrib.learn as skflow\n",
    "\n",
    "PATH = \"./data/\"\n",
    "SHUFFLE = False\n",
    "FOLDS = 10\n",
    "\n",
    "def mlogloss(y_test, preds):\n",
    "    epsilon = 1e-15\n",
    "    sum = 0\n",
    "    for row in zip(preds,y_test):\n",
    "        x = row[0][row[1]]\n",
    "        x = max(epsilon,x)\n",
    "        x = min(1-epsilon,x)\n",
    "        sum+=math.log(x)\n",
    "    return( (-1/len(preds))*sum)\n",
    "\n",
    "def stretch(y):\n",
    "    return (y - y.min()) / (y.max() - y.min())\n",
    "\n",
    "\n",
    "def blend_ensemble(x, y, x_submit):\n",
    "\n",
    "    folds = list(StratifiedKFold(y, FOLDS))\n",
    "\n",
    "    models = [\n",
    "        skflow.TensorFlowDNNClassifier(hidden_units=[100, 50, 25, 5], n_classes=2, steps=1000),\n",
    "        KNeighborsClassifier(n_neighbors=3),\n",
    "        RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "        RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50)]\n",
    "\n",
    "    dataset_blend_train = np.zeros((x.shape[0], len(models)))\n",
    "    dataset_blend_test = np.zeros((x_submit.shape[0], len(models)))\n",
    "\n",
    "    for j, model in enumerate(models):\n",
    "        print(\"Model: {} : {}\".format(j, model) )\n",
    "        fold_sums = np.zeros((x_submit.shape[0], len(folds)))\n",
    "        total_loss = 0\n",
    "        for i, (train, test) in enumerate(folds):\n",
    "            x_train = x[train]\n",
    "            y_train = y[train]\n",
    "            x_test = x[test]\n",
    "            y_test = y[test]\n",
    "            model.fit(x_train, y_train)\n",
    "            pred = model.predict_proba(x_test)\n",
    "            dataset_blend_train[test, j] = pred[:, 1]\n",
    "            fold_sums[:, i] = model.predict_proba(x_submit)[:, 1]\n",
    "            loss = mlogloss(y_test, pred)\n",
    "            total_loss+=loss\n",
    "            print(\"Fold #{}: loss={}\".format(i,loss))\n",
    "        print(\"{}: Mean loss={}\".format(model.__class__.__name__,total_loss/len(folds)))\n",
    "        dataset_blend_test[:, j] = fold_sums.mean(1)\n",
    "\n",
    "    print()\n",
    "    print(\"Blending models.\")\n",
    "    blend = LogisticRegression()\n",
    "    blend.fit(dataset_blend_train, y)\n",
    "    return blend.predict_proba(dataset_blend_test)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    np.random.seed(42)  # seed to shuffle the train set\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    filename_train = os.path.join(PATH, \"bio_train.csv\")\n",
    "    df_train = pd.read_csv(filename_train, na_values=['NA', '?'])\n",
    "\n",
    "    filename_submit = os.path.join(PATH, \"bio_test.csv\")\n",
    "    df_submit = pd.read_csv(filename_submit, na_values=['NA', '?'])\n",
    "\n",
    "    predictors = list(df_train.columns.values)\n",
    "    predictors.remove('Activity')\n",
    "    x = df_train.as_matrix(predictors)\n",
    "    y = df_train['Activity']\n",
    "    x_submit = df_submit.as_matrix()\n",
    "\n",
    "    if SHUFFLE:\n",
    "        idx = np.random.permutation(y.size)\n",
    "        x = x[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    submit_data = blend_ensemble(x, y, x_submit)\n",
    "    submit_data = stretch(submit_data)\n",
    "\n",
    "    ####################\n",
    "    # Build submit file\n",
    "    ####################\n",
    "    ids = [id+1 for id in range(submit_data.shape[0])]\n",
    "    submit_filename = os.path.join(PATH, \"bio_submit.csv\")\n",
    "    submit_df = pd.DataFrame({'MoleculeId': ids, 'PredictedProbability': submit_data[:, 1]},\n",
    "                             columns=['MoleculeId','PredictedProbability'])\n",
    "    submit_df.to_csv(submit_filename, index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
