{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 13: Advanced/Other Topics**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 13 Video Material\n",
    "\n",
    "* Part 13.1: Flask and Deep Learning Web Services [[Video]]() [[Notebook]](t81_558_class_13_01_flask.ipynb)\n",
    "* **Part 13.2: Deploying a Model to AWS** [[Video]]() [[Notebook]](t81_558_class_13_01_flask.ipynb)\n",
    "* Part 13.3: Using a Keras Deep Neural Network with a Web Application  [[Video]]() [[Notebook]](t81_558_class_13_01_flask.ipynb)\n",
    "* Part 13.4: When to Retrain Your Neural Network [[Video]]() [[Notebook]](t81_558_class_13_01_flask.ipynb)\n",
    "* Part 13.5: AI at the Edge: Using Keras on a Mobile Device  [[Video]]() [[Notebook]](t81_558_class_13_01_flask.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 13.2: Deploying a Model to AWS\n",
    "\n",
    "Some additional material:\n",
    "\n",
    "* [Serving TensorFlow Models](https://www.tensorflow.org/tfx/guide/serving) - Using Google's own deployment server.\n",
    "* [Deploy trained Keras or TensorFlow models using Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker/) - Using AWS to deploy TensorFlow ProtoBuffer models.\n",
    "* [Google ProtoBuf](https://developers.google.com/protocol-buffers/) - The file format used to store neural networks for deployment.\n",
    "\n",
    "# Part 13.2.1: Train Model (optionally, outside of AWS)\n",
    "\n",
    "A portion of this part will need to be run from [AWS SageMaker](https://aws.amazon.com/sagemaker/). To do this you will need to upload this IPYNB (for Module 13.2) to AWS Sage Maker and open it from Jupyter.  This complete process is demonstrated in the above YouTube video.\n",
    "\n",
    "We begin by training a MPG dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0724 06:35:34.837449 140735591678848 deprecation.py:323] From /Users/jheaton/miniconda3/envs/wustl2/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:468: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Apply a constraint manually following the optimizer update step.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 298 samples, validate on 100 samples\n",
      "Epoch 1/1000\n",
      "298/298 - 0s - loss: 96169.3133 - val_loss: 44005.8667\n",
      "Epoch 2/1000\n",
      "298/298 - 0s - loss: 29730.0499 - val_loss: 14304.3897\n",
      "Epoch 3/1000\n",
      "298/298 - 0s - loss: 8551.6825 - val_loss: 2999.6948\n",
      "Epoch 4/1000\n",
      "298/298 - 0s - loss: 1560.0859 - val_loss: 421.5208\n",
      "Epoch 5/1000\n",
      "298/298 - 0s - loss: 233.4629 - val_loss: 148.9569\n",
      "Epoch 6/1000\n",
      "298/298 - 0s - loss: 175.1925 - val_loss: 211.0289\n",
      "Epoch 7/1000\n",
      "298/298 - 0s - loss: 209.2298 - val_loss: 201.0904\n",
      "Epoch 8/1000\n",
      "298/298 - 0s - loss: 181.6304 - val_loss: 162.9355\n",
      "Epoch 9/1000\n",
      "298/298 - 0s - loss: 154.7222 - val_loss: 143.6107\n",
      "Epoch 10/1000\n",
      "298/298 - 0s - loss: 141.9971 - val_loss: 140.6226\n",
      "Epoch 11/1000\n",
      "298/298 - 0s - loss: 140.2950 - val_loss: 140.1436\n",
      "Epoch 12/1000\n",
      "298/298 - 0s - loss: 140.1538 - val_loss: 139.4629\n",
      "Epoch 13/1000\n",
      "298/298 - 0s - loss: 139.2939 - val_loss: 138.5891\n",
      "Epoch 14/1000\n",
      "298/298 - 0s - loss: 138.7829 - val_loss: 137.9281\n",
      "Epoch 15/1000\n",
      "298/298 - 0s - loss: 138.0112 - val_loss: 137.2569\n",
      "Epoch 16/1000\n",
      "298/298 - 0s - loss: 137.1858 - val_loss: 136.4137\n",
      "Epoch 17/1000\n",
      "298/298 - 0s - loss: 136.4365 - val_loss: 135.5442\n",
      "Epoch 18/1000\n",
      "298/298 - 0s - loss: 135.6941 - val_loss: 134.6982\n",
      "Epoch 19/1000\n",
      "298/298 - 0s - loss: 135.1863 - val_loss: 133.9221\n",
      "Epoch 20/1000\n",
      "298/298 - 0s - loss: 134.4470 - val_loss: 133.0586\n",
      "Epoch 21/1000\n",
      "298/298 - 0s - loss: 133.5272 - val_loss: 132.1961\n",
      "Epoch 22/1000\n",
      "298/298 - 0s - loss: 132.9747 - val_loss: 131.3089\n",
      "Epoch 23/1000\n",
      "298/298 - 0s - loss: 132.8194 - val_loss: 130.7285\n",
      "Epoch 24/1000\n",
      "298/298 - 0s - loss: 131.3115 - val_loss: 129.4914\n",
      "Epoch 25/1000\n",
      "298/298 - 0s - loss: 130.4023 - val_loss: 128.5693\n",
      "Epoch 26/1000\n",
      "298/298 - 0s - loss: 129.6186 - val_loss: 127.6323\n",
      "Epoch 27/1000\n",
      "298/298 - 0s - loss: 128.8709 - val_loss: 126.6870\n",
      "Epoch 28/1000\n",
      "298/298 - 0s - loss: 128.0598 - val_loss: 125.7166\n",
      "Epoch 29/1000\n",
      "298/298 - 0s - loss: 126.9567 - val_loss: 124.7495\n",
      "Epoch 30/1000\n",
      "298/298 - 0s - loss: 126.4140 - val_loss: 124.0470\n",
      "Epoch 31/1000\n",
      "298/298 - 0s - loss: 125.5389 - val_loss: 122.8515\n",
      "Epoch 32/1000\n",
      "298/298 - 0s - loss: 124.3524 - val_loss: 121.7516\n",
      "Epoch 33/1000\n",
      "298/298 - 0s - loss: 123.6235 - val_loss: 120.7862\n",
      "Epoch 34/1000\n",
      "298/298 - 0s - loss: 122.8514 - val_loss: 119.7927\n",
      "Epoch 35/1000\n",
      "298/298 - 0s - loss: 121.8307 - val_loss: 118.8023\n",
      "Epoch 36/1000\n",
      "298/298 - 0s - loss: 120.9538 - val_loss: 117.8331\n",
      "Epoch 37/1000\n",
      "298/298 - 0s - loss: 120.1286 - val_loss: 116.7794\n",
      "Epoch 38/1000\n",
      "298/298 - 0s - loss: 119.2669 - val_loss: 115.7353\n",
      "Epoch 39/1000\n",
      "298/298 - 0s - loss: 118.2350 - val_loss: 114.7115\n",
      "Epoch 40/1000\n",
      "298/298 - 0s - loss: 117.3161 - val_loss: 113.7796\n",
      "Epoch 41/1000\n",
      "298/298 - 0s - loss: 116.7505 - val_loss: 112.8219\n",
      "Epoch 42/1000\n",
      "298/298 - 0s - loss: 115.3988 - val_loss: 111.6250\n",
      "Epoch 43/1000\n",
      "298/298 - 0s - loss: 114.7602 - val_loss: 110.6280\n",
      "Epoch 44/1000\n",
      "298/298 - 0s - loss: 114.0441 - val_loss: 109.6257\n",
      "Epoch 45/1000\n",
      "298/298 - 0s - loss: 112.6402 - val_loss: 108.8150\n",
      "Epoch 46/1000\n",
      "298/298 - 0s - loss: 112.6665 - val_loss: 108.2436\n",
      "Epoch 47/1000\n",
      "298/298 - 0s - loss: 111.1822 - val_loss: 106.5975\n",
      "Epoch 48/1000\n",
      "298/298 - 0s - loss: 110.1084 - val_loss: 105.5902\n",
      "Epoch 49/1000\n",
      "298/298 - 0s - loss: 109.3120 - val_loss: 104.6150\n",
      "Epoch 50/1000\n",
      "298/298 - 0s - loss: 108.3553 - val_loss: 103.5795\n",
      "Epoch 51/1000\n",
      "298/298 - 0s - loss: 107.4089 - val_loss: 102.5822\n",
      "Epoch 52/1000\n",
      "298/298 - 0s - loss: 106.7965 - val_loss: 101.5656\n",
      "Epoch 53/1000\n",
      "298/298 - 0s - loss: 105.6867 - val_loss: 100.6201\n",
      "Epoch 54/1000\n",
      "298/298 - 0s - loss: 104.8879 - val_loss: 99.5244\n",
      "Epoch 55/1000\n",
      "298/298 - 0s - loss: 103.8597 - val_loss: 98.5399\n",
      "Epoch 56/1000\n",
      "298/298 - 0s - loss: 103.3480 - val_loss: 97.9022\n",
      "Epoch 57/1000\n",
      "298/298 - 0s - loss: 102.3989 - val_loss: 96.5032\n",
      "Epoch 58/1000\n",
      "298/298 - 0s - loss: 101.3765 - val_loss: 95.5217\n",
      "Epoch 59/1000\n",
      "298/298 - 0s - loss: 100.1436 - val_loss: 94.6376\n",
      "Epoch 60/1000\n",
      "298/298 - 0s - loss: 99.4658 - val_loss: 93.7092\n",
      "Epoch 61/1000\n",
      "298/298 - 0s - loss: 98.4019 - val_loss: 92.5407\n",
      "Epoch 62/1000\n",
      "298/298 - 0s - loss: 97.9879 - val_loss: 91.6283\n",
      "Epoch 63/1000\n",
      "298/298 - 0s - loss: 97.3920 - val_loss: 90.7722\n",
      "Epoch 64/1000\n",
      "298/298 - 0s - loss: 95.9620 - val_loss: 89.8217\n",
      "Epoch 65/1000\n",
      "298/298 - 0s - loss: 95.0065 - val_loss: 88.5913\n",
      "Epoch 66/1000\n",
      "298/298 - 0s - loss: 94.1110 - val_loss: 87.6063\n",
      "Epoch 67/1000\n",
      "298/298 - 0s - loss: 93.2714 - val_loss: 86.6795\n",
      "Epoch 68/1000\n",
      "298/298 - 0s - loss: 92.3634 - val_loss: 85.8082\n",
      "Epoch 69/1000\n",
      "298/298 - 0s - loss: 91.6921 - val_loss: 85.3421\n",
      "Epoch 70/1000\n",
      "298/298 - 0s - loss: 91.0244 - val_loss: 84.0227\n",
      "Epoch 71/1000\n",
      "298/298 - 0s - loss: 89.9330 - val_loss: 82.9361\n",
      "Epoch 72/1000\n",
      "298/298 - 0s - loss: 89.0287 - val_loss: 82.3154\n",
      "Epoch 73/1000\n",
      "298/298 - 0s - loss: 88.2501 - val_loss: 81.1564\n",
      "Epoch 74/1000\n",
      "298/298 - 0s - loss: 87.5660 - val_loss: 80.4342\n",
      "Epoch 75/1000\n",
      "298/298 - 0s - loss: 86.8451 - val_loss: 79.2904\n",
      "Epoch 76/1000\n",
      "298/298 - 0s - loss: 85.9090 - val_loss: 78.7334\n",
      "Epoch 77/1000\n",
      "298/298 - 0s - loss: 85.3198 - val_loss: 78.1127\n",
      "Epoch 78/1000\n",
      "298/298 - 0s - loss: 84.2413 - val_loss: 76.7207\n",
      "Epoch 79/1000\n",
      "298/298 - 0s - loss: 84.8755 - val_loss: 75.8977\n",
      "Epoch 80/1000\n",
      "298/298 - 0s - loss: 82.7789 - val_loss: 75.4343\n",
      "Epoch 81/1000\n",
      "298/298 - 0s - loss: 82.0462 - val_loss: 74.6625\n",
      "Epoch 82/1000\n",
      "298/298 - 0s - loss: 81.3824 - val_loss: 73.6945\n",
      "Epoch 83/1000\n",
      "298/298 - 0s - loss: 80.4300 - val_loss: 72.4273\n",
      "Epoch 84/1000\n",
      "298/298 - 0s - loss: 79.8301 - val_loss: 71.6214\n",
      "Epoch 85/1000\n",
      "298/298 - 0s - loss: 78.9245 - val_loss: 71.5275\n",
      "Epoch 86/1000\n",
      "298/298 - 0s - loss: 78.2938 - val_loss: 70.3975\n",
      "Epoch 87/1000\n",
      "298/298 - 0s - loss: 78.0236 - val_loss: 69.2796\n",
      "Epoch 88/1000\n",
      "298/298 - 0s - loss: 76.8602 - val_loss: 68.7649\n",
      "Epoch 89/1000\n",
      "298/298 - 0s - loss: 76.1301 - val_loss: 67.9554\n",
      "Epoch 90/1000\n",
      "298/298 - 0s - loss: 75.4427 - val_loss: 67.2524\n",
      "Epoch 91/1000\n",
      "298/298 - 0s - loss: 74.7112 - val_loss: 66.4638\n",
      "Epoch 92/1000\n",
      "298/298 - 0s - loss: 74.1411 - val_loss: 65.5642\n",
      "Epoch 93/1000\n",
      "298/298 - 0s - loss: 73.3930 - val_loss: 64.8402\n",
      "Epoch 94/1000\n",
      "298/298 - 0s - loss: 72.7509 - val_loss: 64.2808\n",
      "Epoch 95/1000\n",
      "298/298 - 0s - loss: 72.0434 - val_loss: 63.6095\n",
      "Epoch 96/1000\n",
      "298/298 - 0s - loss: 71.5564 - val_loss: 62.9557\n",
      "Epoch 97/1000\n",
      "298/298 - 0s - loss: 70.7748 - val_loss: 62.6398\n",
      "Epoch 98/1000\n",
      "298/298 - 0s - loss: 70.3864 - val_loss: 61.5286\n",
      "Epoch 99/1000\n",
      "298/298 - 0s - loss: 69.6117 - val_loss: 61.3455\n",
      "Epoch 100/1000\n",
      "298/298 - 0s - loss: 68.8077 - val_loss: 60.0055\n",
      "Epoch 101/1000\n",
      "298/298 - 0s - loss: 68.2548 - val_loss: 59.4675\n",
      "Epoch 102/1000\n",
      "298/298 - 0s - loss: 67.7646 - val_loss: 59.0654\n",
      "Epoch 103/1000\n",
      "298/298 - 0s - loss: 67.0528 - val_loss: 58.1573\n",
      "Epoch 104/1000\n",
      "298/298 - 0s - loss: 67.1679 - val_loss: 57.8892\n",
      "Epoch 105/1000\n",
      "298/298 - 0s - loss: 65.8002 - val_loss: 56.7805\n",
      "Epoch 106/1000\n",
      "298/298 - 0s - loss: 66.2266 - val_loss: 56.1470\n",
      "Epoch 107/1000\n",
      "298/298 - 0s - loss: 64.6340 - val_loss: 56.4390\n",
      "Epoch 108/1000\n",
      "298/298 - 0s - loss: 64.5069 - val_loss: 55.3517\n",
      "Epoch 109/1000\n",
      "298/298 - 0s - loss: 63.7770 - val_loss: 54.9765\n",
      "Epoch 110/1000\n",
      "298/298 - 0s - loss: 63.1651 - val_loss: 54.2588\n",
      "Epoch 111/1000\n",
      "298/298 - 0s - loss: 62.8408 - val_loss: 53.3050\n",
      "Epoch 112/1000\n",
      "298/298 - 0s - loss: 62.1382 - val_loss: 52.9852\n",
      "Epoch 113/1000\n",
      "298/298 - 0s - loss: 62.0983 - val_loss: 53.7151\n",
      "Epoch 114/1000\n",
      "298/298 - 0s - loss: 61.7760 - val_loss: 51.6330\n",
      "Epoch 115/1000\n",
      "298/298 - 0s - loss: 60.6749 - val_loss: 51.4160\n",
      "Epoch 116/1000\n",
      "298/298 - 0s - loss: 59.9620 - val_loss: 51.6653\n",
      "Epoch 117/1000\n",
      "298/298 - 0s - loss: 59.6082 - val_loss: 50.4499\n",
      "Epoch 118/1000\n",
      "298/298 - 0s - loss: 59.0359 - val_loss: 49.7585\n",
      "Epoch 119/1000\n",
      "298/298 - 0s - loss: 58.6217 - val_loss: 49.3466\n",
      "Epoch 120/1000\n",
      "298/298 - 0s - loss: 58.1335 - val_loss: 48.8779\n",
      "Epoch 121/1000\n",
      "298/298 - 0s - loss: 57.6439 - val_loss: 48.4624\n",
      "Epoch 122/1000\n",
      "298/298 - 0s - loss: 57.4007 - val_loss: 47.9809\n",
      "Epoch 123/1000\n",
      "298/298 - 0s - loss: 56.8867 - val_loss: 48.2723\n",
      "Epoch 124/1000\n",
      "298/298 - 0s - loss: 56.5398 - val_loss: 46.9814\n",
      "Epoch 125/1000\n",
      "298/298 - 0s - loss: 55.8993 - val_loss: 46.9878\n",
      "Epoch 126/1000\n",
      "298/298 - 0s - loss: 55.4805 - val_loss: 46.5349\n",
      "Epoch 127/1000\n",
      "298/298 - 0s - loss: 55.0647 - val_loss: 45.6478\n",
      "Epoch 128/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298/298 - 0s - loss: 54.7417 - val_loss: 45.2063\n",
      "Epoch 129/1000\n",
      "298/298 - 0s - loss: 54.2013 - val_loss: 45.4963\n",
      "Epoch 130/1000\n",
      "298/298 - 0s - loss: 54.0407 - val_loss: 44.7755\n",
      "Epoch 131/1000\n",
      "298/298 - 0s - loss: 53.6591 - val_loss: 44.2588\n",
      "Epoch 132/1000\n",
      "298/298 - 0s - loss: 53.1840 - val_loss: 44.6649\n",
      "Epoch 133/1000\n",
      "298/298 - 0s - loss: 52.7958 - val_loss: 43.6509\n",
      "Epoch 134/1000\n",
      "298/298 - 0s - loss: 52.5770 - val_loss: 43.0153\n",
      "Epoch 135/1000\n",
      "298/298 - 0s - loss: 52.0579 - val_loss: 42.8798\n",
      "Epoch 136/1000\n",
      "298/298 - 0s - loss: 51.6900 - val_loss: 43.2019\n",
      "Epoch 137/1000\n",
      "298/298 - 0s - loss: 51.2960 - val_loss: 42.1194\n",
      "Epoch 138/1000\n",
      "298/298 - 0s - loss: 50.9670 - val_loss: 41.8993\n",
      "Epoch 139/1000\n",
      "298/298 - 0s - loss: 50.5513 - val_loss: 42.4590\n",
      "Epoch 140/1000\n",
      "298/298 - 0s - loss: 50.6845 - val_loss: 41.2525\n",
      "Epoch 141/1000\n",
      "298/298 - 0s - loss: 50.3873 - val_loss: 40.4025\n",
      "Epoch 142/1000\n",
      "298/298 - 0s - loss: 50.2109 - val_loss: 41.3965\n",
      "Epoch 143/1000\n",
      "298/298 - 0s - loss: 49.5126 - val_loss: 40.1002\n",
      "Epoch 144/1000\n",
      "298/298 - 0s - loss: 49.5714 - val_loss: 40.5221\n",
      "Epoch 145/1000\n",
      "298/298 - 0s - loss: 48.8018 - val_loss: 39.5517\n",
      "Epoch 146/1000\n",
      "298/298 - 0s - loss: 48.4416 - val_loss: 39.1980\n",
      "Epoch 147/1000\n",
      "298/298 - 0s - loss: 48.1338 - val_loss: 39.4687\n",
      "Epoch 148/1000\n",
      "298/298 - 0s - loss: 47.8296 - val_loss: 39.9741\n",
      "Epoch 149/1000\n",
      "298/298 - 0s - loss: 47.6773 - val_loss: 38.3673\n",
      "Epoch 150/1000\n",
      "298/298 - 0s - loss: 47.2437 - val_loss: 38.4386\n",
      "Epoch 151/1000\n",
      "298/298 - 0s - loss: 47.1220 - val_loss: 38.3232\n",
      "Epoch 152/1000\n",
      "298/298 - 0s - loss: 46.6095 - val_loss: 37.4250\n",
      "Epoch 153/1000\n",
      "298/298 - 0s - loss: 46.5915 - val_loss: 37.4771\n",
      "Epoch 154/1000\n",
      "298/298 - 0s - loss: 47.0068 - val_loss: 38.6349\n",
      "Epoch 155/1000\n",
      "298/298 - 0s - loss: 46.1404 - val_loss: 36.4997\n",
      "Epoch 156/1000\n",
      "298/298 - 0s - loss: 45.7438 - val_loss: 37.3163\n",
      "Epoch 157/1000\n",
      "298/298 - 0s - loss: 45.3595 - val_loss: 36.7228\n",
      "Epoch 158/1000\n",
      "298/298 - 0s - loss: 45.1480 - val_loss: 36.5191\n",
      "Epoch 159/1000\n",
      "298/298 - 0s - loss: 45.0707 - val_loss: 36.4415\n",
      "Epoch 160/1000\n",
      "298/298 - 0s - loss: 45.1347 - val_loss: 35.4159\n",
      "Epoch 161/1000\n",
      "298/298 - 0s - loss: 44.5629 - val_loss: 36.7198\n",
      "Epoch 162/1000\n",
      "298/298 - 0s - loss: 44.1827 - val_loss: 35.2556\n",
      "Epoch 163/1000\n",
      "298/298 - 0s - loss: 44.0222 - val_loss: 35.5289\n",
      "Epoch 164/1000\n",
      "298/298 - 0s - loss: 43.7264 - val_loss: 35.5794\n",
      "Epoch 165/1000\n",
      "298/298 - 0s - loss: 43.8281 - val_loss: 35.4502\n",
      "Epoch 166/1000\n",
      "298/298 - 0s - loss: 43.2953 - val_loss: 35.1415\n",
      "Epoch 167/1000\n",
      "298/298 - 0s - loss: 43.0991 - val_loss: 34.4187\n",
      "Epoch 168/1000\n",
      "298/298 - 0s - loss: 42.9728 - val_loss: 34.1197\n",
      "Epoch 169/1000\n",
      "298/298 - 0s - loss: 42.6850 - val_loss: 34.3920\n",
      "Epoch 170/1000\n",
      "298/298 - 0s - loss: 42.5630 - val_loss: 34.9282\n",
      "Epoch 171/1000\n",
      "298/298 - 0s - loss: 42.2601 - val_loss: 33.5277\n",
      "Epoch 172/1000\n",
      "298/298 - 0s - loss: 42.0867 - val_loss: 33.8544\n",
      "Epoch 173/1000\n",
      "298/298 - 0s - loss: 41.8442 - val_loss: 33.9507\n",
      "Epoch 174/1000\n",
      "298/298 - 0s - loss: 41.6960 - val_loss: 33.4290\n",
      "Epoch 175/1000\n",
      "298/298 - 0s - loss: 41.4229 - val_loss: 32.6806\n",
      "Epoch 176/1000\n",
      "298/298 - 0s - loss: 41.3960 - val_loss: 32.9551\n",
      "Epoch 177/1000\n",
      "298/298 - 0s - loss: 41.1540 - val_loss: 33.4306\n",
      "Epoch 178/1000\n",
      "298/298 - 0s - loss: 41.5311 - val_loss: 32.4850\n",
      "Epoch 179/1000\n",
      "298/298 - 0s - loss: 40.8892 - val_loss: 33.9155\n",
      "Epoch 180/1000\n",
      "298/298 - 0s - loss: 40.4831 - val_loss: 32.1132\n",
      "Epoch 181/1000\n",
      "298/298 - 0s - loss: 40.4116 - val_loss: 32.5184\n",
      "Epoch 182/1000\n",
      "298/298 - 0s - loss: 40.6501 - val_loss: 32.3681\n",
      "Epoch 183/1000\n",
      "298/298 - 0s - loss: 40.0555 - val_loss: 31.5034\n",
      "Epoch 184/1000\n",
      "298/298 - 0s - loss: 39.8129 - val_loss: 32.2885\n",
      "Epoch 185/1000\n",
      "298/298 - 0s - loss: 39.6483 - val_loss: 32.0342\n",
      "Epoch 186/1000\n",
      "298/298 - 0s - loss: 39.5699 - val_loss: 31.6272\n",
      "Epoch 187/1000\n",
      "298/298 - 0s - loss: 39.4671 - val_loss: 31.2539\n",
      "Epoch 188/1000\n",
      "298/298 - 0s - loss: 39.5534 - val_loss: 32.0038\n",
      "Epoch 189/1000\n",
      "298/298 - 0s - loss: 38.9004 - val_loss: 31.0145\n",
      "Epoch 190/1000\n",
      "298/298 - 0s - loss: 38.8868 - val_loss: 31.5424\n",
      "Epoch 191/1000\n",
      "298/298 - 0s - loss: 38.6761 - val_loss: 30.9826\n",
      "Epoch 192/1000\n",
      "298/298 - 0s - loss: 39.0015 - val_loss: 30.4797\n",
      "Epoch 193/1000\n",
      "298/298 - 0s - loss: 39.2724 - val_loss: 31.6848\n",
      "Epoch 194/1000\n",
      "298/298 - 0s - loss: 38.6020 - val_loss: 29.8857\n",
      "Epoch 195/1000\n",
      "298/298 - 0s - loss: 38.0602 - val_loss: 31.9946\n",
      "Epoch 196/1000\n",
      "298/298 - 0s - loss: 37.9654 - val_loss: 29.9972\n",
      "Epoch 197/1000\n",
      "298/298 - 0s - loss: 38.2388 - val_loss: 30.3426\n",
      "Epoch 198/1000\n",
      "298/298 - 0s - loss: 37.6491 - val_loss: 30.4051\n",
      "Epoch 199/1000\n",
      "Restoring model weights from the end of the best epoch.\n",
      "298/298 - 0s - loss: 37.5311 - val_loss: 30.2545\n",
      "Epoch 00199: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x135591940>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\", \n",
    "    na_values=['NA', '?'])\n",
    "\n",
    "cars = df['name']\n",
    "\n",
    "# Handle missing value\n",
    "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n",
    "\n",
    "# Pandas to Numpy\n",
    "x = df[['cylinders', 'displacement', 'horsepower', 'weight',\n",
    "       'acceleration', 'year', 'origin']].values\n",
    "y = df['mpg'].values # regression\n",
    "\n",
    "# Split into validation and training sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Build the neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "model.add(Dense(10, activation='relu')) # Hidden 2\n",
    "model.add(Dense(1)) # Output\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto',\n",
    "        restore_best_weights=True)\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we evaluate the RMSE.  The goal is more to show how to create a cloud API than to achieve a really low RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Score: 5.466784454703228\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_test)\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(f\"RMSE Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we save the weights and structure of the neural network, as was demonstrated earlier in this course.  These two files are used to generate a ProtoBuf file that is used for the actual deployment.  We store it to two separate files because we ONLY want the structure and weights.  A single MD5 file, such as model.save(...) also contains training paramaters and other features that may cause version issues when uploading to AWS.  Remember, AWS may have a different version for TensorFlow than you do locally.  Usually AWS will have an older version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./dnn/\"\n",
    "\n",
    "model.save_weights(os.path.join(save_path,\"mpg_model-weights.h5\"))\n",
    "\n",
    "# save neural network structure to JSON (no weights)\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(save_path,\"mpg_model.json\"), \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will upload the two files generated to the **./dnn/** folder to AWS.  If you running the entire process from  AWS, then they will not need to be uploaded.\n",
    "\n",
    "We also print out the values to one car, we will copy these later when we test the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.0, 307.0, 130.0, 3504.0, 12.0, 70.0, 1.0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 13.2.2: Train Model (must use AWS SageMaker Notebook)\n",
    "\n",
    "To complete this portion you will need to be running from s Jupyter notebook on AWS SageMaker.  The following is based on an example from AWS documentation, but customized to this class example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Set up\n",
    "\n",
    "In the AWS Management Console, go to the Amazon SageMaker console. Choose Notebook Instances, and create a new notebook instance. Upload this notebook and set the kernel to conda_tensorflow_p36.\n",
    "\n",
    "The get_execution_role function retrieves the AWS Identity and Access Management (IAM) role you created at the time of creating your notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Load the Keras model using the json and weights file\n",
    "\n",
    "The following cell loads the necessary imports from AWS.  Note that we using \"import keras\" compared to the \"import keras.tensorflow\" advised for the rest of the course.  This is advised by AWS currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory called keras_model, navigate to keras_model from the Jupyter notebook home, and upload the model.json and model-weights.h5 files (using the \"Upload\" menu on the Jupyter notebook home)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir keras_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to keras_model from the Jupyter notebook home, and upload your model.json and model-weights.h5 files (using the \"Upload\" menu on the Jupyter notebook home). Use the files that you generated in step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpg_model.json\tmpg_model-weightd.h5\r\n"
     ]
    }
   ],
   "source": [
    "!ls keras_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you've uploaded your model to the directory by this point.  If you saw no files at the above step, upload your files and rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "json_file = open('/home/ec2-user/SageMaker/keras_model/'+'mpg_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json,custom_objects={\"GlorotUniform\": tf.keras.initializers.glorot_uniform})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "loaded_model.load_weights('/home/ec2-user/SageMaker/keras_model/mpg_model-weights.h5')\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Export the Keras model to the TensorFlow ProtoBuf format (must use AWS SageMaker Notebook)\n",
    "\n",
    "As you are probably noticing there are many ways to save a Keras neural network.  So far we've seen:\n",
    "\n",
    "* YAML File - Structure only\n",
    "* JSON File - Structure only\n",
    "* H5 Complete Model\n",
    "* H5 Weights only\n",
    "\n",
    "There is actually a fifth, which is the ProtoBuf format.  ProtoBuf is typically only used for deployment.  We will now convert the model we just loaded into this format.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.saved_model import builder\n",
    "from tensorflow.python.saved_model.signature_def_utils import predict_signature_def\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "# Note: This directory structure will need to be followed - see notes for the next section\n",
    "model_version = '1'\n",
    "export_dir = 'export/Servo/' + model_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important that this export directory be empty.  Be careful, the following command deletes the entire expor directory. (this should be fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Protocol Buffer SavedModel at 'export_dir'\n",
    "build = builder.SavedModelBuilder(export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    }
   ],
   "source": [
    "# Create prediction signature to be used by TensorFlow Serving Predict API\n",
    "signature = predict_signature_def(\n",
    "    inputs={\"inputs\": loaded_model.input}, outputs={\"score\": loaded_model.output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: export/Servo/1/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "with K.get_session() as sess:\n",
    "    # Save the meta graph and variables\n",
    "    build.add_meta_graph_and_variables(\n",
    "        sess=sess, tags=[tag_constants.SERVING], signature_def_map={\"serving_default\": signature})\n",
    "    build.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notive the **signature_def_map** this bridges any incompatabilities between the version of TensorFlow you were running locally and the AWS version.  You might need to add additional entries here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Convert TensorFlow model to a SageMaker readable format (must use AWS SageMaker Notebook)\n",
    "\n",
    "Move the TensorFlow exported model into a directory export\\Servo. SageMaker will recognize this as a loadable TensorFlow model. Your directory and file structure should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Servo\r\n"
     ]
    }
   ],
   "source": [
    "!ls export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\r\n"
     ]
    }
   ],
   "source": [
    "!ls export/Servo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables.data-00000-of-00001  variables.index\r\n"
     ]
    }
   ],
   "source": [
    "!ls export/Servo/1/variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Tar the entire directory and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "with tarfile.open('model.tar.gz', mode='w:gz') as archive:\n",
    "    archive.add('export', recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload TAR file to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "inputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Deploy the trained model (must use AWS SageMaker Notebook)\n",
    "\n",
    "The entry_point file \"train.py\" can be an empty Python file. The requirement will be removed at a later date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Python 2 tensorflow images will be soon deprecated and may not be supported for newer upcoming versions of the tensorflow images.\n",
      "Please set the argument \"py_version='py3'\" to use the Python 3 tensorflow image.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "sagemaker_model = TensorFlowModel(model_data = 's3://' + sagemaker_session.default_bucket() + '/model/model.tar.gz',\n",
    "                                  role = role,\n",
    "                                  framework_version = '1.12',\n",
    "                                  entry_point = 'train.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the following command cake take 5-8 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------!CPU times: user 480 ms, sys: 35.4 ms, total: 516 ms\n",
      "Wall time: 6min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictor = sagemaker_model.deploy(initial_instance_count=1,\n",
    "                                   instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-tensorflow-2019-07-24-11-56-34-214'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You will need to update the endpoint in the command below with the endpoint name from the output of the previous cell (e.g. sagemaker-tensorflow-2019-07-24-01-47-19-895)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 13.2.3: Test Model Deployment (optionally, outside of AWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "endpoint_name = 'sagemaker-tensorflow-2019-07-24-11-56-34-214' # see above, must be set to current value\n",
    "\n",
    "# Pick one of the following two cells to run based on how you will access..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you access the API from outside of AWS SageMaker notebooks you must authenticate and specify region...\n",
    "# (do not run both this cell and the next)\n",
    "\n",
    "client = boto3.client('runtime.sagemaker', \n",
    "    region_name='us-east-1', # make sure to set correct region\n",
    "    aws_access_key_id='AKIAYKSSG3L5OM4XUG7Q', # These you get from AWS, for your account\n",
    "    aws_secret_access_key='k7JBe+y78XQUdJLt8WaY6TgMd6dy3fZJ3K/Mcnf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you access from inside AWS in a notebook (do not run both this cell and the previous)\n",
    "client = boto3.client('runtime.sagemaker', region_name='us-east-1') # make sure to set correct region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (AccessDeniedException) when calling the InvokeEndpoint operation: User: arn:aws:iam::572476807930:user/wustl-util is not authorized to perform: sagemaker:InvokeEndpoint on resource: arn:aws:sagemaker:us-east-1:572476807930:endpoint/sagemaker-tensorflow-2019-07-24-11-56-34-214",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b19c5d17586c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m307\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m130\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3504\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mresponse_body\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_body\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/wustl2/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/wustl2/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (AccessDeniedException) when calling the InvokeEndpoint operation: User: arn:aws:iam::572476807930:user/wustl-util is not authorized to perform: sagemaker:InvokeEndpoint on resource: arn:aws:sagemaker:us-east-1:572476807930:endpoint/sagemaker-tensorflow-2019-07-24-11-56-34-214"
     ]
    }
   ],
   "source": [
    "# Create a car based on one of the cars captured at beginning of this part.\n",
    "data = [[8,307,130,3504,12,70,1]]\n",
    "\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, Body=json.dumps(data))\n",
    "response_body = response['Body']\n",
    "print(response_body.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:  Make sure to turn off any AWS resources you started to run this.  If you leave them running, they will continue billing you.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "rga"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
