{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 8: Kaggle Data Sets**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8 Material\n",
    "\n",
    "* Part 8.1: Introduction to Kaggle [[Video]](https://www.youtube.com/watch?v=v4lJBhdCuCU&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_08_1_kaggle_intro.ipynb)\n",
    "* Part 8.2: Building Ensembles with Scikit-Learn and Keras [[Video]](https://www.youtube.com/watch?v=LQ-9ZRBLasw&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_08_2_keras_ensembles.ipynb)\n",
    "* Part 8.3: How Should you Architect Your Keras Neural Network: Hyperparameters [[Video]](https://www.youtube.com/watch?v=1q9klwSoUQw&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_08_3_keras_hyperparameters.ipynb)\n",
    "* **Part 8.4: Bayesian Hyperparameter Optimization for Keras** [[Video]](https://www.youtube.com/watch?v=sXdxyUCCm8s&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_08_4_bayesian_hyperparameter_opt.ipynb)\n",
    "* Part 8.5: Current Semester's Kaggle [[Video]](https://www.youtube.com/watch?v=48OrNYYey5E) [[Notebook]](t81_558_class_08_5_kaggle_project.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8.4: Bayesian Hyperparameter Optimization for Keras\n",
    "\n",
    "Snoek, J., Larochelle, H., & Adams, R. P. (2012). [Practical bayesian optimization of machine learning algorithms](https://arxiv.org/pdf/1206.2944.pdf). In *Advances in neural information processing systems* (pp. 2951-2959).\n",
    "\n",
    "\n",
    "* [bayesian-optimization](https://github.com/fmfn/BayesianOptimization)\n",
    "* [hyperopt](https://github.com/hyperopt/hyperopt)\n",
    "* [spearmint](https://github.com/JasperSnoek/spearmint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore useless W0819 warnings generated by TensorFlow 2.0.  Hopefully can remove this ignore in the future.\n",
    "# See https://github.com/tensorflow/tensorflow/issues/31308\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Read the data set\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Generate dummies for job\n",
    "df = pd.concat([df,pd.get_dummies(df['job'],prefix=\"job\")],axis=1)\n",
    "df.drop('job', axis=1, inplace=True)\n",
    "\n",
    "# Generate dummies for area\n",
    "df = pd.concat([df,pd.get_dummies(df['area'],prefix=\"area\")],axis=1)\n",
    "df.drop('area', axis=1, inplace=True)\n",
    "\n",
    "# Missing values for income\n",
    "med = df['income'].median()\n",
    "df['income'] = df['income'].fillna(med)\n",
    "\n",
    "# Standardize ranges\n",
    "df['income'] = zscore(df['income'])\n",
    "df['aspect'] = zscore(df['aspect'])\n",
    "df['save_rate'] = zscore(df['save_rate'])\n",
    "df['age'] = zscore(df['age'])\n",
    "df['subscriptions'] = zscore(df['subscriptions'])\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "x_columns = df.columns.drop('product').drop('id')\n",
    "x = df[x_columns].values\n",
    "dummies = pd.get_dummies(df['product']) # Classification\n",
    "products = dummies.columns\n",
    "y = dummies.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6788914008893789\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow.keras.initializers\n",
    "import statistics\n",
    "import tensorflow.keras\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tensorflow.keras.layers import LeakyReLU,PReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_network(dropout,lr,neuronPct,neuronShrink):\n",
    "    SPLITS = 2\n",
    "\n",
    "    # Bootstrap\n",
    "    boot = StratifiedShuffleSplit(n_splits=SPLITS, test_size=0.1)\n",
    "\n",
    "    # Track progress\n",
    "    mean_benchmark = []\n",
    "    epochs_needed = []\n",
    "    num = 0\n",
    "    \n",
    "\n",
    "    # Loop through samples\n",
    "    for train, test in boot.split(x,df['product']):\n",
    "        neuronCount = int(neuronPct * 5000)\n",
    "        start_time = time.time()\n",
    "        num+=1\n",
    "\n",
    "        # Split train and test\n",
    "        x_train = x[train]\n",
    "        y_train = y[train]\n",
    "        x_test = x[test]\n",
    "        y_test = y[test]\n",
    "\n",
    "        # Construct neural network\n",
    "        # kernel_initializer = tensorflow.keras.initializers.he_uniform(seed=None)\n",
    "        model = Sequential()\n",
    "        \n",
    "        layer = 0\n",
    "        while neuronCount>25 and layer<10:\n",
    "            #print(neuronCount)\n",
    "            if layer==0:\n",
    "                model.add(Dense(neuronCount, \n",
    "                    input_dim=x.shape[1], \n",
    "                    activation=PReLU())) \n",
    "            else:\n",
    "                model.add(Dense(neuronCount, activation=PReLU())) \n",
    "            model.add(Dropout(dropout))\n",
    "        \n",
    "            neuronCount = neuronCount * neuronShrink\n",
    "        \n",
    "        model.add(Dense(y.shape[1],activation='softmax')) # Output\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=lr))\n",
    "        monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
    "            patience=100, verbose=0, mode='auto', restore_best_weights=True)\n",
    "\n",
    "        # Train on the bootstrap sample\n",
    "        model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=0,epochs=1000)\n",
    "        epochs = monitor.stopped_epoch\n",
    "        epochs_needed.append(epochs)\n",
    "\n",
    "        # Predict on the out of boot (validation)\n",
    "        pred = model.predict(x_test)\n",
    "\n",
    "        # Measure this bootstrap's log loss\n",
    "        y_compare = np.argmax(y_test,axis=1) # For log loss calculation\n",
    "        score = metrics.log_loss(y_compare, pred)\n",
    "        mean_benchmark.append(score)\n",
    "        m1 = statistics.mean(mean_benchmark)\n",
    "        m2 = statistics.mean(epochs_needed)\n",
    "        mdev = statistics.pstdev(mean_benchmark)\n",
    "\n",
    "        # Record this iteration\n",
    "        time_took = time.time() - start_time\n",
    "        #print(f\"#{num}: score={score:.6f}, mean score={m1:.6f}, stdev={mdev:.6f}, epochs={epochs}, mean epochs={int(m2)}, time={hms_string(time_took)}\")\n",
    "\n",
    "    tensorflow.keras.backend.clear_session()\n",
    "    return (-m1)\n",
    "\n",
    "print(evaluate_network(\n",
    "    dropout=0.2,\n",
    "    lr=1e-3,\n",
    "    neuronPct=0.2,\n",
    "    neuronShrink=0.2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |  dropout  |    lr     | neuronPct | neuron... |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.7819  \u001b[0m | \u001b[0m 0.2081  \u001b[0m | \u001b[0m 0.07203 \u001b[0m | \u001b[0m 0.01011 \u001b[0m | \u001b[0m 0.3093  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-0.7311  \u001b[0m | \u001b[95m 0.07323 \u001b[0m | \u001b[95m 0.009234\u001b[0m | \u001b[95m 0.1944  \u001b[0m | \u001b[95m 0.3521  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-17.7    \u001b[0m | \u001b[0m 0.198   \u001b[0m | \u001b[0m 0.05388 \u001b[0m | \u001b[0m 0.425   \u001b[0m | \u001b[0m 0.6884  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.7534  \u001b[0m | \u001b[0m 0.102   \u001b[0m | \u001b[0m 0.08781 \u001b[0m | \u001b[0m 0.03711 \u001b[0m | \u001b[0m 0.6738  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.7598  \u001b[0m | \u001b[0m 0.2082  \u001b[0m | \u001b[0m 0.05587 \u001b[0m | \u001b[0m 0.149   \u001b[0m | \u001b[0m 0.2061  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-19.86   \u001b[0m | \u001b[0m 0.3996  \u001b[0m | \u001b[0m 0.09683 \u001b[0m | \u001b[0m 0.3203  \u001b[0m | \u001b[0m 0.6954  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-5.358   \u001b[0m | \u001b[0m 0.4373  \u001b[0m | \u001b[0m 0.08946 \u001b[0m | \u001b[0m 0.09419 \u001b[0m | \u001b[0m 0.04866 \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-2.075   \u001b[0m | \u001b[0m 0.08475 \u001b[0m | \u001b[0m 0.08781 \u001b[0m | \u001b[0m 0.1074  \u001b[0m | \u001b[0m 0.4269  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-3.323   \u001b[0m | \u001b[0m 0.478   \u001b[0m | \u001b[0m 0.05332 \u001b[0m | \u001b[0m 0.695   \u001b[0m | \u001b[0m 0.3224  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-1.537   \u001b[0m | \u001b[0m 0.3426  \u001b[0m | \u001b[0m 0.08346 \u001b[0m | \u001b[0m 0.02811 \u001b[0m | \u001b[0m 0.7526  \u001b[0m |\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m-0.65    \u001b[0m | \u001b[95m 0.1277  \u001b[0m | \u001b[95m 0.007401\u001b[0m | \u001b[95m 0.1077  \u001b[0m | \u001b[95m 0.2785  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.8128  \u001b[0m | \u001b[0m 0.2039  \u001b[0m | \u001b[0m 0.07792 \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.716   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.7538  \u001b[0m | \u001b[0m 0.1015  \u001b[0m | \u001b[0m 0.07812 \u001b[0m | \u001b[0m 0.1825  \u001b[0m | \u001b[0m 0.2574  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-0.7628  \u001b[0m | \u001b[0m 0.07214 \u001b[0m | \u001b[0m 0.01028 \u001b[0m | \u001b[0m 0.1954  \u001b[0m | \u001b[0m 0.3486  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.7238  \u001b[0m | \u001b[0m 0.1895  \u001b[0m | \u001b[0m 0.03047 \u001b[0m | \u001b[0m 0.1534  \u001b[0m | \u001b[0m 0.3196  \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-2.84    \u001b[0m | \u001b[0m 0.1145  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.615   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-0.7442  \u001b[0m | \u001b[0m 0.09211 \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.773   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-0.8384  \u001b[0m | \u001b[0m 0.1615  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.08481 \u001b[0m | \u001b[0m 0.2485  \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-1.01    \u001b[0m | \u001b[0m 0.2467  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.8288  \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-0.7332  \u001b[0m | \u001b[0m 0.003189\u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.6925  \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-2.243   \u001b[0m | \u001b[0m 0.1546  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.2273  \u001b[0m | \u001b[0m 0.2163  \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-2.428   \u001b[0m | \u001b[0m 0.2419  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.04784 \u001b[0m | \u001b[0m 0.263   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-0.8643  \u001b[0m | \u001b[0m 0.1945  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.174   \u001b[0m | \u001b[0m 0.2627  \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-2.233   \u001b[0m | \u001b[0m 0.1039  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.1152  \u001b[0m | \u001b[0m 0.356   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-0.7683  \u001b[0m | \u001b[0m 0.09686 \u001b[0m | \u001b[0m 0.03129 \u001b[0m | \u001b[0m 0.09223 \u001b[0m | \u001b[0m 0.181   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-0.7257  \u001b[0m | \u001b[0m 0.01078 \u001b[0m | \u001b[0m 0.001232\u001b[0m | \u001b[0m 0.2392  \u001b[0m | \u001b[0m 0.4175  \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-0.8061  \u001b[0m | \u001b[0m 0.0886  \u001b[0m | \u001b[0m 0.05635 \u001b[0m | \u001b[0m 0.2383  \u001b[0m | \u001b[0m 0.3894  \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-0.744   \u001b[0m | \u001b[0m 0.2557  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.3904  \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-0.7967  \u001b[0m | \u001b[0m 0.1486  \u001b[0m | \u001b[0m 0.04388 \u001b[0m | \u001b[0m 0.1331  \u001b[0m | \u001b[0m 0.2425  \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m-0.8514  \u001b[0m | \u001b[0m 0.02109 \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.2158  \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-1.446   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.1363  \u001b[0m | \u001b[0m 0.1771  \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m-5.014   \u001b[0m | \u001b[0m 0.05409 \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.2097  \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-0.7734  \u001b[0m | \u001b[0m 0.1255  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.111   \u001b[0m | \u001b[0m 0.118   \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m-0.8292  \u001b[0m | \u001b[0m 0.06519 \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.09093 \u001b[0m | \u001b[0m 0.2395  \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m-0.8161  \u001b[0m | \u001b[0m 0.2445  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.08933 \u001b[0m | \u001b[0m 0.3353  \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-13.75   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.08886 \u001b[0m | \u001b[0m 0.7506  \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m-0.8015  \u001b[0m | \u001b[0m 0.1113  \u001b[0m | \u001b[0m 0.08568 \u001b[0m | \u001b[0m 0.1196  \u001b[0m | \u001b[0m 0.1883  \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m-0.7406  \u001b[0m | \u001b[0m 0.132   \u001b[0m | \u001b[0m 0.09355 \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.7276  \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m-0.8567  \u001b[0m | \u001b[0m 0.182   \u001b[0m | \u001b[0m 0.06748 \u001b[0m | \u001b[0m 0.1062  \u001b[0m | \u001b[0m 0.2998  \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m-0.718   \u001b[0m | \u001b[0m 0.04281 \u001b[0m | \u001b[0m 0.0999  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.6614  \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m-0.8214  \u001b[0m | \u001b[0m 0.1324  \u001b[0m | \u001b[0m 0.0552  \u001b[0m | \u001b[0m 0.1609  \u001b[0m | \u001b[0m 0.3033  \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m-0.7903  \u001b[0m | \u001b[0m 0.08639 \u001b[0m | \u001b[0m 0.04873 \u001b[0m | \u001b[0m 0.1257  \u001b[0m | \u001b[0m 0.2462  \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m-0.806   \u001b[0m | \u001b[0m 0.2147  \u001b[0m | \u001b[0m 0.09282 \u001b[0m | \u001b[0m 0.04002 \u001b[0m | \u001b[0m 0.3558  \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m-0.7886  \u001b[0m | \u001b[0m 0.04016 \u001b[0m | \u001b[0m 0.03324 \u001b[0m | \u001b[0m 0.2129  \u001b[0m | \u001b[0m 0.3899  \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m-0.8681  \u001b[0m | \u001b[0m 0.1589  \u001b[0m | \u001b[0m 0.05766 \u001b[0m | \u001b[0m 0.1022  \u001b[0m | \u001b[0m 0.1666  \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m-2.063   \u001b[0m | \u001b[0m 0.06768 \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.2469  \u001b[0m | \u001b[0m 0.3865  \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m-0.7352  \u001b[0m | \u001b[0m 0.1222  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.1313  \u001b[0m | \u001b[0m 0.2638  \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m-0.7369  \u001b[0m | \u001b[0m 0.08774 \u001b[0m | \u001b[0m 0.0658  \u001b[0m | \u001b[0m 0.1902  \u001b[0m | \u001b[0m 0.3597  \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m-0.7593  \u001b[0m | \u001b[0m 0.1601  \u001b[0m | \u001b[0m 0.09453 \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.7852  \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m-0.7381  \u001b[0m | \u001b[0m 0.262   \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.02257 \u001b[0m | \u001b[0m 0.3287  \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m-0.8728  \u001b[0m | \u001b[0m 0.1716  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.1445  \u001b[0m | \u001b[0m 0.2058  \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m-0.7756  \u001b[0m | \u001b[0m 0.07649 \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.0701  \u001b[0m | \u001b[0m 0.1657  \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m-0.7975  \u001b[0m | \u001b[0m 0.08264 \u001b[0m | \u001b[0m 0.05783 \u001b[0m | \u001b[0m 0.1191  \u001b[0m | \u001b[0m 0.1348  \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m-0.8423  \u001b[0m | \u001b[0m 0.2293  \u001b[0m | \u001b[0m 0.09192 \u001b[0m | \u001b[0m 0.1154  \u001b[0m | \u001b[0m 0.2532  \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m-0.7271  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.08457 \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.6459  \u001b[0m |\n",
      "| \u001b[0m 56      \u001b[0m | \u001b[0m-0.7559  \u001b[0m | \u001b[0m 0.1521  \u001b[0m | \u001b[0m 0.0526  \u001b[0m | \u001b[0m 0.2113  \u001b[0m | \u001b[0m 0.3552  \u001b[0m |\n",
      "| \u001b[0m 57      \u001b[0m | \u001b[0m-0.8829  \u001b[0m | \u001b[0m 0.196   \u001b[0m | \u001b[0m 0.09467 \u001b[0m | \u001b[0m 0.1641  \u001b[0m | \u001b[0m 0.3414  \u001b[0m |\n",
      "| \u001b[0m 58      \u001b[0m | \u001b[0m-1.052   \u001b[0m | \u001b[0m 0.1238  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.2244  \u001b[0m | \u001b[0m 0.3177  \u001b[0m |\n",
      "| \u001b[0m 59      \u001b[0m | \u001b[0m-0.8405  \u001b[0m | \u001b[0m 0.2466  \u001b[0m | \u001b[0m 0.05903 \u001b[0m | \u001b[0m 0.1641  \u001b[0m | \u001b[0m 0.2964  \u001b[0m |\n",
      "| \u001b[0m 60      \u001b[0m | \u001b[0m-3.56    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.05098 \u001b[0m | \u001b[0m 0.2208  \u001b[0m | \u001b[0m 0.447   \u001b[0m |\n",
      "| \u001b[0m 61      \u001b[0m | \u001b[0m-0.7652  \u001b[0m | \u001b[0m 0.2598  \u001b[0m | \u001b[0m 0.05267 \u001b[0m | \u001b[0m 0.04168 \u001b[0m | \u001b[0m 0.3629  \u001b[0m |\n",
      "| \u001b[0m 62      \u001b[0m | \u001b[0m-2.2     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.2057  \u001b[0m | \u001b[0m 0.368   \u001b[0m |\n",
      "| \u001b[0m 63      \u001b[0m | \u001b[0m-0.8408  \u001b[0m | \u001b[0m 0.1913  \u001b[0m | \u001b[0m 0.05598 \u001b[0m | \u001b[0m 0.2062  \u001b[0m | \u001b[0m 0.3021  \u001b[0m |\n",
      "| \u001b[0m 64      \u001b[0m | \u001b[0m-0.7768  \u001b[0m | \u001b[0m 0.13    \u001b[0m | \u001b[0m 0.04662 \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.7657  \u001b[0m |\n",
      "| \u001b[0m 65      \u001b[0m | \u001b[0m-0.7419  \u001b[0m | \u001b[0m 0.2127  \u001b[0m | \u001b[0m 0.05054 \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.7769  \u001b[0m |\n",
      "| \u001b[0m 66      \u001b[0m | \u001b[0m-0.6951  \u001b[0m | \u001b[0m 0.1793  \u001b[0m | \u001b[0m 0.06806 \u001b[0m | \u001b[0m 0.05702 \u001b[0m | \u001b[0m 0.7501  \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 67      \u001b[0m | \u001b[0m-8.185   \u001b[0m | \u001b[0m 0.2322  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.04204 \u001b[0m | \u001b[0m 0.7642  \u001b[0m |\n",
      "| \u001b[0m 68      \u001b[0m | \u001b[0m-0.7321  \u001b[0m | \u001b[0m 0.1735  \u001b[0m | \u001b[0m 0.04985 \u001b[0m | \u001b[0m 0.01545 \u001b[0m | \u001b[0m 0.7418  \u001b[0m |\n",
      "| \u001b[0m 69      \u001b[0m | \u001b[0m-0.7174  \u001b[0m | \u001b[0m 0.1397  \u001b[0m | \u001b[0m 0.07448 \u001b[0m | \u001b[0m 0.04489 \u001b[0m | \u001b[0m 0.7575  \u001b[0m |\n",
      "| \u001b[0m 70      \u001b[0m | \u001b[0m-0.7855  \u001b[0m | \u001b[0m 0.1958  \u001b[0m | \u001b[0m 0.0584  \u001b[0m | \u001b[0m 0.1492  \u001b[0m | \u001b[0m 0.2731  \u001b[0m |\n",
      "| \u001b[0m 71      \u001b[0m | \u001b[0m-0.7469  \u001b[0m | \u001b[0m 0.1754  \u001b[0m | \u001b[0m 0.0434  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.7943  \u001b[0m |\n",
      "| \u001b[0m 72      \u001b[0m | \u001b[0m-0.8869  \u001b[0m | \u001b[0m 0.1197  \u001b[0m | \u001b[0m 0.06651 \u001b[0m | \u001b[0m 0.0921  \u001b[0m | \u001b[0m 0.2301  \u001b[0m |\n",
      "| \u001b[0m 73      \u001b[0m | \u001b[0m-0.8673  \u001b[0m | \u001b[0m 0.1611  \u001b[0m | \u001b[0m 0.0694  \u001b[0m | \u001b[0m 0.04351 \u001b[0m | \u001b[0m 0.7064  \u001b[0m |\n",
      "| \u001b[0m 74      \u001b[0m | \u001b[0m-0.7843  \u001b[0m | \u001b[0m 0.1126  \u001b[0m | \u001b[0m 0.07106 \u001b[0m | \u001b[0m 0.0852  \u001b[0m | \u001b[0m 0.1506  \u001b[0m |\n",
      "| \u001b[0m 75      \u001b[0m | \u001b[0m-0.8291  \u001b[0m | \u001b[0m 0.233   \u001b[0m | \u001b[0m 0.07163 \u001b[0m | \u001b[0m 0.05493 \u001b[0m | \u001b[0m 0.3163  \u001b[0m |\n",
      "| \u001b[0m 76      \u001b[0m | \u001b[0m-0.8312  \u001b[0m | \u001b[0m 0.1868  \u001b[0m | \u001b[0m 0.06928 \u001b[0m | \u001b[0m 0.1092  \u001b[0m | \u001b[0m 0.2254  \u001b[0m |\n",
      "| \u001b[0m 77      \u001b[0m | \u001b[0m-0.8796  \u001b[0m | \u001b[0m 0.06368 \u001b[0m | \u001b[0m 0.07061 \u001b[0m | \u001b[0m 0.1061  \u001b[0m | \u001b[0m 0.1899  \u001b[0m |\n",
      "| \u001b[0m 78      \u001b[0m | \u001b[0m-0.8619  \u001b[0m | \u001b[0m 0.1259  \u001b[0m | \u001b[0m 0.05677 \u001b[0m | \u001b[0m 0.1071  \u001b[0m | \u001b[0m 0.2832  \u001b[0m |\n",
      "| \u001b[0m 79      \u001b[0m | \u001b[0m-1.053   \u001b[0m | \u001b[0m 0.166   \u001b[0m | \u001b[0m 0.03078 \u001b[0m | \u001b[0m 0.05955 \u001b[0m | \u001b[0m 0.7588  \u001b[0m |\n",
      "| \u001b[0m 80      \u001b[0m | \u001b[0m-1.182   \u001b[0m | \u001b[0m 0.1575  \u001b[0m | \u001b[0m 0.09613 \u001b[0m | \u001b[0m 0.1725  \u001b[0m | \u001b[0m 0.3013  \u001b[0m |\n",
      "| \u001b[0m 81      \u001b[0m | \u001b[0m-0.7993  \u001b[0m | \u001b[0m 0.1117  \u001b[0m | \u001b[0m 0.04246 \u001b[0m | \u001b[0m 0.2127  \u001b[0m | \u001b[0m 0.3272  \u001b[0m |\n",
      "| \u001b[0m 82      \u001b[0m | \u001b[0m-0.7499  \u001b[0m | \u001b[0m 0.2221  \u001b[0m | \u001b[0m 0.09667 \u001b[0m | \u001b[0m 0.1346  \u001b[0m | \u001b[0m 0.3038  \u001b[0m |\n",
      "| \u001b[0m 83      \u001b[0m | \u001b[0m-0.9089  \u001b[0m | \u001b[0m 0.2262  \u001b[0m | \u001b[0m 0.05628 \u001b[0m | \u001b[0m 0.1238  \u001b[0m | \u001b[0m 0.3393  \u001b[0m |\n",
      "| \u001b[0m 84      \u001b[0m | \u001b[0m-0.8054  \u001b[0m | \u001b[0m 0.1192  \u001b[0m | \u001b[0m 0.03739 \u001b[0m | \u001b[0m 0.1393  \u001b[0m | \u001b[0m 0.1814  \u001b[0m |\n",
      "| \u001b[0m 85      \u001b[0m | \u001b[0m-0.6832  \u001b[0m | \u001b[0m 0.2102  \u001b[0m | \u001b[0m 0.009566\u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.7708  \u001b[0m |\n",
      "| \u001b[0m 86      \u001b[0m | \u001b[0m-0.9815  \u001b[0m | \u001b[0m 0.2704  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.0555  \u001b[0m | \u001b[0m 0.3728  \u001b[0m |\n",
      "| \u001b[0m 87      \u001b[0m | \u001b[0m-0.7735  \u001b[0m | \u001b[0m 0.2233  \u001b[0m | \u001b[0m 0.03055 \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.8227  \u001b[0m |\n",
      "| \u001b[0m 88      \u001b[0m | \u001b[0m-0.7563  \u001b[0m | \u001b[0m 0.1215  \u001b[0m | \u001b[0m 0.08004 \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.8107  \u001b[0m |\n",
      "| \u001b[0m 89      \u001b[0m | \u001b[0m-0.8229  \u001b[0m | \u001b[0m 0.234   \u001b[0m | \u001b[0m 0.07693 \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.8686  \u001b[0m |\n",
      "| \u001b[0m 90      \u001b[0m | \u001b[0m-0.7364  \u001b[0m | \u001b[0m 0.2041  \u001b[0m | \u001b[0m 0.0386  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.694   \u001b[0m |\n",
      "| \u001b[0m 91      \u001b[0m | \u001b[0m-0.7511  \u001b[0m | \u001b[0m 0.1791  \u001b[0m | \u001b[0m 0.08641 \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.6727  \u001b[0m |\n",
      "| \u001b[0m 92      \u001b[0m | \u001b[0m-0.6907  \u001b[0m | \u001b[0m 0.07189 \u001b[0m | \u001b[0m 0.08054 \u001b[0m | \u001b[0m 0.1534  \u001b[0m | \u001b[0m 0.295   \u001b[0m |\n",
      "| \u001b[0m 93      \u001b[0m | \u001b[0m-1.3     \u001b[0m | \u001b[0m 0.0664  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.15    \u001b[0m | \u001b[0m 0.2388  \u001b[0m |\n",
      "| \u001b[0m 94      \u001b[0m | \u001b[0m-0.711   \u001b[0m | \u001b[0m 0.09473 \u001b[0m | \u001b[0m 0.02019 \u001b[0m | \u001b[0m 0.1631  \u001b[0m | \u001b[0m 0.2787  \u001b[0m |\n",
      "| \u001b[0m 95      \u001b[0m | \u001b[0m-1.121   \u001b[0m | \u001b[0m 0.06232 \u001b[0m | \u001b[0m 0.07226 \u001b[0m | \u001b[0m 0.208   \u001b[0m | \u001b[0m 0.306   \u001b[0m |\n",
      "| \u001b[0m 96      \u001b[0m | \u001b[0m-0.9408  \u001b[0m | \u001b[0m 0.2185  \u001b[0m | \u001b[0m 0.04406 \u001b[0m | \u001b[0m 0.192   \u001b[0m | \u001b[0m 0.3532  \u001b[0m |\n",
      "| \u001b[0m 97      \u001b[0m | \u001b[0m-0.9301  \u001b[0m | \u001b[0m 0.145   \u001b[0m | \u001b[0m 0.07515 \u001b[0m | \u001b[0m 0.1555  \u001b[0m | \u001b[0m 0.1402  \u001b[0m |\n",
      "| \u001b[0m 98      \u001b[0m | \u001b[0m-2.482   \u001b[0m | \u001b[0m 0.1521  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.1576  \u001b[0m | \u001b[0m 0.2834  \u001b[0m |\n",
      "| \u001b[0m 99      \u001b[0m | \u001b[0m-0.7977  \u001b[0m | \u001b[0m 0.2191  \u001b[0m | \u001b[0m 0.0503  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.3636  \u001b[0m |\n",
      "| \u001b[0m 100     \u001b[0m | \u001b[0m-0.8019  \u001b[0m | \u001b[0m 0.2841  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.8664  \u001b[0m |\n",
      "| \u001b[0m 101     \u001b[0m | \u001b[0m-0.742   \u001b[0m | \u001b[0m 0.1323  \u001b[0m | \u001b[0m 0.02977 \u001b[0m | \u001b[0m 0.1157  \u001b[0m | \u001b[0m 0.118   \u001b[0m |\n",
      "| \u001b[0m 102     \u001b[0m | \u001b[0m-0.7932  \u001b[0m | \u001b[0m 0.1145  \u001b[0m | \u001b[0m 0.03578 \u001b[0m | \u001b[0m 0.1932  \u001b[0m | \u001b[0m 0.3975  \u001b[0m |\n",
      "| \u001b[0m 103     \u001b[0m | \u001b[0m-0.7798  \u001b[0m | \u001b[0m 0.1679  \u001b[0m | \u001b[0m 0.05166 \u001b[0m | \u001b[0m 0.1557  \u001b[0m | \u001b[0m 0.3693  \u001b[0m |\n",
      "| \u001b[0m 104     \u001b[0m | \u001b[0m-0.7485  \u001b[0m | \u001b[0m 0.1337  \u001b[0m | \u001b[0m 0.09468 \u001b[0m | \u001b[0m 0.2093  \u001b[0m | \u001b[0m 0.3942  \u001b[0m |\n",
      "| \u001b[0m 105     \u001b[0m | \u001b[0m-0.8369  \u001b[0m | \u001b[0m 0.2493  \u001b[0m | \u001b[0m 0.08655 \u001b[0m | \u001b[0m 0.1755  \u001b[0m | \u001b[0m 0.2325  \u001b[0m |\n",
      "| \u001b[0m 106     \u001b[0m | \u001b[0m-2.368   \u001b[0m | \u001b[0m 0.2339  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.1303  \u001b[0m | \u001b[0m 0.1812  \u001b[0m |\n",
      "| \u001b[0m 107     \u001b[0m | \u001b[0m-0.9719  \u001b[0m | \u001b[0m 0.2433  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.2107  \u001b[0m | \u001b[0m 0.2903  \u001b[0m |\n",
      "| \u001b[0m 108     \u001b[0m | \u001b[0m-0.7724  \u001b[0m | \u001b[0m 0.04823 \u001b[0m | \u001b[0m 0.02972 \u001b[0m | \u001b[0m 0.1392  \u001b[0m | \u001b[0m 0.2903  \u001b[0m |\n",
      "| \u001b[0m 109     \u001b[0m | \u001b[0m-0.7067  \u001b[0m | \u001b[0m 0.1799  \u001b[0m | \u001b[0m 0.06094 \u001b[0m | \u001b[0m 0.2034  \u001b[0m | \u001b[0m 0.4153  \u001b[0m |\n",
      "| \u001b[0m 110     \u001b[0m | \u001b[0m-0.9793  \u001b[0m | \u001b[0m 0.1924  \u001b[0m | \u001b[0m 0.09753 \u001b[0m | \u001b[0m 0.2322  \u001b[0m | \u001b[0m 0.374   \u001b[0m |\n",
      "=========================================================================\n",
      "{'target': -0.6500334282952827, 'params': {'dropout': 0.12771198428037775, 'lr': 0.0074010841641111965, 'neuronPct': 0.10774655638231533, 'neuronShrink': 0.2784788676498257}}\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "import time\n",
    "\n",
    "# Supress NaN warnings, see: https://stackoverflow.com/questions/34955158/what-might-be-the-cause-of-invalid-value-encountered-in-less-equal-in-numpy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category =RuntimeWarning)\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = {'dropout': (0.0, 0.499),\n",
    "           'lr': (0.0, 0.1),\n",
    "           'neuronPct': (0.01, 1),\n",
    "           'neuronShrink': (0.01, 1)\n",
    "          }\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=evaluate_network,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "optimizer.maximize(init_points=10, n_iter=100,)\n",
    "time_took = time.time() - start_time\n",
    "\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'target': -0.6500334282952827, 'params': {'dropout': 0.12771198428037775, 'lr': 0.0074010841641111965, 'neuronPct': 0.10774655638231533, 'neuronShrink': 0.2784788676498257}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
