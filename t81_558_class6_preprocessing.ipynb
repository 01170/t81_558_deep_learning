{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Class 6: Preprocessing.**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Why is Preprocessing Necessary\n",
    "\n",
    "The feature vector, the input to a model (such as a neural network), must be completely numeric. Converting non-numeric data into numeric is one major component of preprocessing.  It is also often important to preprocess numeric values.  Scikit-learn provides a large number of preprocessing functions: \n",
    "\n",
    "* [Scikit-Learn Preprocessing](http://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "\n",
    "However, this is just the beginning.  The success of your neural network's predictions is often directly tied to the data representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Functions\n",
    "\n",
    "The following functions will be used in conjunction with TensorFlow to help preprocess the data.  Some of these were [covered previously](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class2_tensor_flow.ipynb), some are new.\n",
    "\n",
    "It is okay to just use them. For better understanding, try to see how they work.\n",
    "\n",
    "These functions allow you to build the feature vector for a neural network. Consider the following:\n",
    "\n",
    "* Predictors/Inputs \n",
    "    * Fill any missing inputs with the median for that column.  Use **missing_median**.\n",
    "    * Encode textual/categorical values with **encode_text_dummy** or more creative means (see last part of this class session). \n",
    "    * Encode numeric values with **encode_numeric_zscore**, **encode_numeric_binary** or **encode_numeric_range**. \n",
    "    * Consider removing outliers: **remove_outliers**\n",
    "* Output\n",
    "    * Discard rows with missing outputs.\n",
    "    * Encode textual/categorical values with **encode_text_index**. \n",
    "    * Do not encode output numeric values.\n",
    "    * Consider removing outliers: **remove_outliers**\n",
    "* Produce final feature vectors (x) and expected output (y) with **to_xy**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions\n",
    "\n",
    "These are exactly the same feature vector encoding functions from [Class 3](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class3_training.ipynb).  They must be defined for this class as well.  For more information, refer to class 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing a Dataset\n",
    "\n",
    "The following script can be used to give a high level overview of how a dataset appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODING = 'utf-8'\n",
    "\n",
    "def expand_categories(values):\n",
    "    result = []\n",
    "    s = values.value_counts()\n",
    "    t = float(len(values))\n",
    "    for v in s.index:\n",
    "        result.append(\"{}:{}%\".format(v,round(100*(s[v]/t),2)))\n",
    "    return \"[{}]\".format(\",\".join(result))\n",
    "        \n",
    "def analyze(filename):\n",
    "    print()\n",
    "    print(\"Analyzing: {}\".format(filename))\n",
    "    df = pd.read_csv(filename,encoding=ENCODING)\n",
    "    cols = df.columns.values\n",
    "    total = float(len(df))\n",
    "\n",
    "    print(\"{} rows\".format(int(total)))\n",
    "    for col in cols:\n",
    "        uniques = df[col].unique()\n",
    "        unique_count = len(uniques)\n",
    "        if unique_count>100:\n",
    "            print(\"** {}:{} ({}%)\".format(col,unique_count,int(((unique_count)/total)*100)))\n",
    "        else:\n",
    "            print(\"** {}:{}\".format(col,expand_categories(df[col])))\n",
    "            expand_categories(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analyze script can be run on the MPG dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing: ./data/auto-mpg.csv\n",
      "398 rows\n",
      "** mpg:129 (32%)\n",
      "** cylinders:[4:51.26%,8:25.88%,6:21.11%,3:1.01%,5:0.75%]\n",
      "** displacement:[97.0:5.28%,98.0:4.52%,350.0:4.52%,250.0:4.27%,318.0:4.27%,140.0:4.02%,400.0:3.27%,225.0:3.27%,91.0:3.02%,232.0:2.76%,121.0:2.76%,302.0:2.76%,151.0:2.51%,120.0:2.26%,231.0:2.01%,200.0:2.01%,90.0:2.01%,85.0:2.01%,351.0:2.01%,304.0:1.76%,122.0:1.76%,105.0:1.76%,156.0:1.51%,79.0:1.51%,119.0:1.51%,108.0:1.26%,107.0:1.26%,89.0:1.26%,258.0:1.26%,135.0:1.26%,360.0:1.01%,86.0:1.01%,116.0:1.01%,112.0:1.01%,305.0:1.01%,134.0:1.01%,455.0:0.75%,307.0:0.75%,429.0:0.75%,173.0:0.75%,198.0:0.75%,168.0:0.75%,113.0:0.75%,260.0:0.75%,146.0:0.75%,70.0:0.75%,383.0:0.5%,71.0:0.5%,163.0:0.5%,262.0:0.5%,141.0:0.5%,199.0:0.5%,440.0:0.5%,104.0:0.25%,390.0:0.25%,454.0:0.25%,340.0:0.25%,110.0:0.25%,267.0:0.25%,88.0:0.25%,111.0:0.25%,144.0:0.25%,181.0:0.25%,145.0:0.25%,100.0:0.25%,81.0:0.25%,183.0:0.25%,131.0:0.25%,78.0:0.25%,80.0:0.25%,130.0:0.25%,72.0:0.25%,101.0:0.25%,115.0:0.25%,171.0:0.25%,83.0:0.25%,76.0:0.25%,68.0:0.25%,155.0:0.25%,96.0:0.25%,97.5:0.25%,114.0:0.25%]\n",
      "** horsepower:[150:5.53%,90:5.03%,88:4.77%,110:4.52%,100:4.27%,95:3.52%,75:3.52%,67:3.02%,105:3.02%,70:3.02%,65:2.51%,97:2.26%,85:2.26%,80:1.76%,140:1.76%,145:1.76%,68:1.51%,72:1.51%,92:1.51%,?:1.51%,78:1.51%,84:1.51%,130:1.26%,71:1.26%,115:1.26%,86:1.26%,175:1.26%,60:1.26%,180:1.26%,170:1.26%,120:1.01%,52:1.01%,76:1.01%,165:1.01%,83:1.01%,225:0.75%,63:0.75%,112:0.75%,215:0.75%,48:0.75%,190:0.75%,74:0.75%,69:0.75%,125:0.75%,96:0.75%,153:0.5%,46:0.5%,139:0.5%,81:0.5%,62:0.5%,79:0.5%,160:0.5%,53:0.5%,198:0.5%,155:0.5%,58:0.5%,98:0.5%,87:0.5%,129:0.5%,82:0.25%,208:0.25%,167:0.25%,64:0.25%,108:0.25%,220:0.25%,132:0.25%,149:0.25%,107:0.25%,116:0.25%,93:0.25%,77:0.25%,122:0.25%,210:0.25%,135:0.25%,148:0.25%,137:0.25%,61:0.25%,94:0.25%,102:0.25%,200:0.25%,193:0.25%,133:0.25%,138:0.25%,66:0.25%,230:0.25%,49:0.25%,158:0.25%,103:0.25%,113:0.25%,152:0.25%,91:0.25%,89:0.25%,142:0.25%,54:0.25%]\n",
      "** weight:351 (88%)\n",
      "** acceleration:[14.5:5.78%,15.5:5.28%,16.0:4.02%,14.0:4.02%,13.5:3.77%,17.0:3.52%,15.0:3.52%,16.5:3.27%,13.0:3.02%,19.0:3.02%,12.0:2.51%,16.4:2.26%,18.0:2.01%,12.5:2.01%,11.5:1.76%,14.9:1.76%,15.8:1.76%,11.0:1.76%,19.5:1.51%,13.2:1.51%,17.3:1.26%,18.2:1.26%,21.0:1.26%,14.7:1.26%,18.5:1.26%,14.4:1.26%,15.4:1.01%,15.7:1.01%,10.0:1.01%,16.9:1.01%,16.2:1.01%,18.6:1.01%,17.5:1.01%,17.6:1.01%,16.6:0.75%,17.7:0.75%,19.4:0.75%,15.3:0.75%,14.2:0.75%,12.8:0.75%,15.2:0.75%,20.5:0.75%,14.8:0.75%,19.2:0.75%,16.7:0.75%,13.4:0.5%,14.3:0.5%,20.1:0.5%,9.5:0.5%,8.5:0.5%,13.8:0.5%,16.8:0.5%,12.2:0.5%,18.7:0.5%,13.9:0.5%,13.6:0.5%,12.9:0.5%,17.8:0.5%,17.4:0.5%,15.9:0.5%,22.2:0.5%,13.7:0.5%,11.4:0.5%,19.6:0.5%,15.1:0.5%,17.2:0.5%,12.6:0.5%,22.1:0.25%,19.9:0.25%,20.4:0.25%,8.0:0.25%,12.1:0.25%,18.3:0.25%,9.0:0.25%,21.7:0.25%,10.5:0.25%,17.9:0.25%,11.1:0.25%,11.2:0.25%,24.8:0.25%,14.1:0.25%,18.1:0.25%,11.3:0.25%,21.8:0.25%,23.7:0.25%,18.8:0.25%,20.7:0.25%,21.9:0.25%,11.6:0.25%,21.5:0.25%,17.1:0.25%,24.6:0.25%,23.5:0.25%,16.1:0.25%,15.6:0.25%]\n",
      "** year:[73:10.05%,78:9.05%,76:8.54%,82:7.79%,75:7.54%,81:7.29%,80:7.29%,79:7.29%,70:7.29%,77:7.04%,72:7.04%,71:7.04%,74:6.78%]\n",
      "** origin:[1:62.56%,3:19.85%,2:17.59%]\n",
      "** name:305 (76%)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.learn as skflow\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "filename_read = os.path.join(path,\"auto-mpg.csv\")\n",
    "analyze(filename_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Examples\n",
    "\n",
    "The above preprocessing functions can be used in a variety of ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before MPG outliers dropped: 398\n",
      "Length after MPG outliers dropped: 388\n",
      "      mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
      "0    18.0          8         307.0       130.0    3504          12.0    70   \n",
      "1    15.0          8         350.0       165.0    3693          11.5    70   \n",
      "2    18.0          8         318.0       150.0    3436          11.0    70   \n",
      "3    16.0          8         304.0       150.0    3433          12.0    70   \n",
      "4    17.0          8         302.0       140.0    3449          10.5    70   \n",
      "5    15.0          8         429.0       198.0    4341          10.0    70   \n",
      "6    14.0          8         454.0       220.0    4354           9.0    70   \n",
      "7    14.0          8         440.0       215.0    4312           8.5    70   \n",
      "8    14.0          8         455.0       225.0    4425          10.0    70   \n",
      "9    15.0          8         390.0       190.0    3850           8.5    70   \n",
      "10   15.0          8         383.0       170.0    3563          10.0    70   \n",
      "11   14.0          8         340.0       160.0    3609           8.0    70   \n",
      "12   15.0          8         400.0       150.0    3761           9.5    70   \n",
      "13   14.0          8         455.0       225.0    3086          10.0    70   \n",
      "14   24.0          4         113.0        95.0    2372          15.0    70   \n",
      "15   22.0          6         198.0        95.0    2833          15.5    70   \n",
      "16   18.0          6         199.0        97.0    2774          15.5    70   \n",
      "17   21.0          6         200.0        85.0    2587          16.0    70   \n",
      "18   27.0          4          97.0        88.0    2130          14.5    70   \n",
      "19   26.0          4          97.0        46.0    1835          20.5    70   \n",
      "20   25.0          4         110.0        87.0    2672          17.5    70   \n",
      "21   24.0          4         107.0        90.0    2430          14.5    70   \n",
      "22   25.0          4         104.0        95.0    2375          17.5    70   \n",
      "23   26.0          4         121.0       113.0    2234          12.5    70   \n",
      "24   21.0          6         199.0        90.0    2648          15.0    70   \n",
      "25   10.0          8         360.0       215.0    4615          14.0    70   \n",
      "26   10.0          8         307.0       200.0    4376          15.0    70   \n",
      "27   11.0          8         318.0       210.0    4382          13.5    70   \n",
      "28    9.0          8         304.0       193.0    4732          18.5    70   \n",
      "29   27.0          4          97.0        88.0    2130          14.5    71   \n",
      "..    ...        ...           ...         ...     ...           ...   ...   \n",
      "367  28.0          4         112.0        88.0    2605          19.6    82   \n",
      "368  27.0          4         112.0        88.0    2640          18.6    82   \n",
      "369  34.0          4         112.0        88.0    2395          18.0    82   \n",
      "370  31.0          4         112.0        85.0    2575          16.2    82   \n",
      "371  29.0          4         135.0        84.0    2525          16.0    82   \n",
      "372  27.0          4         151.0        90.0    2735          18.0    82   \n",
      "373  24.0          4         140.0        92.0    2865          16.4    82   \n",
      "374  23.0          4         151.0        93.5    3035          20.5    82   \n",
      "375  36.0          4         105.0        74.0    1980          15.3    82   \n",
      "376  37.0          4          91.0        68.0    2025          18.2    82   \n",
      "377  31.0          4          91.0        68.0    1970          17.6    82   \n",
      "378  38.0          4         105.0        63.0    2125          14.7    82   \n",
      "379  36.0          4          98.0        70.0    2125          17.3    82   \n",
      "380  36.0          4         120.0        88.0    2160          14.5    82   \n",
      "381  36.0          4         107.0        75.0    2205          14.5    82   \n",
      "382  34.0          4         108.0        70.0    2245          16.9    82   \n",
      "383  38.0          4          91.0        67.0    1965          15.0    82   \n",
      "384  32.0          4          91.0        67.0    1965          15.7    82   \n",
      "385  38.0          4          91.0        67.0    1995          16.2    82   \n",
      "386  25.0          6         181.0       110.0    2945          16.4    82   \n",
      "387  38.0          6         262.0        85.0    3015          17.0    82   \n",
      "388  26.0          4         156.0        92.0    2585          14.5    82   \n",
      "389  22.0          6         232.0       112.0    2835          14.7    82   \n",
      "390  32.0          4         144.0        96.0    2665          13.9    82   \n",
      "391  36.0          4         135.0        84.0    2370          13.0    82   \n",
      "392  27.0          4         151.0        90.0    2950          17.3    82   \n",
      "393  27.0          4         140.0        86.0    2790          15.6    82   \n",
      "395  32.0          4         135.0        84.0    2295          11.6    82   \n",
      "396  28.0          4         120.0        79.0    2625          18.6    82   \n",
      "397  31.0          4         119.0        82.0    2720          19.4    82   \n",
      "\n",
      "     origin  \n",
      "0         1  \n",
      "1         1  \n",
      "2         1  \n",
      "3         1  \n",
      "4         1  \n",
      "5         1  \n",
      "6         1  \n",
      "7         1  \n",
      "8         1  \n",
      "9         1  \n",
      "10        1  \n",
      "11        1  \n",
      "12        1  \n",
      "13        1  \n",
      "14        3  \n",
      "15        1  \n",
      "16        1  \n",
      "17        1  \n",
      "18        3  \n",
      "19        2  \n",
      "20        2  \n",
      "21        2  \n",
      "22        2  \n",
      "23        2  \n",
      "24        1  \n",
      "25        1  \n",
      "26        1  \n",
      "27        1  \n",
      "28        1  \n",
      "29        3  \n",
      "..      ...  \n",
      "367       1  \n",
      "368       1  \n",
      "369       1  \n",
      "370       1  \n",
      "371       1  \n",
      "372       1  \n",
      "373       1  \n",
      "374       1  \n",
      "375       2  \n",
      "376       3  \n",
      "377       3  \n",
      "378       1  \n",
      "379       1  \n",
      "380       3  \n",
      "381       3  \n",
      "382       3  \n",
      "383       3  \n",
      "384       3  \n",
      "385       3  \n",
      "386       1  \n",
      "387       1  \n",
      "388       1  \n",
      "389       1  \n",
      "390       3  \n",
      "391       1  \n",
      "392       1  \n",
      "393       1  \n",
      "395       1  \n",
      "396       1  \n",
      "397       1  \n",
      "\n",
      "[388 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.learn as skflow\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "filename_read = os.path.join(path,\"auto-mpg.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "# create feature vector\n",
    "missing_median(df, 'horsepower')\n",
    "df.drop('name',1,inplace=True)\n",
    "#encode_numeric_binary(df,'mpg',20)\n",
    "#df['origin'] = df['origin'].astype(str)\n",
    "#encode_text_tfidf(df, 'origin')\n",
    "\n",
    "# Drop outliers in horsepower\n",
    "print(\"Length before MPG outliers dropped: {}\".format(len(df)))\n",
    "remove_outliers(df,'mpg',2)\n",
    "print(\"Length after MPG outliers dropped: {}\".format(len(df)))\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Ranking\n",
    "\n",
    "Feature ranking is an important process where you determine which input columns (features) are the most important. I implemented several feature ranking algorithms for the following academic paper:\n",
    "\n",
    "Heaton, J., McElwee, S., & Cannady, J. (May 2017). [Early stabilizing feature importance for TensorFlow deep neural networks](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/pdf/heaton_et_al_ijcnn_2017-pre.pdf). In *International Joint Conference on Neural Networks (IJCNN 2017)* (accepted for publication). IEEE.\n",
    "\n",
    "Two feature ranking algorithms are provided here (a total of 4 are in the paper):\n",
    "\n",
    "* **CorrelationCoefficientRank** - A simple statistical analysis of the correlation between each input field and the target.  Does not require a trained neural network and does not consider interactions.\n",
    "* **InputPerturbationRank** - Uses a trained neural network and scrambles each input one-by-one. Neural network does not need to be retrained.  Slower, but more accurate, than CorrelationCoefficientRank.\n",
    "\n",
    "Some of the code from this paper is provieded here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature ranking code\n",
    "\n",
    "class Ranking(object):\n",
    "    def __init__(self, names):\n",
    "        self.names = names\n",
    "\n",
    "    def _normalize(self, x, y, impt):\n",
    "        impt = impt / sum(impt)\n",
    "        impt = list(zip(impt, self.names, range(x.shape[1])))\n",
    "        impt.sort(key=lambda x: -x[0])\n",
    "        return impt\n",
    "    \n",
    "class CorrelationCoefficientRank(Ranking):\n",
    "    def __init__(self, names):\n",
    "        super(CorrelationCoefficientRank, self).__init__(names)\n",
    "\n",
    "    def rank(self, x, y, model=None):\n",
    "        impt = []\n",
    "\n",
    "        for i in range(x.shape[1]):\n",
    "            c = abs(np.corrcoef(x[:, i], y[:, 0]))\n",
    "            impt.append(abs(c[1, 0]))\n",
    "\n",
    "        impt = impt / sum(impt)\n",
    "        impt = list(zip(impt, self.names, range(x.shape[1])))\n",
    "        impt.sort(key=lambda x: -x[0])\n",
    "\n",
    "        return (impt)\n",
    "\n",
    "\n",
    "class InputPerturbationRank(Ranking):\n",
    "    def __init__(self, names):\n",
    "        super(InputPerturbationRank, self).__init__(names)\n",
    "\n",
    "    def _raw_rank(self, x, y, network):\n",
    "        impt = np.zeros(x.shape[1])\n",
    "\n",
    "        for i in range(x.shape[1]):\n",
    "            hold = np.array(x[:, i])\n",
    "            np.random.shuffle(x[:, i])\n",
    "\n",
    "            # Handle both TensorFlow and SK-Learn models.\n",
    "            if 'tensorflow' in str(type(network)).lower():\n",
    "                pred = list(network.predict(x, as_iterable=True))\n",
    "            else:\n",
    "                pred = network.predict(x)\n",
    "\n",
    "            rmse = metrics.mean_squared_error(y, pred)\n",
    "            impt[i] = rmse\n",
    "            x[:, i] = hold\n",
    "\n",
    "        return impt\n",
    "\n",
    "    def rank(self, x, y, network):\n",
    "        impt = self._raw_rank(x, y, network)\n",
    "        return self._normalize(x, y, impt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00289: early stopping\n",
      "\n",
      "*** InputPerturbationRank ***\n",
      "(0.15225173246018681, 'weight', 3)\n",
      "(0.13172398478946723, 'year', 5)\n",
      "(0.11513291435257444, 'horsepower', 2)\n",
      "(0.11217235777181986, 'displacement', 1)\n",
      "(0.10490684428706226, 'cylinders', 0)\n",
      "(0.10047782470459782, 'origin-3', 8)\n",
      "(0.095492142248928824, 'acceleration', 4)\n",
      "(0.094753727508829239, 'origin-1', 6)\n",
      "(0.093088471876533518, 'origin-2', 7)\n"
     ]
    }
   ],
   "source": [
    "# Rank MPG fields\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "# Set the desired TensorFlow output level for this example\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "filename_read = os.path.join(path,\"auto-mpg.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "# create feature vector\n",
    "missing_median(df, 'horsepower')\n",
    "df.drop('name',1,inplace=True)\n",
    "encode_numeric_zscore(df, 'horsepower')\n",
    "encode_numeric_zscore(df, 'weight')\n",
    "encode_numeric_zscore(df, 'cylinders')\n",
    "encode_numeric_zscore(df, 'displacement')\n",
    "encode_numeric_zscore(df, 'acceleration')\n",
    "encode_text_dummy(df, 'origin')\n",
    "\n",
    "# Encode to a 2D matrix for training\n",
    "x,y = to_xy(df,'mpg')\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.20, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath=\"best_weights.hdf5\", verbose=0, save_best_only=True) # save best model\n",
    "model.fit(x,y,validation_data=(x_test,y_test),callbacks=[monitor,checkpointer],verbose=0,epochs=1000)\n",
    "model.load_weights('best_weights.hdf5') # load weights from best model\n",
    "    \n",
    "# Fit/train neural network\n",
    "names = list(df.columns) \n",
    "names.remove('mpg') # must remove target field MPG so that index aligns with x (which does not have mpg)\n",
    "\n",
    "ranker = InputPerturbationRank\n",
    "print()\n",
    "print(\"*** InputPerturbationRank ***\")\n",
    "l1 = ranker(names).rank(x_test, y_test, model)\n",
    "\n",
    "for itm in l1:\n",
    "    print(itm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** CorrelationCoefficientRank ***\n",
      "(0.1523953056674856, 'weight', 3)\n",
      "(0.14617209454138816, 'displacement', 1)\n",
      "(0.14485626531604004, 'horsepower', 2)\n",
      "(0.14248640927492129, 'cylinders', 0)\n",
      "(0.099341860269407153, 'year', 5)\n",
      "(0.097932975012336415, 'acceleration', 4)\n",
      "(0.095503698574517557, 'origin-1', 6)\n",
      "(0.084529264419704569, 'origin-3', 8)\n",
      "(0.036782126924199104, 'origin-2', 7)\n"
     ]
    }
   ],
   "source": [
    "ranker = CorrelationCoefficientRank\n",
    "print()\n",
    "print(\"*** CorrelationCoefficientRank ***\")\n",
    "l1 = ranker(names).rank(x_test, y_test, model)\n",
    "\n",
    "for itm in l1:\n",
    "    print(itm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Examples: Dealing with Addresses\n",
    "\n",
    "Addresses can be difficult to encode into a neural network.  There are many different approaches, and you must consider how you can transform the address into something more meaningful.  Map coordinates can be a good approach.  [Latitude and longitude](https://en.wikipedia.org/wiki/Geographic_coordinate_system) can be a useful encoding.  Thanks to the power of the Internet, it is relatively easy to transform an address into its latitude and longitude values.  The following code determines the coordinates of [Washington University](https://wustl.edu/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lat': 38.6471178, 'lng': -90.3026148}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "address = \"1 Brookings Dr, St. Louis, MO 63130\"\n",
    "\n",
    "response = requests.get('https://maps.googleapis.com/maps/api/geocode/json?address='+address)\n",
    "\n",
    "resp_json_payload = response.json()\n",
    "\n",
    "print(resp_json_payload['results'][0]['geometry']['location'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If latitude and longitude are simply fed into the neural network as two features, they might not be overly helpful.  These two values would allow your neural network to cluster locations on a map.  Sometimes cluster locations on a map can be useful.  Consider the percentage of the population that smokes in the USA by state:\n",
    "\n",
    "![Smokers by State](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_6_smokers.png \"Smokers by State\")\n",
    "\n",
    "The above map shows that certian behaviors, like smoking, can be clustered by global region. \n",
    "\n",
    "However, often you will want to transform the coordinates into distances.  It is reasonably easy to estimate the distance between any two points on Earth by using the [great circle distance](https://en.wikipedia.org/wiki/Great-circle_distance) between any two points on a sphere:\n",
    "\n",
    "The following code implements this formula:\n",
    "\n",
    "$\\Delta\\sigma=\\arccos\\bigl(\\sin\\phi_1\\cdot\\sin\\phi_2+\\cos\\phi_1\\cdot\\cos\\phi_2\\cdot\\cos(\\Delta\\lambda)\\bigr)$\n",
    "\n",
    "$d = r \\, \\Delta\\sigma$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance, St. Louis, MO to Ft. Lauderdale, FL: 1685.0869618595307 km\n"
     ]
    }
   ],
   "source": [
    "from math import sin, cos, sqrt, atan2, radians\n",
    "\n",
    "# Distance function\n",
    "def distance_lat_lng(lat1,lng1,lat2,lng2):\n",
    "    # approximate radius of earth in km\n",
    "    R = 6373.0\n",
    "\n",
    "    # degrees to radians (lat/lon are in degrees)\n",
    "    lat1 = radians(lat1)\n",
    "    lng1 = radians(lng1)\n",
    "    lat2 = radians(lat2)\n",
    "    lng2 = radians(lng2)\n",
    "\n",
    "    dlng = lng2 - lng1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlng / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    return R * c\n",
    "\n",
    "# Find lat lon for address\n",
    "def lookup_lat_lng(address):\n",
    "    response = requests.get('https://maps.googleapis.com/maps/api/geocode/json?address='+address)\n",
    "    json = response.json()\n",
    "    if len(json['results']) == 0:\n",
    "        print(\"Can't find: {}\".format(address))\n",
    "        return 0,0\n",
    "    map = json['results'][0]['geometry']['location']\n",
    "    return map['lat'],map['lng']\n",
    "\n",
    "\n",
    "# Distance between two locations\n",
    "\n",
    "import requests\n",
    "\n",
    "address1 = \"1 Brookings Dr, St. Louis, MO 63130\" \n",
    "address2 = \"3301 College Ave, Fort Lauderdale, FL 33314\"\n",
    "\n",
    "lat1, lng1 = lookup_lat_lng(address1)\n",
    "lat2, lng2 = lookup_lat_lng(address2)\n",
    "\n",
    "print(\"Distance, St. Louis, MO to Ft. Lauderdale, FL: {} km\".format(\n",
    "        distance_lat_lng(lat1,lng1,lat2,lng2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distances can be useful to encode addresses as.  You must consider what distance might be useful for your dataset.  Consider:\n",
    "\n",
    "* Distance to major metropolitan area\n",
    "* Distance to competitor\n",
    "* Distance to distribution center\n",
    "* Distance to retail outlet\n",
    "\n",
    "The following code calculates the distance between 10 universities and washu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "School 'Princeton', distance to wustl is: 1354.7554246422899\n",
      "School 'Harvard', distance to wustl is: 1670.4945469414515\n",
      "School 'University of Chicago', distance to wustl is: 418.07074534593164\n",
      "School 'Yale', distance to wustl is: 1508.0574212290953\n",
      "School 'Columbia University', distance to wustl is: 1418.0702619177403\n",
      "School 'Stanford', distance to wustl is: 2781.0147376094565\n",
      "School 'MIT', distance to wustl is: 1672.3056259012085\n",
      "School 'Duke University', distance to wustl is: 1046.5733989340915\n",
      "School 'University of Pennsylvania', distance to wustl is: 1307.0188113752533\n",
      "School 'Johns Hopkins', distance to wustl is: 1184.1983102146019\n"
     ]
    }
   ],
   "source": [
    "# Encoding other universities by their distance to Washington University\n",
    "\n",
    "schools = [\n",
    "    [\"Princeton University, Princeton, NJ 08544\", 'Princeton'],\n",
    "    [\"Massachusetts Hall, Cambridge, MA 02138\", 'Harvard'],\n",
    "    [\"5801 S Ellis Ave, Chicago, IL 60637\", 'University of Chicago'],\n",
    "    [\"Yale, New Haven, CT 06520\", 'Yale'],\n",
    "    [\"116th St & Broadway, New York, NY 10027\", 'Columbia University'],\n",
    "    [\"450 Serra Mall, Stanford, CA 94305\", 'Stanford'],\n",
    "    [\"77 Massachusetts Ave, Cambridge, MA 02139\", 'MIT'],\n",
    "    [\"Duke University, Durham, NC 27708\", 'Duke University'],\n",
    "    [\"University of Pennsylvania, Philadelphia, PA 19104\", 'University of Pennsylvania'],\n",
    "    [\"Johns Hopkins University, Baltimore, MD 21218\", 'Johns Hopkins']\n",
    "]\n",
    "\n",
    "lat1, lng1 = lookup_lat_lng(\"1 Brookings Dr, St. Louis, MO 63130\")\n",
    "\n",
    "for address, name in schools:\n",
    "    lat2,lng2 = lookup_lat_lng(address)\n",
    "    dist = distance_lat_lng(lat1,lng1,lat2,lng2)\n",
    "    print(\"School '{}', distance to wustl is: {}\".format(name,dist))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Examples: Bag of Words\n",
    "\n",
    "The Bag of Words algorithm is a common means of encoding strings. (Harris, 1954) Each input represents the count of one particular word. The entire input vector would contain one value for each unique word. Consider the following strings.\n",
    "\n",
    "```\n",
    "Of Mice and Men\n",
    "Three Blind Mice\n",
    "Blind Man’s Bluff\n",
    "Mice and More Mice\n",
    "```\n",
    "\n",
    "We have the following unique words. This is our “dictionary.”\n",
    "\n",
    "```\n",
    "Input 0 : and\n",
    "Input 1 : blind\n",
    "Input 2 : bluff\n",
    "Input 3 : man’s\n",
    "Input 4 : men\n",
    "Input 5 : mice\n",
    "Input 6 : more\n",
    "Input 7 : of\n",
    "Input 8 : three\n",
    "```\n",
    "\n",
    "The four lines above would be encoded as follows.\n",
    "\n",
    "```\n",
    "Of Mice and Men [ 0 4 5 7 ]\n",
    "Three Blind Mice [ 1 5 8 ]\n",
    "Blind Man ’ s Bl u f f [ 1 2 3 ]\n",
    "Mice and More Mice [ 0 5 6 ]\n",
    "```\n",
    "\n",
    "Of course we have to fill in the missing words with zero, so we end up with\n",
    "the following.\n",
    "\n",
    "* Of Mice and Men [ 1 , 0 , 0 , 0 , 1 , 1 , 0 , 1 , 0 ]\n",
    "* Three Blind Mice [ 0 , 1 , 0 , 0 , 0 , 1 , 0 , 0 , 1 ]\n",
    "* Blind Man’s Bluff [ 0 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 ]\n",
    "* Mice and More Mice [ 1 , 0 , 0 , 0 , 0 , 2 , 1 , 0 , 0 ]\n",
    "\n",
    "Notice that we now have a consistent vector length of nine. Nine is the total\n",
    "number of words in our “dictionary”. Each component number in the vector is\n",
    "an index into our dictionary of available words. At each vector component is\n",
    "stored a count of the number of words for that dictionary entry. Each string\n",
    "will usually contain only a small subset of the dictionary. As a result, most of\n",
    "the vector values will be zero.\n",
    "\n",
    "As you can see, one of the most difficult aspects of machine learning programming\n",
    "is translating your problem into a fixed-length array of floating point\n",
    "numbers. The following section shows how to translate several examples.\n",
    "\n",
    "\n",
    "* [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping\n",
      "{'the': 6, 'third': 7, 'document': 1, 'and': 0, 'first': 2, 'is': 3, 'one': 4, 'second': 5, 'this': 8}\n",
      "\n",
      "Encoded\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?']\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "print(\"Mapping\")\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "print()\n",
    "print(\"Encoded\")\n",
    "x = vectorizer.transform(corpus)\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping\n",
      "{'310': 36, 'ranger': 227, 'glc': 150, 'futura': 146, 'bel': 71, '1131': 4, 'f108': 137, 'opel': 213, 'custom': 116, 'arrow': 64, '245': 29, '4w': 43, 'omega': 211, 'eldorado': 133, 'seville': 248, '304': 35, 'v6': 282, 'nissan': 208, 'colt': 102, '810': 54, 'super': 266, 'starlet': 261, 'brougham': 74, 'bmw': 73, 'safari': 240, 'ventura': 286, 'dasher': 122, 'royale': 234, '340': 39, 'mercury': 197, 'lebaron': 172, 'subaru': 263, 'liftback': 176, '300d': 34, '2h': 33, '100': 1, 'concours': 104, 'fox': 144, 'renault': 232, 'rebel': 228, 'datsun': 123, '124': 8, '500': 44, 'sport': 254, 'triumph': 279, 'toyouta': 277, 'delta': 124, '88': 55, 'lesabre': 175, 'grand': 153, 'sportabout': 255, 'jetta': 170, 'j2000': 169, '5000': 45, 'town': 275, 'mazda': 194, 'royal': 233, 'mpg': 204, 'sedan': 247, '604sl': 50, '350': 40, '320i': 38, 'lynx': 183, 'tc': 270, '320': 37, 'corona': 107, 'pl510': 219, 'corolla': 106, 'rx3': 238, 'gtl': 157, 'lecar': 173, 'st': 258, '18i': 18, 'auto': 68, 'rampage': 226, 'hardtop': 159, 'landau': 171, 'century': 87, 'mercedes': 196, 'cruiser': 114, 'regis': 230, 'cougar': 109, '5000s': 46, '1600': 17, 'impala': 167, 'v8': 283, 'classic': 100, 'mustang': 205, 'isuzu': 168, 'civic': 99, 'type': 281, 'fairmont': 139, 'torino': 274, '124b': 9, 'door': 130, '200': 20, 'fury': 145, 'ambassador': 61, 'pinto': 218, 'escort': 135, 'cadillac': 78, 'sj': 249, '610': 51, 'magnum': 184, '504': 47, 'concord': 103, '200sx': 23, 'beetle': 70, 'sunbird': 265, '145e': 15, 'dodge': 129, 'pontiac': 221, '710': 53, 'suburb': 264, 'b210': 69, 'buick': 75, 'vw': 292, 'cressida': 112, 'reliant': 231, 'charger': 90, 'omni': 212, 'challenger': 88, 'matador': 190, '2000': 21, '1200': 6, 'benz': 72, 'cvcc': 118, 'sebring': 246, 'chevroelt': 93, '244dl': 28, 'ghia': 148, '2300': 26, 'mark': 188, 'satellite': 243, 'x1': 295, 'honda': 162, 'cuda': 115, 'special': 252, 'fiat': 140, 'capri': 80, 'ls': 179, 'd100': 119, 'medallion': 195, 'fiesta': 141, 'peugeot': 215, 'prix': 224, 'volvo': 291, '510': 49, 'xe': 296, 'electra': 134, 'valiant': 284, 'volare': 289, '240d': 27, 'yorker': 297, 'volkswagen': 290, 'premier': 223, 'cobra': 101, 'gx': 158, 'hatchback': 160, 'firebird': 142, '111': 3, 'rabbit': 225, 'vista': 287, 'chevy': 95, 'salon': 241, 'gs': 155, 'sx': 269, 'chevrolet': 94, '144ea': 14, 'aspen': 65, 'lx': 182, '280s': 32, 'ciera': 97, 'granada': 152, 'deluxe': 125, '411': 42, 'skylark': 251, '280': 31, 'prelude': 222, 'tc3': 271, '99e': 56, 'pickup': 217, 'hi': 161, 'squire': 256, 'toyota': 276, 'chevette': 92, 'lj': 178, 'starfire': 260, 'vega': 285, 'champ': 89, '264gl': 30, 'gran': 151, 'catalina': 84, 'miser': 198, 'monza': 203, 'astro': 66, 'plymouth': 220, 'se': 245, 'newport': 207, 'marquis': 189, '99le': 58, 'rx': 236, 'c10': 76, 'runabout': 235, 'citation': 98, '12': 5, 'wagon': 293, 'turbo': 280, 'ford': 143, 'caprice': 81, 'spirit': 253, 'stanza': 259, 'sw': 268, 'cricket': 113, 'monarch': 201, 'galaxie': 147, '1500': 16, 'cavalier': 85, 'gl': 149, 'vokswagen': 288, 'coronet': 108, '505s': 48, 'maverick': 191, 'sapporo': 242, 'estate': 136, 'amc': 62, 'air': 60, 'chrysler': 96, 'diesel': 126, 'celica': 86, 'ltd': 180, 'skyhawk': 250, 'tercel': 272, '131': 13, 'regal': 229, 'chevelle': 91, 'duster': 132, '2002': 22, 'camaro': 79, 'carina': 82, 'thunderbird': 273, 'phoenix': 216, 'monaco': 200, '1200d': 7, 'zephyr': 298, '4000': 41, '1300': 12, 'saab': 239, 'maxima': 193, 'limited': 177, '225': 25, 'gt': 156, 'c20': 77, 'tr7': 278, 'dart': 121, 'carlo': 83, 'oldsmobile': 210, 'hornet': 164, 'monte': 202, '100ls': 2, 'model': 199, '210': 24, 'audi': 67, 'pacer': 214, 'strada': 262, 'luxus': 181, 'nova': 209, 'man': 186, 'accord': 59, 'manta': 187, 'malibu': 185, 'lemans': 174, 'scirocco': 244, 'ii': 165, 'country': 110, '10': 0, 'coupe': 111, 'cutlass': 117, 'diplomat': 127, 'f250': 138, 'horizon': 163, 'aries': 63, 'maxda': 192, '128': 10, 'sst': 257, 'woody': 294, 'supreme': 267, '626': 52, '1900': 19, '12tl': 11, 'dl': 128, 'dpl': 131, 'zx': 299, 'd200': 120, 'new': 206, 'iii': 166, 'rx2': 237, 'cordoba': 105, 'gremlin': 154, '99gle': 57}\n",
      "\n",
      "Encoded\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [1 0 0 ..., 0 0 0]]\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "filename_read = os.path.join(path,\"auto-mpg.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "corpus = df['name']\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "print(\"Mapping\")\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "print()\n",
    "print(\"Encoded\")\n",
    "x = vectorizer.transform(corpus).toarray()\n",
    "print(x)\n",
    "\n",
    "print(len(vectorizer.vocabulary_))\n",
    "\n",
    "# reverse lookup for columns\n",
    "bag_cols = [0] * len(vectorizer.vocabulary_)\n",
    "for i,key in enumerate(vectorizer.vocabulary_):\n",
    "    bag_cols[i] = key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Feature Ranking ***\n",
      "(0.014310616934244406, '2000', 123)\n",
      "(0.013192509739466823, 'coupe', 276)\n",
      "(0.0099981774975503694, 'coronet', 220)\n",
      "(0.008601968275715622, 'dl', 290)\n",
      "(0.0085593570854305447, 'vista', 162)\n",
      "(0.0075387469551304355, 'tercel', 232)\n",
      "(0.007494562143636847, '244dl', 129)\n",
      "(0.0071540947572694968, 'monza', 194)\n",
      "(0.0068560995009796172, 'door', 94)\n",
      "(0.0064777911893417256, 'pacer', 263)\n",
      "(0.0064566799240983099, 'valiant', 150)\n",
      "(0.0064094431542827653, 'cvcc', 126)\n",
      "(0.0063526188561083535, 'fiesta', 143)\n",
      "(0.0061105406928404675, 'zx', 292)\n",
      "(0.0059462883979366395, 'ls', 140)\n",
      "(0.0051357533245011524, 'chevy', 163)\n",
      "(0.0049296957443534118, 'corolla', 67)\n",
      "(0.0048561174456105858, 'amc', 225)\n",
      "(0.0046929446624483602, 'escort', 99)\n",
      "(0.0046447220894352331, 'torino', 92)\n",
      "(0.0046427220614357013, '610', 102)\n",
      "(0.0044558005076173373, '505s', 221)\n",
      "(0.0041877870452294748, '200', 95)\n",
      "(0.0040773371501462577, 'd100', 141)\n",
      "(0.0040303027286434957, 'se', 197)\n",
      "(0.0040211256978554977, 'squire', 183)\n",
      "(0.0039769863879363645, 'safari', 24)\n",
      "(0.0039214298979407336, 'prix', 145)\n",
      "(0.0039000710202746745, 'civic', 89)\n",
      "(0.0038705597424634709, 'regis', 80)\n",
      "(0.0038680866302705521, 'cavalier', 217)\n",
      "(0.0038628527169051719, '1500', 216)\n",
      "(0.0037949943062196563, 'granada', 173)\n",
      "(0.0037843269167367522, '604sl', 59)\n",
      "(0.0037557888150303214, 'century', 77)\n",
      "(0.0037070341595827885, 'v8', 85)\n",
      "(0.0036887519541125161, 'type', 90)\n",
      "(0.0036796385144004106, 'hardtop', 75)\n",
      "(0.0036638351930690228, 'concours', 36)\n",
      "(0.0036506839607195183, 'scirocco', 272)\n",
      "(0.0036475684298252135, 'galaxie', 215)\n",
      "(0.0036283529680146806, '111', 160)\n",
      "(0.0035645256631675511, 'catalina', 192)\n",
      "(0.0035590715541786769, 'audi', 262)\n",
      "(0.0035510816658529576, 'chevrolet', 167)\n",
      "(0.0035204380588105648, 'landau', 76)\n",
      "(0.0035051877658652578, 'champ', 189)\n",
      "(0.0035042021926646595, 'dodge', 110)\n",
      "(0.0034792593807485908, 'lynx', 62)\n",
      "(0.0034629077152811567, 'cricket', 213)\n",
      "(0.0034564374638219137, '200sx', 106)\n",
      "(0.0034473458203766257, 'maxda', 282)\n",
      "(0.0034278319802030798, '124', 41)\n",
      "(0.0034150575252345884, 'honda', 135)\n",
      "(0.0034004749401971932, 'pinto', 98)\n",
      "(0.0033888867961130906, '1200d', 243)\n",
      "(0.0033702723597655903, 'spirit', 210)\n",
      "(0.0033561324320824774, '210', 261)\n",
      "(0.0033545022358362473, 'gt', 251)\n",
      "(0.0033463693505655701, 'cutlass', 277)\n",
      "(0.0033048448380606616, '99le', 200)\n",
      "(0.003293660166370089, 'gl', 218)\n",
      "(0.0032892874253253072, 'air', 226)\n",
      "(0.0032801882682238618, '510', 147)\n",
      "(0.0032779534124874948, '320i', 61)\n",
      "(0.0032747372896302475, 'vega', 188)\n",
      "(0.0032700742047199888, 'deluxe', 174)\n",
      "(0.0032692910979871771, 'capri', 139)\n",
      "(0.0032674700101228442, 'impala', 84)\n",
      "(0.0032471388748206592, 'dpl', 291)\n",
      "(0.0032448169237596419, '144ea', 168)\n",
      "(0.0032207135314919976, 'sw', 212)\n",
      "(0.003220651967354844, 'tc', 63)\n",
      "(0.0032165909751009172, 'gtl', 69)\n",
      "(0.0032084774104797547, '100', 35)\n",
      "(0.0032041934204885521, 'carina', 239)\n",
      "(0.003199340974278128, 'cressida', 117)\n",
      "(0.0031806987907308636, 'ford', 208)\n",
      "(0.0031770447200076646, 'benz', 125)\n",
      "(0.003175256802938951, 'electra', 149)\n",
      "(0.0031620447021686988, 'sunbird', 108)\n",
      "(0.0031582455940318255, 'chrysler', 227)\n",
      "(0.0031550538108938635, 'omni', 120)\n",
      "(0.0031504093034383307, '810', 19)\n",
      "(0.0031398426236979035, 'beetle', 107)\n",
      "(0.0031393027390789934, 'diplomat', 278)\n",
      "(0.0031321538622584967, '100ls', 259)\n",
      "(0.0031311022102653856, '2002', 237)\n",
      "(0.0031183043169254621, 'strada', 264)\n",
      "(0.003108976807752344, 'fox', 37)\n",
      "(0.0030954133486663792, 'f250', 279)\n",
      "(0.0030844329104568655, '1900', 288)\n",
      "(0.0030824900607672669, 'fiat', 138)\n",
      "(0.0030822677471976354, 'tr7', 253)\n",
      "(0.0030806574357391792, 'prelude', 178)\n",
      "(0.0030786407983544022, 'lx', 170)\n",
      "(0.0030730179344389116, 'maverick', 222)\n",
      "(0.0030657656287641825, 'vw', 116)\n",
      "(0.0030625583203674387, 'colt', 18)\n",
      "(0.0030625460207481621, 'volare', 151)\n",
      "(0.0030619824293986121, 'cruiser', 79)\n",
      "(0.0030591562541883556, '88', 47)\n",
      "(0.0030488939764578872, 'sport', 43)\n",
      "(0.0030462051763206076, 'j2000', 52)\n",
      "(0.0030458066087392893, 'charger', 119)\n",
      "(0.0030451131915203598, '99e', 180)\n",
      "(0.0030447820301470491, '18i', 72)\n",
      "(0.0030424028420390167, 'toyota', 184)\n",
      "(0.0030389001777263646, 'plymouth', 196)\n",
      "(0.003038614942256916, 'hornet', 257)\n",
      "(0.0030379008830717594, 'magnum', 103)\n",
      "(0.0030362158033261913, 'sedan', 58)\n",
      "(0.0030359416170760065, 'salon', 164)\n",
      "(0.0030346926178205194, 'camaro', 238)\n",
      "(0.0030344737383139705, 'gx', 157)\n",
      "(0.003028014737530543, 'rampage', 74)\n",
      "(0.0030272049515246121, 'zephyr', 244)\n",
      "(0.0030251631441714573, 'royale', 27)\n",
      "(0.0030216794139098194, 'skyhawk', 231)\n",
      "(0.0030210683177442775, '280', 177)\n",
      "(0.0030189942672503023, 'starlet', 21)\n",
      "(0.0030095480531043702, 'f108', 6)\n",
      "(0.0030092810047871706, 'pontiac', 111)\n",
      "(0.0030063506009296399, 'auto', 73)\n",
      "(0.003005823315798687, 'matador', 122)\n",
      "(0.0030056875974054141, 'mercedes', 78)\n",
      "(0.0030035273448690363, 'v6', 16)\n",
      "(0.0030033983583146392, 'thunderbird', 240)\n",
      "(0.0030017061417191384, 'diesel', 228)\n",
      "(0.0029990769542398532, 'tc3', 179)\n",
      "(0.0029960055847352294, 'datsun', 40)\n",
      "(0.0029916659268778888, '504', 104)\n",
      "(0.0029882976309224185, '10', 275)\n",
      "(0.0029849933583736637, 'isuzu', 88)\n",
      "(0.0029794752410549677, 'fury', 96)\n",
      "(0.0029793997899400194, 'buick', 115)\n",
      "(0.0029793399115279665, 'hatchback', 158)\n",
      "(0.0029746168351175513, 'ranger', 1)\n",
      "(0.0029731130078885898, 'woody', 285)\n",
      "(0.0029709796158961525, 'firebird', 159)\n",
      "(0.0029697139138629217, 'ciera', 172)\n",
      "(0.0029652418400587599, 'cougar', 81)\n",
      "(0.0029650255194749392, 'peugeot', 144)\n",
      "(0.0029644213044962998, 'accord', 268)\n",
      "(0.0029628487998612368, 'brougham', 22)\n",
      "(0.0029613727916208599, 'starfire', 187)\n",
      "(0.0029529638717053072, 'dasher', 26)\n",
      "(0.0029529586197026975, 'liftback', 32)\n",
      "(0.0029512363210163952, 'bel', 4)\n",
      "(0.0029431932733419174, 'rx2', 296)\n",
      "(0.0029423477379940787, 'new', 294)\n",
      "(0.0029413557162741777, 'model', 260)\n",
      "(0.0029410903082674154, '99gle', 299)\n",
      "(0.0029358168620055893, '411', 175)\n",
      "(0.0029330407566154205, '1600', 83)\n",
      "(0.0029303985735267056, 'c10', 202)\n",
      "(0.0029284694507452276, 'satellite', 133)\n",
      "(0.0029283033444045831, 'sst', 284)\n",
      "(0.0029278332214857064, 'special', 137)\n",
      "(0.0029249567070710183, 'sportabout', 50)\n",
      "(0.0029242689207405925, '5000s', 82)\n",
      "(0.0029225846679560067, 'chevette', 185)\n",
      "(0.0029210807906561841, 'iii', 295)\n",
      "(0.0029194311673093197, 'man', 267)\n",
      "(0.0029150987945802303, 'skylark', 176)\n",
      "(0.0029129815345337157, 'pickup', 181)\n",
      "(0.0029124021293610068, 'oldsmobile', 256)\n",
      "(0.0029121542871260687, 'royal', 56)\n",
      "(0.0029110926635716895, '300d', 33)\n",
      "(0.0029105296879840593, '280s', 171)\n",
      "(0.0029105170774400436, 'rebel', 39)\n",
      "(0.0029104904811558517, '2300', 131)\n",
      "(0.0029101995928474023, 'astro', 195)\n",
      "(0.0029095462754278089, 'opel', 7)\n",
      "(0.0029094049992911151, 'grand', 49)\n",
      "(0.0029090743264982401, 'volkswagen', 154)\n",
      "(0.0029057813292050344, 'phoenix', 241)\n",
      "(0.0029043927098464186, 'aspen', 169)\n",
      "(0.0029042557213960285, 'ghia', 130)\n",
      "(0.0029010218107370232, 'town', 54)\n",
      "(0.0029004015007682979, 'newport', 198)\n",
      "(0.0028975780443955103, 'challenger', 121)\n",
      "(0.0028972769045662859, '124b', 93)\n",
      "(0.0028916087589066449, 'chevroelt', 128)\n",
      "(0.0028910512894783492, '12tl', 289)\n",
      "(0.0028905671502549839, 'sj', 101)\n",
      "(0.0028902592626622362, 'jetta', 51)\n",
      "(0.0028897554113802163, 'premier', 155)\n",
      "(0.0028891171937547961, 'country', 274)\n",
      "(0.0028839266732178931, '310', 0)\n",
      "(0.0028827513037417889, '145e', 109)\n",
      "(0.0028824440083948721, 'cordoba', 297)\n",
      "(0.0028816339278150806, '128', 283)\n",
      "(0.0028801199125841463, 'arrow', 9)\n",
      "(0.0028764256119364049, '340', 28)\n",
      "(0.0028717910022696062, 'toyouta', 45)\n",
      "(0.0028715208998479117, '264gl', 190)\n",
      "(0.0028684274667290649, 'ambassador', 97)\n",
      "(0.0028673739659680663, 'lesabre', 48)\n",
      "(0.0028671316678955229, 'hi', 182)\n",
      "(0.0028669928260843451, 'cadillac', 100)\n",
      "(0.0028664349167019654, 'sapporo', 223)\n",
      "(0.0028661514448643536, '2h', 34)\n",
      "(0.0028654853447282924, 'chevelle', 235)\n",
      "(0.002865304366313437, 'mustang', 87)\n",
      "(0.0028643961179017721, '5000', 53)\n",
      "(0.0028643270132489729, '225', 250)\n",
      "(0.0028632791378410466, 'horizon', 280)\n",
      "(0.0028614525999551563, 'fairmont', 91)\n",
      "(0.0028601908819852516, 'super', 20)\n",
      "(0.0028600146881587983, 'caprice', 209)\n",
      "(0.0028597064187275713, 'wagon', 206)\n",
      "(0.0028594893661059405, 'yorker', 153)\n",
      "(0.0028570758221565847, 'bmw', 23)\n",
      "(0.0028559844583345532, 'rx', 201)\n",
      "(0.0028553266045516826, 'corona', 65)\n",
      "(0.0028537477159933816, 'citation', 204)\n",
      "(0.002853610575578681, 'ii', 273)\n",
      "(0.00285200828224858, 'rx3', 68)\n",
      "(0.0028519222620635555, 'delta', 46)\n",
      "(0.0028518571087036201, 'eldorado', 13)\n",
      "(0.002850948507819182, 'dart', 254)\n",
      "(0.0028506496402085349, 'rabbit', 161)\n",
      "(0.0028503758162391271, 'pl510', 66)\n",
      "(0.0028500112383253932, 'xe', 148)\n",
      "(0.0028484318512273874, 'cobra', 156)\n",
      "(0.0028481366441283967, '12', 205)\n",
      "(0.0028469508464361096, 'miser', 193)\n",
      "(0.0028462005885414639, 'omega', 12)\n",
      "(0.0028448777686939323, 'nissan', 17)\n",
      "(0.002844300703098455, 'regal', 234)\n",
      "(0.0028438970022681771, '350', 60)\n",
      "(0.0028438036355774945, 'glc', 2)\n",
      "(0.0028437910295554072, 'x1', 134)\n",
      "(0.002843751629824828, 'classic', 86)\n",
      "(0.0028430474325693848, 'st', 71)\n",
      "(0.0028425427572103174, 'luxus', 265)\n",
      "(0.002840849455711318, 'triumph', 44)\n",
      "(0.0028408147249126261, 'ventura', 25)\n",
      "(0.0028405147773074325, 'monarch', 214)\n",
      "(0.0028404915715958949, 'turbo', 207)\n",
      "(0.0028402203823591752, 'mazda', 55)\n",
      "(0.0028395466143834669, 'd200', 293)\n",
      "(0.0028386102222685773, '240d', 152)\n",
      "(0.0028384842257856099, '1131', 5)\n",
      "(0.0028383189499929166, 'sebring', 127)\n",
      "(0.002838049869780201, 'lebaron', 30)\n",
      "(0.0028376075430125974, 'aries', 281)\n",
      "(0.0028371312355128489, 'mark', 132)\n",
      "(0.0028370093545692578, '131', 233)\n",
      "(0.0028369195989500205, 'medallion', 142)\n",
      "(0.0028368237877844931, 'futura', 3)\n",
      "(0.0028367442215165963, 'marquis', 199)\n",
      "(0.002836595817912989, 'manta', 269)\n",
      "(0.0028365258869086302, 'monte', 258)\n",
      "(0.0028363519029409924, 'reliant', 118)\n",
      "(0.0028355010051868243, 'vokswagen', 219)\n",
      "(0.0028347945219330448, '4000', 245)\n",
      "(0.0028345041547000183, 'concord', 105)\n",
      "(0.0028343558232983741, 'lecar', 70)\n",
      "(0.0028340956028869448, '4w', 11)\n",
      "(0.002833765838888475, 'sx', 166)\n",
      "(0.0028336334139802081, 'subaru', 31)\n",
      "(0.0028334497271416095, 'mpg', 57)\n",
      "(0.0028334397665842272, 'lj', 186)\n",
      "(0.0028326077176338066, 'c20', 252)\n",
      "(0.0028325978002402587, 'saab', 247)\n",
      "(0.0028323219647109856, 'gs', 165)\n",
      "(0.002831937298616795, 'limited', 249)\n",
      "(0.0028318489388031217, '500', 42)\n",
      "(0.0028311633746404539, '626', 287)\n",
      "(0.0028298434019515826, 'mercury', 29)\n",
      "(0.0028292120671382158, '710', 112)\n",
      "(0.0028289659821540336, 'supreme', 286)\n",
      "(0.0028287969303834731, 'volvo', 146)\n",
      "(0.0028286397992729004, '1300', 246)\n",
      "(0.0028286129466490325, 'stanza', 211)\n",
      "(0.0028285121624330748, 'cuda', 136)\n",
      "(0.0028285020070465045, 'seville', 14)\n",
      "(0.0028282859155352156, 'carlo', 255)\n",
      "(0.0028281310973769938, 'monaco', 242)\n",
      "(0.0028280586370901077, 'celica', 229)\n",
      "(0.0028279913755881302, '304', 15)\n",
      "(0.0028279466647642203, 'lemans', 271)\n",
      "(0.0028275548242947292, 'malibu', 270)\n",
      "(0.0028275101537478149, 'maxima', 248)\n",
      "(0.0028273140602719021, 'renault', 38)\n",
      "(0.0028268876172608368, '320', 64)\n",
      "(0.0028267988063756049, 'gremlin', 298)\n",
      "(0.0028261190483324181, '1200', 124)\n",
      "(0.0028259601523627068, 'runabout', 203)\n",
      "(0.0028239552485797548, 'estate', 224)\n",
      "(0.0028223402547714353, 'gran', 191)\n",
      "(0.0028221199895507435, 'nova', 266)\n",
      "(0.002818762601129475, 'custom', 8)\n",
      "(0.0028187276317817203, '245', 10)\n",
      "(0.0028101846297231975, 'ltd', 230)\n",
      "(0.0028027698079231551, 'duster', 236)\n",
      "(0.0028010389307663404, 'b210', 114)\n",
      "(0.0027803945115550685, 'suburb', 113)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#x = x.toarray() #.as_matrix()\n",
    "y = df['mpg'].as_matrix()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(x,y,verbose=0,epochs=1000)\n",
    "\n",
    "# Rank features\n",
    "ranker = InputPerturbationRank\n",
    "print()\n",
    "print(\"*** Feature Ranking ***\")\n",
    "l1 = ranker(bag_cols).rank(x, y, model)\n",
    "\n",
    "for itm in l1:\n",
    "    print(itm)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Examples: Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Time series data will need to be encoded for a regular feedforward neural network.  In a few classes we will see how to use a recurrent neural network to find patterns over time.  For now, we will encode the series into input neurons.\n",
    "\n",
    "Financial forecasting is a very popular form of temporal algorithm. A temporal algorithm is one that accepts input for values that range over time. If the algorithm supports short term memory (internal state) then ranges over time are supported automatically. If your algorithm does not have an internal state then you should use an input window and a prediction window. Most algorithms do not have an internal state. To see how to use these windows, consider if you would like the algorithm to predict the stock market. You begin with the closing price for a stock over several days:\n",
    "\n",
    "```\n",
    "Day 1 : $45\n",
    "Day 2 : $47\n",
    "Day 3 : $48\n",
    "Day 4 : $40\n",
    "Day 5 : $41\n",
    "Day 6 : $43\n",
    "Day 7 : $45\n",
    "Day 8 : $57\n",
    "Day 9 : $50\n",
    "Day 10 : $41\n",
    "```\n",
    "\n",
    "The first step is to normalize the data. This is necessary whether your algorithm has internal state or not. To normalize, we want to change each number into the percent movement from the previous day. For example, day 2 would become 0.04, because there is a 4% difference between $45 and $47. Once you perform this calculation for every day, the data set will look like the following:\n",
    "\n",
    "```\n",
    "Day 2 : 0. 04\n",
    "Day 3 : 0. 02\n",
    "Day 4:−0.16\n",
    "Day 5 : 0. 02\n",
    "Day 6 : 0. 04\n",
    "Day 7 : 0. 04\n",
    "Day 8 : 0. 04\n",
    "Day 9:−0.12\n",
    "Day 10:−0.18\n",
    "```\n",
    "\n",
    "In order to create an algorithm that will predict the next day’s values, we need to think about how to encode this data to be presented to the algorithm. The encoding depends on whether the algorithm has an internal state. The internal state allows the algorithm to use the last few values inputted to help establish trends.\n",
    "\n",
    "Many machine learning algorithms have no internal state. If this is the case, then you will typically use a sliding window algorithm to encode the data. To do this, we use the last three prices to predict the next one. The inputs would be the last three-day prices, and the output would be the fourth day. The above data could be organized in the following way to provide training data.\n",
    "\n",
    "These cases specified the ideal output for the given inputs:\n",
    "\n",
    "```\n",
    "[ 0.04 , 0.02 , −0.16 ] −> 0.02\n",
    "[ 0.02 , −0.16 , 0.02 ] −> 0.04\n",
    "[ −0.16 , 0.02 , 0.04 ] −> 0.04\n",
    "[ 0.02 , 0.04 , 0.04 ] −> 0. 26\n",
    "[ 0.04 , 0.04 , 0.26 ] −> −0.12\n",
    "[ 0.04 , 0.26 , −0.12 ] −> −0.18\n",
    "```\n",
    "\n",
    "The above encoding would require that the algorithm have three inputs and one output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized price history:\n",
      "[0.044444444444444446, 0.02127659574468085, -0.16666666666666666, 0.025, 0.04878048780487805, 0.046511627906976744, 0.26666666666666666, -0.12280701754385964, -0.18]\n",
      "\n",
      "Rounded normalized price history:\n",
      "[ 0.04  0.02 -0.17  0.02  0.05  0.05  0.27 -0.12 -0.18]\n",
      "\n",
      "Time Boxed(time series encoded):\n",
      "[ 0.04  0.02 -0.17] -> [ 0.02]\n",
      "[ 0.02 -0.17  0.02] -> [ 0.05]\n",
      "[-0.17  0.02  0.05] -> [ 0.05]\n",
      "[ 0.02  0.05  0.05] -> [ 0.27]\n",
      "[ 0.05  0.05  0.27] -> [-0.12]\n",
      "[ 0.05  0.27 -0.12] -> [-0.18]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize_price_change(history):\n",
    "    last = None\n",
    "    \n",
    "    result = []\n",
    "    for price in history:\n",
    "        if last is not None:\n",
    "            result.append( float(price-last)/last )\n",
    "        last = price\n",
    "\n",
    "    return result\n",
    "\n",
    "def encode_timeseries_window(source, lag_size, lead_size):\n",
    "    \"\"\"\n",
    "    Encode raw data to a time-series window.\n",
    "    :param source: A 2D array that specifies the source to be encoded.\n",
    "    :param lag_size: The number of rows uses to predict.\n",
    "    :param lead_size: The number of rows to be predicted\n",
    "    :return: A tuple that contains the x (input) & y (expected output) for training.\n",
    "    \"\"\"\n",
    "    result_x = []\n",
    "    result_y = []\n",
    "\n",
    "    output_row_count = len(source) - (lag_size + lead_size) + 1\n",
    "    \n",
    "\n",
    "    for raw_index in range(output_row_count):\n",
    "        encoded_x = []\n",
    "\n",
    "        # Encode x (predictors)\n",
    "        for j in range(lag_size):\n",
    "            encoded_x.append(source[raw_index+j])\n",
    "\n",
    "        result_x.append(encoded_x)\n",
    "\n",
    "        # Encode y (prediction)\n",
    "        encoded_y = []\n",
    "\n",
    "        for j in range(lead_size):\n",
    "            encoded_y.append(source[lag_size+raw_index+j])\n",
    "\n",
    "        result_y.append(encoded_y)\n",
    "\n",
    "    return result_x, result_y\n",
    "\n",
    "\n",
    "price_history = [ 45, 47, 48, 40, 41, 43, 45, 57, 50, 41 ]\n",
    "norm_price_history = normalize_price_change(price_history)\n",
    "\n",
    "print(\"Normalized price history:\")\n",
    "print(norm_price_history)\n",
    "\n",
    "print()\n",
    "print(\"Rounded normalized price history:\")\n",
    "norm_price_history = np.round(norm_price_history,2)\n",
    "print(norm_price_history)\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Time Boxed(time series encoded):\")\n",
    "x, y = encode_timeseries_window(norm_price_history, 3, 1)\n",
    "\n",
    "for x_row, y_row in zip(x,y):\n",
    "    print(\"{} -> {}\".format(np.round(x_row,2), np.round(y_row,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
