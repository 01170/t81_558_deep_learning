{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Class 5: Backpropagation.**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Functions from Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df,name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name,x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the origional column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df,name,target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x)==str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name,tv)\n",
    "        df[name2] = l\n",
    "    \n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df,name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df,name,mean=None,sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name]-mean)/sd\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df,target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    \n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        return df.as_matrix(result).astype(np.float32),df.as_matrix([target]).astype(np.int32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32),df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "# Regression chart, we will see more of this chart in the next class.\n",
    "def chart_regression(pred,y):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y_test.flatten()})\n",
    "    t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# Get a new directory to hold checkpoints from a neural network.  This allows the neural network to be\n",
    "# loaded later.  If the erase param is set to true, the contents of the directory will be cleared.\n",
    "def get_model_dir(name,erase):\n",
    "    base_path = os.path.join(\".\",\"dnn\")\n",
    "    model_dir = os.path.join(base_path,name)\n",
    "    os.makedirs(model_dir,exist_ok=True)\n",
    "    if erase and len(model_dir)>4 and os.path.isdir(model_dir):\n",
    "        shutil.rmtree(model_dir,ignore_errors=True) # be careful, this deletes everything below the specified path\n",
    "    return model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classic Backpropagation\n",
    "Backpropagation is the primary means by which a neural network's weights are determined during training. Backpropagation works by calculating a weight change amount ($v_t$) for every weight($\\theta$, theata) in the neural network.  This value is subtracted from every weight by the following equation: \n",
    "\n",
    "$ \\theta_t = \\theta_{t-1} - v_t $\n",
    "\n",
    "This process is repeated for every iteration($t$).  How the weight change is calculated depends on the training algorithm.  Classic backpropagation simply calculates a gradient ($\\nabla$, nabla) for every weight in the neural network with respect to the error function ($J$) of the neural network.  The gradient is scaled by a learning rate ($\\eta$, eta).\n",
    "\n",
    "$ v_t = \\eta \\nabla_{\\theta_{t-1}} J(\\theta_{t-1}) $\n",
    "\n",
    "The learning rate is an important concept for backpropagation training.  Setting the learning rate can be complex:\n",
    "\n",
    "* Too low of a learning rate will usually converge to a good solution; however, the process will be very slow.\n",
    "* Too high of a learning rate will either fail outright, or converge to a higher error than a better learning rate.\n",
    "\n",
    "Common values for learning rate are: 0.1, 0.01, 0.001, etc.\n",
    "\n",
    "Gradients:\n",
    "\n",
    "![Derivative](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_2_deriv.png \"Derivative\")\n",
    "\n",
    "The following link, from the book, shows how a simple [neural network is trained with backpropagation](http://www.heatonresearch.com/aifh/vol3/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum Backpropagation\n",
    "\n",
    "Momentum adds another term to the calculation of $v_t$:\n",
    "\n",
    "$ v_t = \\eta \\nabla_{\\theta_{t-1}} J(\\theta_{t-1}) + \\lambda v_{t-1} $\n",
    "\n",
    "Like the learning rate, momentum adds another training parameter that scales the effect of momentum.  Momentum backpropagation has two training parameters: learning rate ($\\eta$, eta) and momentum ($\\lambda$, lambda).  Momentum simply adds the scaled value of the previous weight change amount ($v_{t-1}$) to the current weight change amount($v_t$).\n",
    "\n",
    "This has the effect of adding additional force behind a direction a weight was moving.  This might allow the weight to escape a local minima:\n",
    "\n",
    "![Momentum](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_5_momentum.png \"Momentum\")\n",
    "\n",
    "A very common value for momentum is 0.9.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch and Online Backpropagation\n",
    "\n",
    "How often should the weights of a neural network be updated?  Gradients can be calculated for a training set element.  These gradients can also be summed together into batches and the weights updated once per batch.\n",
    "\n",
    "* **Online Training** - Update the weights based on gradients calculated from a single training set element.\n",
    "* **Batch Training** - Update the weights based on the sum of the gradients over all training set elements.\n",
    "* **Batch Size** - Update the weights based on the sum of some batch size of training set elements.\n",
    "* **Mini-Batch Training** - The same as batch size, but with a very small batch size.  Mini-batches are very popular and they are often in the 32-64 element range.\n",
    "\n",
    "Because the batch size is smaller than the complete training set size, it may take several batches to make it completely through the training set.  \n",
    "\n",
    "* **Step/Iteration** - The number of batches that were processed.\n",
    "* **Epoch** - The number of times the complete training set was processed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "Stochastic gradient descent (SGD) is currently one of the most popular neural network training algorithms.  It works very similarly to Batch/Mini-Batch training, except that the batches are made up of a random set of training elements.\n",
    "\n",
    "This leads to a very irregular convergence in error during training:\n",
    "\n",
    "![SGD Error](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_5_sgd_error.png \"SGD Error\")\n",
    "[Image from Wikipedia](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
    "\n",
    "Because the neural network is trained on a random sample of the complete training set each time, the error does not make a smooth transition downward.  However, the error usually does go down.\n",
    "\n",
    "Advantages to SGD include:\n",
    "\n",
    "* Computationally efficient.  Even with a very large training set, each training step can be relatively fast.\n",
    "* Decreases overfitting by focusing on only a portion of the training set each step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Techniques\n",
    "\n",
    "One problem with simple backpropagation training algorithms is that they are highly sensative to learning rate and momentum.  This is difficult because:\n",
    "\n",
    "* Learning rate must be adjusted to a small enough level to train an accurate neural network.\n",
    "* Momentum must be large enough to overcome local minima, yet small enough to not destabilize the training.\n",
    "* A single learning rate/momentum is often not good enough for the entire training process. It is often useful to automatically decrease learning rate as the training progresses.\n",
    "* All weights share a single learning rate/momentum.\n",
    "\n",
    "Other training techniques:\n",
    "\n",
    "* **Resilient Propagation** - Use only the magnitude of the gradient and allow each neuron to learn at its own rate.  No need for learning rate/momentum; however, only works in full batch mode.\n",
    "* **Nesterov accelerated gradient** - Helps midigate the risk of choosing a bad mini-batch.\n",
    "* **Adagrad** - Allows an automatically decaying per-weight learning rate and momentum concept.\n",
    "* **Adadelta** - Extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate.\n",
    "* **Non-Gradient Methods** - Non-gradient methods can *sometimes* be useful, though rarely outperform gradient-based backpropagation methods.  These include: [simulated annealing](https://en.wikipedia.org/wiki/Simulated_annealing), [genetic algorithms](https://en.wikipedia.org/wiki/Genetic_algorithm), [particle swarm optimization](https://en.wikipedia.org/wiki/Particle_swarm_optimization), [Nelder Mead](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method), and [many more](https://en.wikipedia.org/wiki/Category:Optimization_algorithms_and_methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAM Update\n",
    "\n",
    "ADAM is the first training algorithm you should try.  It is very effective.  Kingma and Ba (2014) introduced the Adam update rule that derives its name from the adaptive moment estimates that it uses.  Adam estimates the first (mean) and second (variance) moments to determine the weight corrections.  Adam begins with an exponentially decaying average of past gradients (m):\n",
    "\n",
    "$ m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t $\n",
    "\n",
    "This average accomplishes a similar goal as classic momentum update; however, its value is calculated automatically based on the current gradient ($g_t$).  The update rule then calculates the second moment ($v_t$):\n",
    "\n",
    "$ v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2 $\n",
    "\n",
    "The values $m_t$ and $v_t$ are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively.  However, they will have a strong bias towards zero in the initial training cycles.  The first moment’s bias is corrected as follows.\n",
    "\n",
    "$ \\hat{m}_t = \\frac{m_t}{1-\\beta^t_1} $\n",
    "\n",
    "Similarly, the second moment is also corrected:\n",
    "\n",
    "$ \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t} $\n",
    "\n",
    "These bias-corrected first and second moment estimates are applied to the ultimate Adam update rule, as follows:\n",
    "\n",
    "$ \\theta_t = \\theta_{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}_t}+\\eta} \\hat{m}_t $\n",
    "\n",
    "Adam is very tolerant to initial learning rate (η) and other training parameters. Kingma and Ba (2014)  propose default values of 0.9 for $\\beta_1$, 0.999 for $\\beta_2$, and 10-8 for $\\eta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods Compared\n",
    "\n",
    "The following image shows how each of these algorithms train (image credits: [author](Alec Radford), [where I found it](http://sebastianruder.com/optimizing-gradient-descent/index.html#visualizationofalgorithms) ):\n",
    "\n",
    "![Training Techniques](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/contours_evaluation_optimizers.gif \"Training Techniques\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying the Update Rule in Tensorflow\n",
    "\n",
    "TensorFlow allows the update rule to be set to one of:\n",
    "\n",
    "* Adagrad\n",
    "* **Adam**\n",
    "* Ftrl\n",
    "* Momentum\n",
    "* RMSProp\n",
    "* **SGD**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (RMSE): 7.535488605499268\n",
      "Best step: 30775, Last successful step: 31785\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAFkCAYAAABvkjJwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEjMKHiIiIhIzCh4iIiISMz8P2jtxbB9mwSnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25f2826a588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.contrib.learn as learn\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "\n",
    "# Set the desired TensorFlow output level for this example\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "filename_read = os.path.join(path,\"auto-mpg.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "# create feature vector\n",
    "missing_median(df, 'horsepower')\n",
    "df.drop('name',1,inplace=True)\n",
    "encode_numeric_zscore(df, 'horsepower')\n",
    "encode_numeric_zscore(df, 'weight')\n",
    "encode_numeric_zscore(df, 'cylinders')\n",
    "encode_numeric_zscore(df, 'displacement')\n",
    "encode_numeric_zscore(df, 'acceleration')\n",
    "encode_text_dummy(df, 'origin')\n",
    "\n",
    "# Encode to a 2D matrix for training\n",
    "x,y = to_xy(df,'mpg')\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Get/clear a directory to store the neural network to\n",
    "model_dir = get_model_dir('mpg',True)\n",
    "\n",
    "# Choose an optimizer\n",
    "#opt=tf.train.AdamOptimizer()\n",
    "opt=tf.train.MomentumOptimizer(learning_rate=1e-5,momentum=0.9)\n",
    "\n",
    "# Create a deep neural network with 3 hidden layers\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x.shape[0])]\n",
    "regressor = learn.DNNRegressor(\n",
    "    model_dir= model_dir,\n",
    "    optimizer=opt,\n",
    "    config=tf.contrib.learn.RunConfig(save_checkpoints_secs=1),\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=[25, 5])\n",
    "\n",
    "# Might be needed in future versions of \"TensorFlow Learn\"\n",
    "#regressor = learn.SKCompat(regressor) # For Sklearn compatibility\n",
    "\n",
    "# Early stopping\n",
    "validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    every_n_steps=5,\n",
    "    early_stopping_metric=\"loss\",\n",
    "    early_stopping_metric_minimize=True,\n",
    "    early_stopping_rounds=1000)\n",
    "\n",
    "# Fit/train neural network\n",
    "regressor.fit(x_train, y_train,monitors=[validation_monitor],steps=50000)\n",
    "\n",
    "# Predict and measure RMSE\n",
    "pred = list(regressor.predict(x_test, as_iterable=True))\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Score (RMSE): {}\".format(score))\n",
    "print(\"Best step: {}, Last successful step: {}\".format(\n",
    "    validation_monitor.best_step,validation_monitor._last_successful_step))\n",
    "\n",
    "# Plot the chart\n",
    "chart_regression(pred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Important Parameters\n",
    "\n",
    "* **learning_rate** - How quickly should the optimizer attempt to train the neural network.  Too high will fail to train.  Too low will train too slowly.\n",
    "* **momentum** - Only used with the momentum optimizer.  How much of the previous weight change direction should be used in the current step.\n",
    "* **every_n_steps** - How often should the validation set be evaluated.\n",
    "* **early_stopping_rounds** - How many rounds (steps) of non-improvement should be tolerated.\n",
    "\n",
    "# Some results\n",
    "* **ADAM**, learning_rate = 1e-1, early_stopping = 100\n",
    "    * Score (RMSE): 3.5879969596862793\n",
    "    * Best step: 1505, Last successful step: 1640\n",
    "* **ADAM**, learning_rate = 1e-3, early_stopping = 100\n",
    "    * Score (RMSE): 2.4205124378204346\n",
    "    * Best step: 4650, Last successful step: 4770\n",
    "* **ADAM**, default learning rate, early_stopping = 1000\n",
    "    * Score (RMSE): 2.410527467727661\n",
    "    * Best step: 6630, Last successful step: 7720\n",
    "* **Momentum**, learning_rate=1e-5,momentum=0.9, early_stopping = 1000\n",
    "    * Score (RMSE): 2.4050464630126953\n",
    "    * Best step: 41270, Last successful step: 42280\n",
    "\n",
    "ADAM will converge quicker (fewer steps) than momentum and get a good RMSE.  For all, it is important to allow enough early_stopping steps to find a good solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:gpu]",
   "language": "python",
   "name": "conda-env-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
