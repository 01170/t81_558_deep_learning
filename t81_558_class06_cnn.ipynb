{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YDTXd8-Lmp8Q"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 6: Convolutional Neural Networks (CNN) for Computer Vision**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ncNrAEpzmp8S"
   },
   "source": [
    "# Module Video Material\n",
    "\n",
    "Main video lecture:\n",
    "\n",
    "* [Part 6.1: Image Processing in Python](https://www.youtube.com/watch?v=Oe0-hX4KSZQ&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN&index=18)\n",
    "* [Part 6.2: Keras Neural Networks for Digits and Fashion MINST](https://www.youtube.com/watch?v=GDlI-3O5r6I&index=19&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN)\n",
    "* [Part 6.3: Implementing a ResNet in Keras](https://www.youtube.com/watch?v=zUZRUTJbYm8&index=20&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN)\n",
    "* [Part 6.4: Using Your Own Images with Keras](https://www.youtube.com/watch?v=zUZRUTJbYm8&index=20&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN)\n",
    "* [Part 6.5: Recognizing Multiple Images with Darknet](https://www.youtube.com/watch?v=zUZRUTJbYm8&index=20&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fU9UhAxTmp8S"
   },
   "outputs": [],
   "source": [
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return f\"{h}:{m:>02}:{s:>05.2f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-qb-mcqmp8U"
   },
   "source": [
    "# Part 6.1: Image Processing in Python\n",
    "\n",
    "We will make use of images to demonstrate auto encoders.  To use images in Python, we will make use of the Pillow package.  This package can be installed with the following command.  \n",
    "\n",
    "```\n",
    "pip install pillow\n",
    "```\n",
    "\n",
    "The following program uses Pillow to load and display an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6gCRC1P9mp8V",
    "outputId": "8c0decda-da31-425c-e4b2-486e248cfaf9"
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "from matplotlib.pyplot import imshow\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/9/92/Brookings.jpg\"\n",
    "\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img.load()\n",
    "\n",
    "print(np.asarray(img))\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nKtGkM3Omp8Y"
   },
   "source": [
    "### Creating Images (from pixels) in Python\n",
    "\n",
    "Pillow can also be used to create an image from a 3D numpy cube.  The rows and columns specify the pixels.  The depth, of 3, specifies red, green and blue.  Here a simple image is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ev3jVU9_mp8Z",
    "outputId": "6abd66d7-7883-4892-aa1d-7631f57173b5"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "w, h = 64, 64\n",
    "data = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "\n",
    "# Yellow\n",
    "for row in range(32):\n",
    "    for col in range(32):\n",
    "        data[row,col] = [255,255,0]\n",
    "        \n",
    "# Red\n",
    "for row in range(32):\n",
    "    for col in range(32):\n",
    "        data[row+32,col] = [255,0,0]\n",
    "        \n",
    "# Green\n",
    "for row in range(32):\n",
    "    for col in range(32):\n",
    "        data[row+32,col+32] = [0,255,0]        \n",
    "        \n",
    "# Blue\n",
    "for row in range(32):\n",
    "    for col in range(32):\n",
    "        data[row,col+32] = [0,0,255]                \n",
    "        \n",
    "\n",
    "img = Image.fromarray(data, 'RGB')\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dzsIlZ8Pmp8b"
   },
   "source": [
    "### Transform Images in Python (at the pixel level)\n",
    "\n",
    "We can combine the last two programs and modify images.  Here we take the mean color of each pixel and form a grayscale image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bSz9xwZYmp8c",
    "outputId": "b99c1908-f8b4-4d24-8a1b-f6bd3d0dde53"
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "from matplotlib.pyplot import imshow\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/9/92/Brookings.jpg\"\n",
    "#url = \"http://www.heatonresearch.com/images/about-jeff.jpg\"\n",
    "\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img.load()\n",
    "\n",
    "img_array = np.asarray(img)\n",
    "rows = img_array.shape[0]\n",
    "cols = img_array.shape[1]\n",
    "\n",
    "print(\"Rows: {}, Cols: {}\".format(rows,cols))\n",
    "\n",
    "# Create new image\n",
    "img2_array = np.zeros((rows, cols, 3), dtype=np.uint8)\n",
    "for row in range(rows):\n",
    "    for col in range(cols):\n",
    "        t = np.mean(img_array[row,col])\n",
    "        img2_array[row,col] = [t,t,t]\n",
    "\n",
    "img2 = Image.fromarray(img2_array, 'RGB')\n",
    "img2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tHiHA22Zmp8e"
   },
   "source": [
    "# Standardize Images\n",
    "\n",
    "When processing several images together it is sometimes important to standardize them.  The following code reads a sequence of images and causes them to all be of the same size and perfectly square.  If the input images are not square, cropping will occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7OvopXw1mp8f",
    "outputId": "17d693b9-de7e-41e3-d289-d320b0966864"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from PIL import Image, ImageFile\n",
    "from matplotlib.pyplot import imshow\n",
    "import requests\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "#url = \"http://www.heatonresearch.com/images/about-jeff.jpg\"\n",
    "\n",
    "images = [\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/9/92/Brookings.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/f/ff/WashU_Graham_Chapel.JPG\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/9/9e/SeigleHall.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/a/aa/WUSTLKnight.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/3/32/WashUABhall.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/c/c0/Brown_Hall.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/f/f4/South40.jpg\"    \n",
    "]\n",
    "\n",
    "\n",
    "def make_square(img):\n",
    "    cols,rows = img.size\n",
    "    \n",
    "    if rows>cols:\n",
    "        pad = (rows-cols)/2\n",
    "        img = img.crop((pad,0,cols,cols))\n",
    "    else:\n",
    "        pad = (cols-rows)/2\n",
    "        img = img.crop((0,pad,rows,rows))\n",
    "    \n",
    "    return img\n",
    "    \n",
    "x = [] \n",
    "    \n",
    "for url in images:\n",
    "    ImageFile.LOAD_TRUNCATED_IMAGES = False\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    img.load()\n",
    "    img = make_square(img)\n",
    "    img = img.resize((128,128), Image.ANTIALIAS)\n",
    "    print(url)\n",
    "    display(img)\n",
    "    img_array = np.asarray(img)\n",
    "    img_array = img_array.flatten()\n",
    "    img_array = img_array.astype(np.float32)\n",
    "    img_array = (img_array-128)/128\n",
    "    x.append(img_array)\n",
    "    \n",
    "\n",
    "x = np.array(x)\n",
    "\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X9yPZjf6mp8h"
   },
   "source": [
    "### Adding Noise to an Image\n",
    "\n",
    "Auto encoders can handle noise.  First it is important to see how to add noise to an image.  There are many ways to add such noise.  The following code adds random black squares to the image to produce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xhnc_8D9mp8h",
    "outputId": "1465d287-29cf-48ec-c8b2-3ee095aacb9f"
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "from matplotlib.pyplot import imshow\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def add_noise(a):\n",
    "    a2 = a.copy()\n",
    "    rows = a2.shape[0]\n",
    "    cols = a2.shape[1]\n",
    "    s = int(min(rows,cols)/20) # size of spot is 1/20 of smallest dimension\n",
    "    \n",
    "    for i in range(100):\n",
    "        x = np.random.randint(cols-s)\n",
    "        y = np.random.randint(rows-s)\n",
    "        a2[y:(y+s),x:(x+s)] = 0\n",
    "        \n",
    "    return a2\n",
    "\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/9/92/Brookings.jpg\"\n",
    "#url = \"http://www.heatonresearch.com/images/about-jeff.jpg\"\n",
    "\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img.load()\n",
    "\n",
    "img_array = np.asarray(img)\n",
    "rows = img_array.shape[0]\n",
    "cols = img_array.shape[1]\n",
    "\n",
    "print(\"Rows: {}, Cols: {}\".format(rows,cols))\n",
    "\n",
    "# Create new image\n",
    "img2_array = img_array.astype(np.uint8)\n",
    "print(img2_array.shape)\n",
    "img2_array = add_noise(img2_array)\n",
    "img2 = Image.fromarray(img2_array, 'RGB')\n",
    "img2        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jf_otSJdmp8k"
   },
   "source": [
    "# Part 6.2: Keras Neural Networks for Digits and Fashion MINST\n",
    "\n",
    "# Computer Vision\n",
    "\n",
    "This class will focus on computer vision.  There are some important differences and similarities with previous neural networks.\n",
    "\n",
    "* We will usually use classification, though regression is still an option.\n",
    "* The input to the neural network is now 3D (height, width, color)\n",
    "* Data are not transformed, no z-scores or dummy variables.\n",
    "* Processing time is much longer.\n",
    "* We now have different layer times: dense layers (just like before), convolution layers and max pooling layers.\n",
    "* Data will no longer arrive as CSV files. TensorFlow provides some utilities for going directly from image to the input for a neural network.\n",
    "\n",
    "\n",
    "# Computer Vision Data Sets\n",
    "\n",
    "There are many data sets for computer vision.  Two of the most popular are the MNIST digits data set and the CIFAR image data sets.\n",
    "\n",
    "### MNIST Digits Data Set\n",
    "\n",
    "The [MNIST Digits Data Set](http://yann.lecun.com/exdb/mnist/) is very popular in the neural network research community.  A sample of it can be seen here:\n",
    "\n",
    "![MNIST Data Set](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_8_mnist.png \"MNIST Data Set\")\n",
    "\n",
    "This data set was generated from scanned forms.\n",
    "\n",
    "![Exam](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_8_exam.png \"Exam\")\n",
    "\n",
    "### MNIST Fashion Data Set\n",
    "\n",
    "[Fashion-MNIST](https://www.kaggle.com/zalando-research/fashionmnist) is a dataset of [Zalando](https://jobs.zalando.com/tech/)'s article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Fashion-MNIST is intended to serve as a direct **drop-in replacement** for the original [MNIST dataset](http://yann.lecun.com/exdb/mnist/) for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n",
    "\n",
    "![minst-fashion](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/mnist-fashion.png \"minst-fashion\")\n",
    "\n",
    "### CIFAR Data Set\n",
    "\n",
    "The [CIFAR-10 and CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) datasets are also frequently used by the neural network research community.\n",
    "\n",
    "![cifar-10](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_8_cifar.png \"cifar-10\")\n",
    "\n",
    "The CIFAR-10 data set contains low-rez images that are divided into 10 classes.  The CIFAR-100 data set contains 100 classes in a hierarchy. \n",
    "\n",
    "# Other Resources\n",
    "\n",
    "* [Imagenet:Large Scale Visual Recognition Challenge 2014](http://image-net.org/challenges/LSVRC/2014/index)\n",
    "* [Andrej Karpathy](http://cs.stanford.edu/people/karpathy/) - PhD student/instructor at Stanford.\n",
    "    * [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/) - Stanford course on computer vision/CNN's.\n",
    "        * [CS231n - GitHub](http://cs231n.github.io/)\n",
    "    * [ConvNetJS](http://cs.stanford.edu/people/karpathy/convnetjs/) - JavaScript library for deep learning.\n",
    " \n",
    "\n",
    "# Convolutional Neural Networks (CNNs)\n",
    "\n",
    "The convolutional neural network (CNN) is a neural network technology that has profoundly impacted the area of computer vision (CV). Fukushima (1980) introduced the original concept of a convolutional neural network, and LeCun, Bottou, Bengio & Haffner (1998) greatly improved this work. From this research, Yan LeCun introduced the famous LeNet-5 neural network architecture. This class follows the LeNet-5 style of convolutional neural network.\n",
    "\n",
    "**A LeNET-5 Network (LeCun, 1998)**\n",
    "![LENET5](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_8_lenet5.png \"LENET5\")\n",
    "\n",
    "So far we have only seen one layer type (dense layers).  By the end of this course we will have seen:\n",
    "\n",
    "* **Dense Layers** - Fully connected layers.  (introduced previously)\n",
    "* **Convolution Layers** - Used to scan across images. (introduced this class)\n",
    "* **Max Pooling Layers** - Used to downsample images. (introduced this class)\n",
    "* **Dropout Layer** - Used to add regularization. (introduced next class)\n",
    "\n",
    "## Convolution Layers\n",
    "\n",
    "The first layer that we will examine is the convolutional layer. We will begin by looking at the hyper-parameters that you must specify for a convolutional layer in most neural network frameworks that support the CNN:\n",
    "\n",
    "* Number of filters\n",
    "* Filter Size\n",
    "* Stride\n",
    "* Padding\n",
    "* Activation Function/Non-Linearity\n",
    "\n",
    "The primary purpose for a convolutional layer is to detect features such as edges, lines, blobs of color, and other visual elements. The filters can detect these features. The more filters that we give to a convolutional layer, the more features it can detect.\n",
    "\n",
    "A filter is a square-shaped object that scans over the image. A grid can represent the individual pixels of a grid. You can think of the convolutional layer as a smaller grid that sweeps left to right over each row of the image. There is also a hyper parameter that specifies both the width and height of the square-shaped filter. Figure 10.1 shows this configuration in which you see the six convolutional filters sweeping over the image grid:\n",
    "\n",
    "A convolutional layer has weights between it and the previous layer or image grid. Each pixel on each convolutional layer is a weight. Therefore, the number of weights between a convolutional layer and its predecessor layer or image field is the following:\n",
    "\n",
    "```\n",
    "[FilterSize] * [FilterSize] * [# of Filters]\n",
    "```\n",
    "\n",
    "For example, if the filter size were 5 (5x4) for 10 filters, there would be 250 weights.\n",
    "\n",
    "You need to understand how the convolutional filters sweep across the previous layer’s output or image grid. Figure 10.2 illustrates the sweep:\n",
    "\n",
    "![CNN](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_8_cnn_grid.png \"CNN\")\n",
    "\n",
    "The above figure shows a convolutional filter with a size of 4 and a padding size of 1. The padding size is responsible for the boarder of zeros in the area that the filter sweeps. Even though the image is actually 8x7, the extra padding provides a virtual image size of 9x8 for the filter to sweep across. The stride specifies the number of positions at which the convolutional filters will stop. The convolutional filters move to the right, advancing by the number of cells specified in the stride. Once the far right is reached, the convolutional filter moves back to the far left, then it moves down by the stride amount and\n",
    "continues to the right again.\n",
    "\n",
    "Some constraints exist in relation to the size of the stride. Obviously, the stride cannot be 0. The convolutional filter would never move if the stride were set to 0. Furthermore, neither the stride, nor the convolutional filter size can be larger than the previous grid. There are additional constraints on the stride (s), padding (p) and the filter width (f) for an image of width (w). Specifically, the convolutional filter must be able to start at the far left or top boarder, move a certain number of strides, and land on the far right or bottom boarder. The following equation shows the number of steps a convolutional operator\n",
    "must take to cross the image:\n",
    "\n",
    "$ steps = \\frac{w - f + 2p}{s+1} $\n",
    "\n",
    "The number of steps must be an integer. In other words, it cannot have decimal places. The purpose of the padding (p) is to be adjusted to make this equation become an integer value.\n",
    "\n",
    "## Max Pooling Layers\n",
    "\n",
    "Max-pool layers downsample a 3D box to a new one with smaller dimensions. Typically, you can always place a max-pool layer immediately following convolutional layer. The LENET shows the max-pool layer immediately after layers C1 and C3. These max-pool layers progressively decrease the size of the dimensions of the 3D boxes passing through them. This technique can avoid overfitting (Krizhevsky, Sutskever & Hinton, 2012).\n",
    "\n",
    "A pooling layer has the following hyper-parameters:\n",
    "\n",
    "* Spatial Extent (f )\n",
    "* Stride (s)\n",
    "\n",
    "Unlike convolutional layers, max-pool layers do not use padding. Additionally, max-pool layers have no weights, so training does not affect them. These layers simply downsample their 3D box input. The 3D box output by a max-pool layer will have a width equal to this equation:\n",
    "\n",
    "$ w_2 = \\frac{w_1 - f}{s + 1} $\n",
    "\n",
    "The height of the 3D box produced by the max-pool layer is calculated similarly with this equation:\n",
    "\n",
    "$ h_2 = \\frac{h_1 - f}{s + 1} $\n",
    "\n",
    "The depth of the 3D box produced by the max-pool layer is equal to the depth the 3D box received as input. The most common setting for the hyper-parameters of a max-pool layer are f =2 and s=2. The spatial extent (f) specifies that boxes of 2x2 will be scaled down to single pixels. Of these four pixels, the pixel with the maximum value will represent the 2x2 pixel in the new grid. Because squares of size 4 are replaced with size 1, 75% of the pixel information is lost. The following figure shows this transformation as a 6x6 grid becomes a 3x3:\n",
    "\n",
    "![MaxPool](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_8_conv_maxpool.png \"MaxPool\")\n",
    "\n",
    "Of course, the above diagram shows each pixel as a single number. A grayscale image would have this characteristic. For an RGB image, we usually take the average of the three numbers to determine which pixel has the maximum value.\n",
    "\n",
    "[More information on CNN's](http://cs231n.github.io/convolutional-networks/)\n",
    "\n",
    "# TensorFlow with CNNs\n",
    "\n",
    "The following sections describe how to use TensorFlow/Keras with CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E0QVQe3Rmp8k"
   },
   "source": [
    "### Access to Data Sets - DIGITS\n",
    "\n",
    "Keras provides built in access classes for MNIST.  It is important to note that MNIST data arrives already separated into two sets:\n",
    "\n",
    "* **train** - Neural network will be trained with this.\n",
    "* **test** - Used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9A5mOErFmp8l",
    "outputId": "7ed1b678-2143-4095-ea15-8a614074abf9"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(\"Shape of x_train: {}\".format(x_train.shape))\n",
    "print(\"Shape of y_train: {}\".format(y_train.shape))\n",
    "print()\n",
    "print(\"Shape of x_test: {}\".format(x_test.shape))\n",
    "print(\"Shape of y_test: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4aQApI1jmp8o"
   },
   "source": [
    "### Display the Digits \n",
    "\n",
    "The following code shows what the MNIST files contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Abe2m8u_mp8p",
    "outputId": "d1f01ad5-55dc-4c05-cc96-5ef25d8e7906"
   },
   "outputs": [],
   "source": [
    "# Display as text\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Shape for dataset: {}\".format(x_train.shape))\n",
    "print(\"Labels: {}\".format(y_train))\n",
    "\n",
    "# Single MNIST digit\n",
    "single = x_train[0]\n",
    "print(\"Shape for single: {}\".format(single.shape))\n",
    "\n",
    "display(pd.DataFrame(single.reshape(28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zhbEMQY8mp8s",
    "outputId": "65f8758d-7fc0-4742-fb90-9aa06a8c8046"
   },
   "outputs": [],
   "source": [
    "# Display as image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "digit = 105 # Change to choose new digit\n",
    "a = x_train[digit]\n",
    "plt.imshow(a, cmap='gray', interpolation='nearest')\n",
    "print(\"Image (#{}): Which is digit '{}'\".format(digit,y_train[digit]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cY4T9mcCmp8u",
    "outputId": "06e6ca08-e58b-4bad-8591-feff844e38bb"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "ROWS = 6\n",
    "random_indices = random.sample(range(x_train.shape[0]), ROWS*ROWS)\n",
    "\n",
    "sample_images = x_train[random_indices, :]\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "fig, axes = plt.subplots(ROWS,ROWS, \n",
    "                         figsize=(ROWS,ROWS),\n",
    "                         sharex=True, sharey=True) \n",
    "\n",
    "for i in range(ROWS*ROWS):\n",
    "    subplot_row = i//ROWS \n",
    "    subplot_col = i%ROWS\n",
    "    ax = axes[subplot_row, subplot_col]\n",
    "\n",
    "    plottable_image = np.reshape(sample_images[i,:], (28,28))\n",
    "    ax.imshow(plottable_image, cmap='gray_r')\n",
    "    \n",
    "    ax.set_xbound([0,28])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yH2EoJnAmp8x",
    "outputId": "8d1c3c80-49b4-402b-9ef6-b22631f0b112"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(\"Training samples: {}\".format(x_train.shape[0]))\n",
    "print(\"Test samples: {}\".format(x_test.shape[0]))\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pAf59S0Wmp82"
   },
   "source": [
    "### Training/Fitting CNN - DIGITS\n",
    "\n",
    "The following code will train the CNN for 20,000 steps.  This can take awhile, you might want to scale the step count back. GPU training can help.  My results:\n",
    "\n",
    "* CPU Training Time: Elapsed time: 1:50:13.10\n",
    "* GPU Training Time: Elapsed time: 0:13:43.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5I1A6UNGmp83"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=2,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {}'.format(score[0]))\n",
    "print('Test accuracy: {}'.format(score[1]))\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6JN_aTuvmp86"
   },
   "source": [
    "### Evaluate Accuracy - DIGITS\n",
    "\n",
    "Note, if you are using a GPU you might get the **ResourceExhaustedError**.  This occurs because the GPU might not have enough ram to predict the entire data set at once.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZmOmMaJWmp86"
   },
   "outputs": [],
   "source": [
    "# Predict using either GPU or CPU, send the entire dataset.  This might not work on the GPU.\n",
    "# Set the desired TensorFlow output level for this example\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {}'.format(score[0]))\n",
    "print('Test accuracy: {}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_musbSlQmp88"
   },
   "source": [
    "GPUs are most often used for training rather than prediction.  For prediction either disable the GPU or just predict on a smaller sample.  If your GPU has enough memory, the above prediction code may work just fine.  If not, just prediction on a sample with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_sKiLx_ump88"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# For GPU just grab the first 100 images\n",
    "small_x = x_test[1:100]\n",
    "small_y = y_test[1:100]\n",
    "small_y2 = np.argmax(small_y,axis=1)\n",
    "pred = model.predict(small_x)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "score = metrics.accuracy_score(small_y2, pred)\n",
    "print('Accuracy: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BqWngCONmp8-"
   },
   "source": [
    "### MINST Fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VFjYWvJimp8_",
    "outputId": "d19c3e9b-f4f0-4da5-b63f-e439b807eafa"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "print(\"Shape of x_train: {}\".format(x_train.shape))\n",
    "print(\"Shape of y_train: {}\".format(y_train.shape))\n",
    "print()\n",
    "print(\"Shape of x_test: {}\".format(x_test.shape))\n",
    "print(\"Shape of y_test: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TRXv8k0Ymp9B"
   },
   "source": [
    "### Display the Apparel \n",
    "\n",
    "The following code shows what the Fashion MNIST files contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aQbIIbpAmp9C",
    "outputId": "626ad729-1290-48f7-c48e-e98cf3ab2824"
   },
   "outputs": [],
   "source": [
    "# Display as text\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Shape for dataset: {}\".format(x_train.shape))\n",
    "print(\"Labels: {}\".format(y_train))\n",
    "\n",
    "# Single MNIST digit\n",
    "single = x_train[0]\n",
    "print(\"Shape for single: {}\".format(single.shape))\n",
    "\n",
    "display(pd.DataFrame(single.reshape(28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xZOOenULmp9F",
    "outputId": "4503b9e7-be54-4d07-de86-e3af02e6e227"
   },
   "outputs": [],
   "source": [
    "# Display as image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "digit = 90 # Change to choose new article\n",
    "a = x_train[digit]\n",
    "plt.imshow(a, cmap='gray', interpolation='nearest')\n",
    "print(\"Image (#{}): Which is digit '{}'\".format(digit,y_train[digit]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5cN97uYTmp9H",
    "outputId": "a0b4719e-75ec-4b39-858a-31f26638d50e"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "ROWS = 6\n",
    "random_indices = random.sample(range(x_train.shape[0]), ROWS*ROWS)\n",
    "\n",
    "sample_images = x_train[random_indices, :]\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "fig, axes = plt.subplots(ROWS,ROWS, \n",
    "                         figsize=(ROWS,ROWS),\n",
    "                         sharex=True, sharey=True) \n",
    "\n",
    "for i in range(ROWS*ROWS):\n",
    "    subplot_row = i//ROWS \n",
    "    subplot_col = i%ROWS\n",
    "    ax = axes[subplot_row, subplot_col]\n",
    "\n",
    "    plottable_image = np.reshape(sample_images[i,:], (28,28))\n",
    "    ax.imshow(plottable_image, cmap='gray_r')\n",
    "    \n",
    "    ax.set_xbound([0,28])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ptDi0ZiQmp9J"
   },
   "source": [
    "### Training/Fitting CNN - Fashion\n",
    "\n",
    "The following code will train the CNN for 20,000 steps.  This can take awhile, you might want to scale the step count back. GPU training can help.  My results:\n",
    "\n",
    "* CPU Training Time: Elapsed time: 1:50:13.10\n",
    "* GPU Training Time: Elapsed time: 0:13:43.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vj921rlLmp9K"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(\"Training samples: {}\".format(x_train.shape[0]))\n",
    "print(\"Test samples: {}\".format(x_test.shape[0]))\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QBwmyM6hmp9L"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=2,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {}'.format(score[0]))\n",
    "print('Test accuracy: {}'.format(score[1]))\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q09yMGGcmp9N"
   },
   "source": [
    "# Part 6.3: Implementing a ResNet in Keras\n",
    "\n",
    "Deeper neural networks are more difficult to train. Residual learning was introduced to ease the training of networks that are substantially deeper than those used previously. ResNet explicitly reformulates the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. On the ImageNet dataset this method was evaluated with residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. This technique can also be applied to the CIFAR-10 with 100 and 1000 layers. \n",
    "\n",
    "ResNet was introduced in the following paper:\n",
    "\n",
    "* K. He, X. Zhang, S. Ren, and J. Sun. [Deep residual learning for image recognition](https://arxiv.org/abs/1512.03385). arXiv preprint arXiv:1512.03385,2015.\n",
    "\n",
    "What is a residual?\n",
    "\n",
    "* [Residual](https://www.merriam-webster.com/dictionary/residual): an internal aftereffect of experience or activity that influences later behavior\n",
    "\n",
    "To implement a ResNet we need to give Keras the notion of a residual block.  This is essentially two dense layers with a \"skip connection\" (or residual connection).  A residual block is shown here.\n",
    "\n",
    "\n",
    "![Skip Layers](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/skip-layer.png \"Skip Layers\")\n",
    "\n",
    "Residual blocks are typically used with convolutional neural networks (CNNs).  This allows very deep neural networks of CNNs to be created.  The following diagram shows several different ResNets.\n",
    "\n",
    "![ResNet](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/resnet.png \"ResNet\")\n",
    "\n",
    "The [CIFAR-10 and CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) datasets are also frequently used by the neural network research community.  These datasets were originally part of a competition. \n",
    "\n",
    "The CIFAR-10 data set contains low-res images that are divided into 10 classes.  The CIFAR-100 data set contains 100 classes in a hierarchy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gx-DMPysmp9N"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import AveragePooling2D, Input, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the CIFAR10 data.\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "372-21BTmp9P"
   },
   "source": [
    "Samples from the loaded CIFAR dataset can be displayed using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZyJdYNTDmp9P"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import cPickle \n",
    "\n",
    "ROWS = 10\n",
    "\n",
    "x = x_train.astype(\"uint8\")\n",
    "\n",
    "fig, axes1 = plt.subplots(ROWS,ROWS,figsize=(10,10))\n",
    "for j in range(ROWS):\n",
    "    for k in range(ROWS):\n",
    "        i = np.random.choice(range(len(x)))\n",
    "        axes1[j][k].set_axis_off()\n",
    "        axes1[j][k].imshow(x[i:i+1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AKQDnk2Gmp9R"
   },
   "source": [
    "We will construct a ResNet and train it on the CIFAR-10 dataset.  The following block of code defines some constant values that define how the network is constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E60qRcFzmp9R"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "BATCH_SIZE = 32  # orig paper trained all networks with batch_size=128\n",
    "EPOCHS = 200 # 200\n",
    "USE_AUGMENTATION = True\n",
    "NUM_CLASSES = np.unique(y_train).shape[0] # 10\n",
    "COLORS = x_train.shape[3]\n",
    "\n",
    "# Subtracting pixel mean improves accuracy\n",
    "SUBTRACT_PIXEL_MEAN = True\n",
    "\n",
    "# Model version\n",
    "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
    "VERSION = 1\n",
    "\n",
    "# Computed depth from supplied model parameter n\n",
    "if VERSION == 1:\n",
    "    DEPTH = COLORS * 6 + 2\n",
    "elif version == 2:\n",
    "    DEPTH = COLORS * 9 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I91EXi2hmp9T"
   },
   "source": [
    "The following function implements a learning rate decay schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pHUm2jSUmp9T"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7-du9R2rmp9V"
   },
   "source": [
    "The following code implements a ResNet block.  This includes two convolutional layers with a skip connection.  Both V1 and V2 of ResNet make use of this type of layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DPXtreZ6mp9V"
   },
   "outputs": [],
   "source": [
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FDk1ekrxmp9X"
   },
   "source": [
    "### ResNet V1\n",
    "\n",
    "* K. He, X. Zhang, S. Ren, and J. Sun. [Deep residual learning for image recognition](https://arxiv.org/abs/1512.03385). arXiv preprint arXiv:1512.03385,2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YLQe8BE-mp9X"
   },
   "outputs": [],
   "source": [
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = tensorflow.keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SzNJPXncmp9a"
   },
   "source": [
    "### ResNet V2\n",
    "\n",
    "A second version of ResNet was introduced in the following paper.  This form of ResNet is commonly referred to as ResNet V2.\n",
    "\n",
    "* He, K., Zhang, X., Ren, S., & Sun, J. (2016, October). [Identity mappings in deep residual networks](https://arxiv.org/abs/1603.05027). In European conference on computer vision (pp. 630-645). Springer, Cham.\n",
    "\n",
    "The following code constructs a ResNet V2 network.  The primary difference of the full preactivation 'v2' variant compared to the 'v1' variant is the use of [batch normalization](https://arxiv.org/abs/1502.03167) before every weight layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8WfihOPhmp9a"
   },
   "outputs": [],
   "source": [
    "def resnet_v2(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 2 Model builder [b]\n",
    "\n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
    "    bottleneck layer\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same filter map sizes.\n",
    "    Features maps sizes:\n",
    "    conv1  : 32x32,  16\n",
    "    stage 0: 32x32,  64\n",
    "    stage 1: 16x16, 128\n",
    "    stage 2:  8x8,  256\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # Start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = tensorflow.keras.layers.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-CzYSlxSmp9c"
   },
   "source": [
    "With all of this defined, we can run the ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jg7cCBZnmp9c"
   },
   "outputs": [],
   "source": [
    "# Input image dimensions.\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# If subtract pixel mean is enabled\n",
    "if SUBTRACT_PIXEL_MEAN:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('y_train shape:', y_train.shape)\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = tensorflow.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test = tensorflow.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "# Create the neural network\n",
    "if VERSION == 2:\n",
    "    model = resnet_v2(input_shape=input_shape, depth=DEPTH)\n",
    "else:\n",
    "    model = resnet_v1(input_shape=input_shape, depth=DEPTH)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=lr_schedule(0)),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CC1x1NIvmp9f"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [lr_reducer, lr_scheduler]\n",
    "\n",
    "# Run training, with or without data augmentation.\n",
    "if not USE_AUGMENTATION:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=EPOCHS,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True,\n",
    "              callbacks=callbacks)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        # set input mean to 0 over the dataset\n",
    "        featurewise_center=False,\n",
    "        # set each sample mean to 0\n",
    "        samplewise_center=False,\n",
    "        # divide inputs by std of dataset\n",
    "        featurewise_std_normalization=False,\n",
    "        # divide each input by its std\n",
    "        samplewise_std_normalization=False,\n",
    "        # apply ZCA whitening\n",
    "        zca_whitening=False,\n",
    "        # epsilon for ZCA whitening\n",
    "        zca_epsilon=1e-06,\n",
    "        # randomly rotate images in the range (deg 0 to 180)\n",
    "        rotation_range=0,\n",
    "        # randomly shift images horizontally\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically\n",
    "        height_shift_range=0.1,\n",
    "        # set range for random shear\n",
    "        shear_range=0.,\n",
    "        # set range for random zoom\n",
    "        zoom_range=0.,\n",
    "        # set range for random channel shifts\n",
    "        channel_shift_range=0.,\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        # value used for fill_mode = \"constant\"\n",
    "        cval=0.,\n",
    "        # randomly flip images\n",
    "        horizontal_flip=True,\n",
    "        # randomly flip images\n",
    "        vertical_flip=False,\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        epochs=EPOCHS, verbose=0, workers=4,\n",
    "                        callbacks=callbacks, use_multiprocessing=True)\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SpndV2K_mp9g"
   },
   "source": [
    "The trained neural network can now be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rcllz7JOmp9h"
   },
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nPa_ojR1mp9j"
   },
   "source": [
    "# Part 6.4: Using Your Own Images with Keras\n",
    "\n",
    "So far we've used image data sets that Keras provides convenience functions for accessing.  There are a number of [built-in data sets](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) for Keras.  While these convenience functions do make it easier to create Keras models for these data sets, these functions also hide the internal workings.  You might be wondering how you would train a neural network from your own sets of images.  \n",
    "\n",
    "Consider the convenience functions provided for CIFAR-10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bjsQ2JmOmp9j"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "\n",
    "# Load the CIFAR10 data.\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TnTtsJyBmp9p"
   },
   "source": [
    "The above code extracts the training and test sets from CIFAR-10.  Often these datasets are already pre-split between test and training data.  This allows comparison between many researchers who are working on models for this data.  Without these splits, it would be difficult to compare accuracy results between two different researchers that were using two different train/test splits.  Consider the shape of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cLh4SNjymp9r",
    "outputId": "b43c5b95-7427-4242-dd1b-6b5423b01769"
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7u5pTMromp9t"
   },
   "source": [
    "We are provided with 50,000 training elements.  Each training element is an image that is 32x32 pixels with 3 color channels.  Typically you will either see 1 color channel (grayscale) or 3 color channels (RGB color). \n",
    "\n",
    "If we look inside of one of the 50,000 elements we can see the structure of each image.  It is a matrix of RGB values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b8OEUZxKmp9u",
    "outputId": "61a57a9e-8f07-4d7d-d31b-a060d83a89e5"
   },
   "outputs": [],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XGn_Jd-Amp9w"
   },
   "source": [
    "It is also important to note that the data type is uint8, which is unsigned integer 8-bits (1 byte).  This corresponds well with image binary data (that is typically 24-bit, 8 bits per 3 color channel = 24 bit).  However, while the images may be 8-bit based, neural networks typically expect floating point input.  Because of this, some transformation/normalization of the data is needed.\n",
    "\n",
    "When training, it is usually necessary to handle multiple images at a time.  The code presented here will load in multiple images and convert them so that they are all the same size.  Processing training data images so that each image is of a uniform height and width is a very common step for computer vision programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vJSybIFMmp9x"
   },
   "outputs": [],
   "source": [
    "training_data = []\n",
    "\n",
    "%matplotlib inline\n",
    "from PIL import Image, ImageFile\n",
    "from matplotlib.pyplot import imshow\n",
    "import requests\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "IMAGE_WIDTH = 200\n",
    "IMAGE_HEIGHT = 200\n",
    "IMAGE_CHANNELS = 3\n",
    "\n",
    "images = [\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/9/92/Brookings.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/f/ff/WashU_Graham_Chapel.JPG\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/9/9e/SeigleHall.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/a/aa/WUSTLKnight.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/3/32/WashUABhall.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/c/c0/Brown_Hall.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/f/f4/South40.jpg\"    \n",
    "]\n",
    "\n",
    "\n",
    "def make_square(img):\n",
    "    cols,rows = img.size\n",
    "    \n",
    "    if rows>cols:\n",
    "        pad = (rows-cols)/2\n",
    "        img = img.crop((pad,0,cols,cols))\n",
    "    else:\n",
    "        pad = (cols-rows)/2\n",
    "        img = img.crop((0,pad,rows,rows))\n",
    "    \n",
    "    return img\n",
    "        \n",
    "for url in images:\n",
    "    ImageFile.LOAD_TRUNCATED_IMAGES = False\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    img.load()\n",
    "    img = img.resize((IMAGE_WIDTH,IMAGE_HEIGHT),Image.ANTIALIAS)\n",
    "    training_data.append(np.asarray(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mn9iXlz5mp91"
   },
   "source": [
    "The above code contains a function called **make_square** that ensures that each of the images have the same height and width.  Square images are particularly easy to deal with because each image will have the same aspect ratio.  There are several techniques that can be used to make an image square.  Fundamentally the image will either be cropped or padded to make it square. Padding adds extra space to the image to cause a square shape.  Cropping removes pixels (and therefore information) from the images to make them square. \n",
    "\n",
    "The technique above crops the images to make them square.  The row and column sizes are analyzed and the image is adjusted based on if the row or column size is smaller.  If there are fewer rows and than columns then the extra columns are dropped to cause the row and column count to be equal. Similarly, if there are fewer columns and than rows then the extra columns are rows to cause the row and column count to be equal. \n",
    "\n",
    "For each image in the set, the image is first adjusted to be square and then resized to the common size that we are forcing all images to be.  The resulting images are each added to a list.  The training data, at this point, is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zn6KEEAump92",
    "outputId": "48913599-505f-475a-86e2-1a186c54ee59"
   },
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A3xREkQCmp95"
   },
   "source": [
    "At this point we have a list of Numpy cubes (height by width by color depth).  We need to combine these cubes into a 4D Tensor (element by height by width by color depth).  The **reshape** function is used to do this.  It is not resizing the images, rather, it is combining the list into a 4D tensor.\n",
    "\n",
    "The training data is divided by 127.5 and subtracted by one to normalize to between -1 and 1.  This causes the RGB values to be centered around zero and gives greater predictive power to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QlMIM2c3mp95"
   },
   "outputs": [],
   "source": [
    "training_data = np.reshape(training_data,(-1,IMAGE_WIDTH,IMAGE_HEIGHT,IMAGE_CHANNELS))\n",
    "training_data = training_data / 127.5 - 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HvAOVoS9mp97"
   },
   "source": [
    "We can display the normalized training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PixWCqjhmp98",
    "outputId": "9e8ea78b-21c3-4293-d510-aec994fcbb11"
   },
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZHMjRVAbmp9_"
   },
   "source": [
    "It is sometimes useful to to save a training set.  For image and higher dimensional data, as CSV file is not sufficient. Also, Pickle can experience problems with very large datasets.  Because of this I prefer to use Numpy's own format for binary data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eemePDLwmp-A",
    "outputId": "a32c6473-c468-4516-c9dc-6be2598c2f97"
   },
   "outputs": [],
   "source": [
    "print(\"Saving training image binary...\")\n",
    "np.save(\"training\",training_data) # Saves as \"training.npy\"\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QSKZqD1Mmp-C"
   },
   "source": [
    "# Part 6.5: Recognizing Multiple Images with Darknet\n",
    "\n",
    "Convolutional neural networks are great at recognizing classifying a single item that is centered in an image.  However, as humans we are able to recognize many items in our field of view, in real-time.  It is very useful to be able to recognize multiple items in a single image.  One of the most advanced means of doing this is YOLO DarkNet (not to be confused with the Internet [Darknet](https://en.wikipedia.org/wiki/Darknet).  YOLO is an acronym for You Only Look Once.  This speaks to the efficency of the algorithm.  \n",
    "\n",
    "* Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). [You only look once: Unified, real-time object detection](https://arxiv.org/abs/1506.02640). In *Proceedings of the IEEE conference on computer vision and pattern recognition* (pp. 779-788).\n",
    "\n",
    "The following image shows YOLO tagging in action.\n",
    "\n",
    "![DarkNet](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/darknet-predictions.jpg \"DarkNet\")\n",
    "\n",
    "It is also possible to run YOLO on live video streams.  The following frame is from the YouTube Video for this module.\n",
    "\n",
    "![DarkNet Video](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/yolo_video.png \"DarkNet Video\")\n",
    "\n",
    "As you can see it is classifying many things in this video.  My collection of books behind me is adding considerable \"noise\", as DarkNet tries to classify every book behind me.  If you watch the video you will note that it is less than perfect.  The coffee mug that I pick up gets classified as a cell phone and at times a remote.  The small yellow object behind me on the desk is actually a small toolbox.  However, it gets classified as a book at times and a remote at other times.  Currently this algorithm classifies each frame on its own.  More accuracy could be gained by using multiple images together.  Consider when you see an object coming towards you, if it changes angles, you might form a better opinion of what it was.  If that same object now changes to an unfavorable angle, you still know what it is, based on previous information.\n",
    "\n",
    "### How Does DarkNet/YOLO Work?\n",
    "\n",
    "YOLO begins by resizing the image to an $S \\times S$ grid.  A single convolutional neural network is run against this grid that predicts bounding boxes and what might be contained by those boxes.  Each bounding box also has a confidence in which item it believes the box contains.  This is a regular convolution network, just like we've seen privously.  The only difference is that a YOLO CNN outputs a number of prediction bounding boxes. At a high level this can be seen by the following diagram.\n",
    "\n",
    "![The YOLO Detection System](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/yolo-1.png \"The YOLO Detection System\")\n",
    "\n",
    "The output of the YOLO convolutional neural networks is essentially a multiple regression.  The following values are generated for each of the bounding records that are generated.\n",
    "\n",
    "* **x** - The x-coordinate of the center of a bounding rectangle.\n",
    "* **y** - The y-coordinate of the center of a bounding rectangle.\n",
    "* **w** - The width of each bounding rectangle.\n",
    "* **h** - The height of each bounding rectangle.\n",
    "* **labels** - The relative probabilities of each of the labels (1 value for each label)\n",
    "* **confidence** - The confidence in this rectangle.\n",
    "\n",
    "The output layer of a Keras neural network is a Tensor.  In the case of YOLO, this output tensor is 3D and is of the following dimensions.\n",
    "\n",
    "$$S \\times S \\times (B \\cdot 5 + C)$$\n",
    "\n",
    "The constants in the above expression are:\n",
    "\n",
    "* *S* - The dimensions of the YOLO grid that is overlaid across the source image.\n",
    "* *B* - The number of potential bounding rectangles generated for each grid cell.\n",
    "* *C* - The number of class labels that here are.\n",
    "\n",
    "The value 5 in the above expression is simply the count of non-label components of each bounding rectangle ($x$, $y$, $h$, $w$, $confidence$.\n",
    "\n",
    "Because there are $S^2 \\cdot B$ total potential bounding rectangles, the image will get very full.  Because of this it is important to drop all rectangles below some threshold of confidence.  This is demonstrated by the image below.\n",
    "\n",
    "![The YOLO Detection System](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/yolo-2.png \"The YOLO Detection System\")\n",
    "\n",
    "The actual structure of the convolutional neural network behind YOLO is relatively simple and is shown in the following image.  Because there is only one convolutional neural network, and it \"only looks once,\" the performance is not impacted by how many objects are detected. \n",
    "\n",
    "![The YOLO Detection System](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/yolo-3.png \"The YOLO Detection System\")\n",
    "\n",
    "The following image shows some additional recognitions being performed by a YOLO.\n",
    "\n",
    "![The YOLO Detection System](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/yolo-4.png \"The YOLO Detection System\")\n",
    "\n",
    "\n",
    "\n",
    "### Using DarkFlow in Python\n",
    "\n",
    "To make use of DarkFlow you have several options:\n",
    "\n",
    "* **[DarkNet](https://pjreddie.com/darknet/yolo/)** - The original implementation of YOLO, written in C.\n",
    "* **[DarkFlow](https://github.com/thtrieu/darkflow)** - Python package that implements YOLO in Python, using TensorFlow.\n",
    "\n",
    "DarkFlow can be used from the command line.  This allows videos to be produced from existing videos.  This is how the YOLO videos used in the class module video were created.\n",
    "\n",
    "It is also possible call DarkFlow directly from Python.  The following code performs a classification of the image of my dog and I in the kitchen from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2vJqUpvNrEXo"
   },
   "source": [
    "### Running DarkFlow (YOLO) from Google CoLab\n",
    "\n",
    "Make sure you create the following folders on your Google drive and download yolo.weights, coco.names, and yolo.cfg into the correct locations.  See the helper script below to set this up.\n",
    "\n",
    "'/content/drive/My Drive/projects/yolo':\n",
    "bin  cfg\n",
    "\n",
    "'/content/drive/My Drive/projects/yolo/bin':\n",
    "yolo.weights\n",
    "\n",
    "'/content/drive/My Drive/projects/yolo/cfg':\n",
    "coco.names  yolo.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8559,
     "status": "ok",
     "timestamp": 1558698672288,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "mzrhtSdnmp-N",
    "outputId": "8369fe88-1f87-4915-9c17-d0d74e956565"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/thtrieu/darkflow.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26493,
     "status": "ok",
     "timestamp": 1558698701514,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "uqifB-iUnbLP",
    "outputId": "f21f1c88-5bc2-4715-ba6d-0ac7e2178fe5"
   },
   "outputs": [],
   "source": [
    "!pip install ./darkflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1260,
     "status": "ok",
     "timestamp": 1558703520631,
     "user": {
      "displayName": "Jeff Heaton",
      "photoUrl": "https://lh4.googleusercontent.com/-1VjIZeNN-zc/AAAAAAAAAAI/AAAAAAAAAC8/5x6qkhHqLr0/s64/photo.jpg",
      "userId": "18292667475657506830"
     },
     "user_tz": 300
    },
    "id": "5Q13cQ3hmp-D",
    "outputId": "2524033b-bc0f-496c-dd58-fd6219417e5d"
   },
   "outputs": [],
   "source": [
    "# Note, if you are using Google CoLab, this can be used to mount your drive to load YOLO config and weights.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42686,
     "status": "ok",
     "timestamp": 1558704047287,
     "user": {
      "displayName": "Jeff Heaton",
      "photoUrl": "https://lh4.googleusercontent.com/-1VjIZeNN-zc/AAAAAAAAAAI/AAAAAAAAAC8/5x6qkhHqLr0/s64/photo.jpg",
      "userId": "18292667475657506830"
     },
     "user_tz": 300
    },
    "id": "IffPhFlstITr",
    "outputId": "33d4bb1a-a6dd-4edd-d0b5-793120f86131"
   },
   "outputs": [],
   "source": [
    "# The following helper script will create a projects/yolo folder for you \n",
    "# and download the needed files.  \n",
    "\n",
    "!mkdir -p /content/drive/My\\ Drive/projects\n",
    "!mkdir -p /content/drive/My\\ Drive/projects/yolo\n",
    "!mkdir -p /content/drive/My\\ Drive/projects/yolo/bin\n",
    "!mkdir -p /content/drive/My\\ Drive/projects/yolo/cfg\n",
    "!wget https://raw.githubusercontent.com/thtrieu/darkflow/master/cfg/coco.names -O /content/drive/My\\ Drive/projects/yolo/cfg/coco.names\n",
    "!wget https://raw.githubusercontent.com/thtrieu/darkflow/master/cfg/yolo.cfg -O /content/drive/My\\ Drive/projects/yolo/cfg/yolo.cfg\n",
    "!wget https://pjreddie.com/media/files/yolov2.weights -O /content/drive/My\\ Drive/projects/yolo/bin/yolo.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running DarkFlow (YOLO) Locally\n",
    "\n",
    "If you wish to run YOLO from your own computer you will need to pip install cython and then follow the instructions [here](https://github.com/thtrieu/darkflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running DarkFlow (YOLO)\n",
    "\n",
    "Regardless of which path you take above (Google CoLab or Local) you will run this code to continue.  Make sure to uncomment the correct **os.chdir** command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 860
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11317,
     "status": "ok",
     "timestamp": 1558704081263,
     "user": {
      "displayName": "Jeff Heaton",
      "photoUrl": "https://lh4.googleusercontent.com/-1VjIZeNN-zc/AAAAAAAAAAI/AAAAAAAAAC8/5x6qkhHqLr0/s64/photo.jpg",
      "userId": "18292667475657506830"
     },
     "user_tz": 300
    },
    "id": "i_y-EqBRmp-H",
    "outputId": "211a6abf-f616-4eb8-8f02-d4817bfffb3c"
   },
   "outputs": [],
   "source": [
    "from darkflow.net.build import TFNet\n",
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "from scipy import misc\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "\n",
    "os.chdir('/content/drive/My Drive/projects/yolo') # Google CoLab\n",
    "#os.chdir('/Users/jheaton/projects/darkflow') # Local\n",
    "\n",
    "# For GPU (Google CoLab)\n",
    "options = {\"model\": \"./cfg/yolo.cfg\", \"load\": \"./bin/yolo.weights\", \"threshold\": 0.1, \"gpu\": 1.0}\n",
    "\n",
    "# For CPU \n",
    "#options = {\"model\": \"./cfg/yolo.cfg\", \"load\": \"./bin/yolo.weights\", \"threshold\": 0.1, \"gpu\": 1.0}\n",
    "\n",
    "tfnet = TFNet(options)\n",
    "\n",
    "\n",
    "# Read image to classify\n",
    "url = \"https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/cook.jpg\"\n",
    "resp = urlopen(url)\n",
    "img = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
    "\n",
    "result = tfnet.return_predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 379,
     "status": "ok",
     "timestamp": 1558698194934,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "MY3gVyidmp-K",
    "outputId": "9cdb1c18-d997-470a-d664-2284697ee9b2"
   },
   "outputs": [],
   "source": [
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0eBtaFbimp-M"
   },
   "source": [
    "# Module 6 Assignment\n",
    "\n",
    "You can find the first assignment here: [assignment 6](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class1.ipynb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4aQApI1jmp8o",
    "pAf59S0Wmp82",
    "6JN_aTuvmp86",
    "BqWngCONmp8-",
    "TRXv8k0Ymp9B",
    "ptDi0ZiQmp9J",
    "FDk1ekrxmp9X",
    "SzNJPXncmp9a"
   ],
   "name": "Copy of t81_558_class06_backpropagation.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class06_backpropagation.ipynb",
     "timestamp": 1558698961697
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3.6 (yolo)",
   "language": "python",
   "name": "yolo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
