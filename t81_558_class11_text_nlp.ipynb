{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Class 11: Natural Language Processing and Speech Recognition**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sources: DBPedia\n",
    "\n",
    "[DBPedia](http://wiki.dbpedia.org/) uses the data contained in [WikiPedia]() in database form.  The data in DBPedia can be queried in an SQL-like syntax named Protocol and RDF Query Language, or [SPARQL](https://en.wikipedia.org/wiki/SPARQL). \n",
    "\n",
    "For the text examples in this class we will use a sample of the DBPedia articles classified into 14 high level document classifications:\n",
    "\n",
    "* Company (1)\n",
    "* EducationalInstitution (2)\n",
    "* Artist (3)\n",
    "* Athlete (4)\n",
    "* OfficeHolder (5)\n",
    "* MeanOfTransportation (6)\n",
    "* Building (7)\n",
    "* NaturalPlace (8)\n",
    "* Village (9)\n",
    "* Animal (10)\n",
    "* Plant (11)\n",
    "* Album (12)\n",
    "* Film (13)\n",
    "* WrittenWork (14)\n",
    "\n",
    "The data files can be found at this [location](https://drive.google.com/drive/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow makes available several operators designed for text classification.\n",
    "\n",
    "* skflow.preprocessing.**ByteProcessor (doc_len)** - Turn a list of text strings into fixed length arrays (specified by doc_len) using integer ASCII values, for example \"ABC\" becomes [65, 66, 67, 0, 0] if the doc_len is 5.\n",
    "* skflow.ops.**one_hot_matrix** - One hot is the same as dummy variables. Expands multiple inputs into a cube, with dimensions [num_samples, input_size, num_samples].\n",
    "* skflow.ops.**split_squeeze** - Splits input on given dimension and then squeezes that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 84, 104, 105, 115,  32], dtype=uint8), array([65, 66, 67,  0,  0], dtype=uint8), array([97, 98, 99,  0,  0], dtype=uint8)]\n"
     ]
    }
   ],
   "source": [
    "# Classifying Text Documents\n",
    "\n",
    "data = [\n",
    "    \"This is a test\",\n",
    "    \"ABC\",\n",
    "    \"abc\"\n",
    "]\n",
    "\n",
    "char_processor = skflow.preprocessing.ByteProcessor(5)\n",
    "\n",
    "z = list(char_processor.fit_transform(data))\n",
    "\n",
    "print(z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/bs/_w74fpx157v3vs82q0fwnflr0000gn/T/tmph03vov2s\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'save_checkpoints_steps': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11715fa58>, '_task_id': 0, 'save_summary_steps': 100, '_task_type': None, 'tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", 'save_checkpoints_secs': 600, '_environment': 'local', '_master': '', '_is_chief': True, '_evaluation_master': '', 'keep_checkpoint_max': 5, 'keep_checkpoint_every_n_hours': 10000, '_num_ps_replicas': 0, 'tf_random_seed': None}\n",
      "WARNING:tensorflow:From <ipython-input-3-4b349a8a94f9>:58 in <module>.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-3-4b349a8a94f9>:58 in <module>.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.contrib.rnn' has no attribute 'GRUCell'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4b349a8a94f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Train and predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m y_predicted = [\n\u001b[1;32m     60\u001b[0m   p['class'] for p in classifier.predict(\n",
      "\u001b[0;32m/Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0m_call_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorator_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_qualified_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             func.__module__, arg_name, date, instructions)\n\u001b[0;32m--> 191\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(\n\u001b[1;32m    193\u001b[0m         func.__doc__, date, instructions)\n",
      "\u001b[0;32m/Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\u001b[0m\n\u001b[1;32m    353\u001b[0m                              \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                              \u001b[0mmonitors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmonitors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                              max_steps=max_steps)\n\u001b[0m\u001b[1;32m    356\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, steps, feed_fn, init_op, init_feed_fn, init_fn, device_fn, monitors, log_every_steps, fail_on_nan_loss, max_steps)\u001b[0m\n\u001b[1;32m    697\u001b[0m       \u001b[0;31m# cases, but will soon be deleted after the subclasses are updated.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m       \u001b[0;31m# TODO(b/32664904): Update subclasses and delete the else-statement.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m       \u001b[0mtrain_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelFnOps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Default signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_get_train_ops\u001b[0;34m(self, features, labels)\u001b[0m\n\u001b[1;32m   1050\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0mModelFnOps\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m     \"\"\"\n\u001b[0;32m-> 1052\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_eval_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m       \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelFnOps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4b349a8a94f9>\u001b[0m in \u001b[0;36mchar_rnn_model\u001b[0;34m(features, target)\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mbyte_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRUCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHIDDEN_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbyte_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.contrib.rnn' has no attribute 'GRUCell'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "\n",
    "learn = tf.contrib.learn\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "MAX_DOCUMENT_LENGTH = 100\n",
    "HIDDEN_SIZE = 20\n",
    "\n",
    "\n",
    "def char_rnn_model(features, target):\n",
    "  \"\"\"Character level recurrent neural network model to predict classes.\"\"\"\n",
    "  target = tf.one_hot(target, 15, 1, 0)\n",
    "  byte_list = tf.one_hot(features, 256, 1, 0)\n",
    "  byte_list = tf.unstack(byte_list, axis=1)\n",
    "\n",
    "  cell = tf.contrib.rnn.GRUCell(HIDDEN_SIZE)\n",
    "  _, encoding = tf.contrib.rnn.static_rnn(cell, byte_list, dtype=tf.float32)\n",
    "\n",
    "  logits = tf.contrib.layers.fully_connected(encoding, 15, activation_fn=None)\n",
    "  loss = tf.contrib.losses.softmax_cross_entropy(logits, target)\n",
    "\n",
    "  train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss,\n",
    "      tf.contrib.framework.get_global_step(),\n",
    "      optimizer='Adam',\n",
    "      learning_rate=0.01)\n",
    "\n",
    "  return ({\n",
    "      'class': tf.argmax(logits, 1),\n",
    "      'prob': tf.nn.softmax(logits)\n",
    "  }, loss, train_op)\n",
    "\n",
    "\n",
    "# Prepare training and testing data\n",
    "dbpedia = learn.datasets.load_dataset(\n",
    "  'dbpedia', test_with_fake_data=True)\n",
    "x_train = pandas.DataFrame(dbpedia.train.data)[1]\n",
    "y_train = pandas.Series(dbpedia.train.target)\n",
    "x_test = pandas.DataFrame(dbpedia.test.data)[1]\n",
    "y_test = pandas.Series(dbpedia.test.target)\n",
    "\n",
    "# Process vocabulary\n",
    "char_processor = learn.preprocessing.ByteProcessor(MAX_DOCUMENT_LENGTH)\n",
    "x_train = np.array(list(char_processor.fit_transform(x_train)))\n",
    "x_test = np.array(list(char_processor.transform(x_test)))\n",
    "\n",
    "# Build model\n",
    "classifier = learn.Estimator(model_fn=char_rnn_model)\n",
    "\n",
    "# Train and predict\n",
    "classifier.fit(x_train, y_train, steps=100)\n",
    "y_predicted = [\n",
    "  p['class'] for p in classifier.predict(\n",
    "      x_test, as_iterable=True)\n",
    "]\n",
    "score = metrics.accuracy_score(y_test, y_predicted)\n",
    "print('Accuracy: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow.models.rnn.ops'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5c6b7dfee443>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mskflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tensorflow.models.rnn.ops'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn import metrics\n",
    "import pandas\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn import rnn, rnn_cell\n",
    "from tensorflow.contrib import skflow\n",
    "\n",
    "### Training data\n",
    "\n",
    "# Download dbpedia_csv.tar.gz from\n",
    "# https://drive.google.com/folderview?id=0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M\n",
    "# Unpack: tar -xvf dbpedia_csv.tar.gz\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "train = pandas.read_csv(os.path.join(path,\"train.csv\"), header=None)\n",
    "X_train, y_train = train[2], train[0]\n",
    "test = pandas.read_csv(os.path.join(path,\"test.csv\"), header=None)\n",
    "X_test, y_test = test[2], test[0]\n",
    "\n",
    "### Process vocabulary\n",
    "\n",
    "MAX_DOCUMENT_LENGTH = 100\n",
    "\n",
    "char_processor = skflow.preprocessing.ByteProcessor(MAX_DOCUMENT_LENGTH)\n",
    "X_train = np.array(list(char_processor.fit_transform(X_train)))\n",
    "X_test = np.array(list(char_processor.transform(X_test)))\n",
    "\n",
    "### Models\n",
    "\n",
    "HIDDEN_SIZE = 20\n",
    "\n",
    "def char_rnn_model(X, y):\n",
    "    byte_list = skflow.ops.one_hot_matrix(X, 256)\n",
    "    byte_list = skflow.ops.split_squeeze(1, MAX_DOCUMENT_LENGTH, byte_list)\n",
    "    cell = rnn_cell.GRUCell(HIDDEN_SIZE)\n",
    "    _, encoding = rnn.rnn(cell, byte_list, dtype=tf.float32)\n",
    "    return skflow.models.logistic_regression(encoding, y)\n",
    "\n",
    "classifier = skflow.TensorFlowEstimator(model_fn=char_rnn_model, n_classes=15,\n",
    "    steps=1000, optimizer='Adam', learning_rate=0.01, continue_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #99, avg. train loss: 2.68226\n",
      "Step #199, avg. train loss: 2.42822\n",
      "Step #299, avg. train loss: 1.94456\n",
      "Step #399, avg. train loss: 1.62389\n",
      "Step #499, avg. train loss: 1.38159\n",
      "Step #599, avg. train loss: 1.22085\n",
      "Step #699, avg. train loss: 1.15357\n",
      "Step #799, avg. train loss: 0.99735\n",
      "Step #899, avg. train loss: 0.96319\n",
      "Step #999, avg. train loss: 0.89216\n",
      "Accuracy: 0.687457\n",
      "Step #99, avg. train loss: 0.90357\n",
      "Step #199, avg. train loss: 0.84236\n",
      "Step #299, avg. train loss: 0.82514\n",
      "Step #399, avg. train loss: 0.76895\n",
      "Step #499, avg. train loss: 0.77610\n",
      "Step #599, avg. train loss: 0.74104\n",
      "Step #699, avg. train loss: 0.76284\n",
      "Step #799, avg. train loss: 0.69159\n",
      "Step #899, avg. train loss: 0.68181\n",
      "Step #999, avg. train loss: 0.66279\n",
      "Accuracy: 0.757243\n",
      "Step #99, avg. train loss: 0.68387\n",
      "Step #199, avg. train loss: 0.63289\n",
      "Step #299, avg. train loss: 0.63192\n",
      "Step #399, avg. train loss: 0.60658\n",
      "Step #499, avg. train loss: 0.61538\n",
      "Step #599, avg. train loss: 0.61461\n",
      "Step #699, avg. train loss: 0.64005\n",
      "Step #799, avg. train loss: 0.60081\n",
      "Step #899, avg. train loss: 0.56577\n",
      "Step #999, avg. train loss: 0.56744\n",
      "Accuracy: 0.793429\n",
      "Step #99, avg. train loss: 0.58773\n",
      "Step #199, avg. train loss: 0.55587\n",
      "Step #299, avg. train loss: 0.55278\n",
      "Step #399, avg. train loss: 0.53777\n",
      "Step #499, avg. train loss: 0.52889\n",
      "Step #599, avg. train loss: 0.54934\n",
      "Step #699, avg. train loss: 0.55660\n",
      "Step #799, avg. train loss: 0.53622\n",
      "Step #899, avg. train loss: 0.50377\n",
      "Step #999, avg. train loss: 0.51069\n",
      "Accuracy: 0.803243\n",
      "Step #99, avg. train loss: 0.54197\n",
      "Step #199, avg. train loss: 0.51163\n",
      "Step #299, avg. train loss: 0.50413\n",
      "Step #399, avg. train loss: 0.49793\n",
      "Step #499, avg. train loss: 0.46783\n",
      "Step #599, avg. train loss: 0.50312\n",
      "Step #699, avg. train loss: 0.50805\n",
      "Step #799, avg. train loss: 0.49145\n",
      "Step #899, avg. train loss: 0.46800\n",
      "Step #999, avg. train loss: 0.47253\n",
      "Accuracy: 0.807371\n"
     ]
    }
   ],
   "source": [
    "# Continuesly train for 1000 steps & predict on test set.\n",
    "for i in range(5):\n",
    "    classifier.fit(X_train, y_train)\n",
    "    score = metrics.accuracy_score(y_test, classifier.predict(X_test))\n",
    "    print(\"Accuracy: %f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(560000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:Tensor(\"OneHot:0\", shape=(560000, 100, 256), dtype=float32)\n",
      "100\n",
      "2:Tensor(\"Squeeze:0\", shape=(560000, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp = skflow.ops.one_hot_matrix(X_train, 256) \n",
    "print(\"1:{}\".format(temp))\n",
    "temp = skflow.ops.split_squeeze(1, MAX_DOCUMENT_LENGTH, temp)\n",
    "print(len(temp))\n",
    "print(\"2:{}\".format(temp[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# **Code below this point will not run in Data Scientist Workbench **\n",
    "\n",
    "The code below interfaces with your computer's microphone and speakers.  It will not run in Data Scientist Workbench.\n",
    "\n",
    "\n",
    "# Speech Recognition\n",
    "\n",
    "A very common use of LSTM and RNN's is [speech recognition](https://en.wikipedia.org/wiki/Speech_recognition).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Google Voice for Speech Recognition\n",
    "\n",
    "Google speech recognition makes use of [LSTM and some other technologies](https://research.googleblog.com/2015/08/the-neural-networks-behind-google-voice.html).\n",
    "\n",
    "See Google [Speech Recognition in action](https://www.google.com/intl/en/chrome/demos/speech.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something!\n",
      "You said: hello\n"
     ]
    }
   ],
   "source": [
    "# pip install SpeechRecognition\n",
    "# see this for PyAudio\n",
    "# pip install pyttsx\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "# NOTE: this example requires PyAudio because it uses the Microphone class\n",
    "\n",
    "import speech_recognition as sr\n",
    "\n",
    "# obtain audio from the microphone\n",
    "r = sr.Recognizer()\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Say something!\")\n",
    "    audio = r.listen(source)\n",
    "\n",
    "# recognize speech using Google Speech Recognition\n",
    "try:\n",
    "    # for testing purposes, we're just using the default API key\n",
    "    # to use another API key, use `r.recognize_google(audio, key=\"GOOGLE_SPEECH_RECOGNITION_API_KEY\")`\n",
    "    # instead of `r.recognize_google(audio)`\n",
    "    str = r.recognize_google(audio)\n",
    "    print(\"You said: {}\".format(str))\n",
    "    os.system(\"say 'I believe you said: {}'\".format(str))\n",
    "except sr.UnknownValueError:\n",
    "    print(\"Google Speech Recognition could not understand audio\")\n",
    "except sr.RequestError as e:\n",
    "    print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Text to Speech\n",
    "\n",
    "Challenges:\n",
    "\n",
    "* [Background Conversation](https://www.youtube.com/watch?v=IKB3Qiglyro&t=119s)\n",
    "* [Klingon](https://www.youtube.com/watch?v=ucO3heC-Ztw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following code works on a Mac\n",
    "import os\n",
    "\n",
    "def say(s):\n",
    "    s = s.replace(\"'\",\"\")\n",
    "    os.system(\"say '{}'\".format(s))\n",
    "    \n",
    "say(\"Shall we play a game?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to Speech and Speech Recognition\n",
    "\n",
    "\n",
    "Text to speech and speech recognition often go hand in hand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You said: hola que tal como se Yama\n"
     ]
    }
   ],
   "source": [
    "# pip install SpeechRecognition\n",
    "# see this for PyAudio\n",
    "# pip install pyttsx\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "# NOTE: this example requires PyAudio because it uses the Microphone class\n",
    "\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "\n",
    "def say(s):\n",
    "    s = s.replace(\"'\",\"\")\n",
    "    os.system(\"say '{}'\".format(s))\n",
    "\n",
    "# obtain audio from the microphone\n",
    "r = sr.Recognizer()\n",
    "with sr.Microphone() as source:\n",
    "    say(\"Hello there, please say something.\")\n",
    "    audio = r.listen(source)\n",
    "\n",
    "# recognize speech using Google Speech Recognition\n",
    "try:\n",
    "    # for testing purposes, we're just using the default API key\n",
    "    # to use another API key, use `r.recognize_google(audio, key=\"GOOGLE_SPEECH_RECOGNITION_API_KEY\")`\n",
    "    # instead of `r.recognize_google(audio)`\n",
    "    str = r.recognize_google(audio)\n",
    "    print(\"You said: {}\".format(str))\n",
    "    say(\"I think you said {}\".format(str))\n",
    "except sr.UnknownValueError:\n",
    "    print(\"Google Speech Recognition could not understand audio\")\n",
    "except sr.RequestError as e:\n",
    "    print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliza Example\n",
    "\n",
    "[ELIZA](https://en.wikipedia.org/wiki/ELIZA) is an early natural language processing computer program created from 1964 to 1966 at the MIT Artificial Intelligence Laboratory by Joseph Weizenbaum.  The following code is based in an [Eliza Python Implementation by SureSmallThing](https://www.smallsurething.com/implementing-the-famous-eliza-chatbot-in-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: my mother hates me\n",
      "Eliza (computer): When your mother hates you, how do you feel?\n",
      "Human: I feel sad\n",
      "Eliza (computer): you feel sad.\n",
      "Human: yes I do do you feel sad\n",
      "Eliza (computer): OK, but can you elaborate a bit?\n",
      "No input, or could not understand audio.\n",
      "Human: quick\n",
      "Eliza (computer): Why do you say that quick?\n",
      "Human: quick\n",
      "Eliza (computer): quick.\n",
      "Human: quit\n",
      "Eliza (computer): Thank you for talking with me.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "\n",
    "reflections = {\n",
    "    \"am\": \"are\",\n",
    "    \"was\": \"were\",\n",
    "    \"i\": \"you\",\n",
    "    \"i'd\": \"you would\",\n",
    "    \"i've\": \"you have\",\n",
    "    \"i'll\": \"you will\",\n",
    "    \"my\": \"your\",\n",
    "    \"are\": \"am\",\n",
    "    \"you've\": \"I have\",\n",
    "    \"you'll\": \"I will\",\n",
    "    \"your\": \"my\",\n",
    "    \"yours\": \"mine\",\n",
    "    \"you\": \"me\",\n",
    "    \"me\": \"you\"\n",
    "}\n",
    "\n",
    "psychobabble = [\n",
    "    [r'i need (.*)',\n",
    "     [\"Why do you need {0}?\",\n",
    "      \"Would it really help you to get {0}?\",\n",
    "      \"Are you sure you need {0}?\"]],\n",
    "\n",
    "    [r'why don\\'?t you ([^\\?]*)\\??',\n",
    "     [\"Do you really think I don't {0}?\",\n",
    "      \"Perhaps eventually I will {0}.\",\n",
    "      \"Do you really want me to {0}?\"]],\n",
    "\n",
    "    [r'why can\\'?t I ([^\\?]*)\\??',\n",
    "     [\"Do you think you should be able to {0}?\",\n",
    "      \"If you could {0}, what would you do?\",\n",
    "      \"I don't know -- why can't you {0}?\",\n",
    "      \"Have you really tried?\"]],\n",
    "\n",
    "    [r'i can\\'?t (.*)',\n",
    "     [\"How do you know you can't {0}?\",\n",
    "      \"Perhaps you could {0} if you tried.\",\n",
    "      \"What would it take for you to {0}?\"]],\n",
    "\n",
    "    [r'i am (.*)',\n",
    "     [\"Did you come to me because you are {0}?\",\n",
    "      \"How long have you been {0}?\",\n",
    "      \"How do you feel about being {0}?\"]],\n",
    "\n",
    "    [r'i\\'?m (.*)',\n",
    "     [\"How does being {0} make you feel?\",\n",
    "      \"Do you enjoy being {0}?\",\n",
    "      \"Why do you tell me you're {0}?\",\n",
    "      \"Why do you think you're {0}?\"]],\n",
    "\n",
    "    [r'are you ([^\\?]*)\\??',\n",
    "     [\"Why does it matter whether I am {0}?\",\n",
    "      \"Would you prefer it if I were not {0}?\",\n",
    "      \"Perhaps you believe I am {0}.\",\n",
    "      \"I may be {0} -- what do you think?\"]],\n",
    "\n",
    "    [r'what (.*)',\n",
    "     [\"Why do you ask?\",\n",
    "      \"How would an answer to that help you?\",\n",
    "      \"What do you think?\"]],\n",
    "\n",
    "    [r'how (.*)',\n",
    "     [\"How do you suppose?\",\n",
    "      \"Perhaps you can answer your own question.\",\n",
    "      \"What is it you're really asking?\"]],\n",
    "\n",
    "    [r'because (.*)',\n",
    "     [\"Is that the real reason?\",\n",
    "      \"What other reasons come to mind?\",\n",
    "      \"Does that reason apply to anything else?\",\n",
    "      \"If {0}, what else must be true?\"]],\n",
    "\n",
    "    [r'(.*) sorry (.*)',\n",
    "     [\"There are many times when no apology is needed.\",\n",
    "      \"What feelings do you have when you apologize?\"]],\n",
    "\n",
    "    [r'hello(.*)',\n",
    "     [\"Hello... I'm glad you could drop by today.\",\n",
    "      \"Hi there... how are you today?\",\n",
    "      \"Hello, how are you feeling today?\"]],\n",
    "\n",
    "    [r'i think (.*)',\n",
    "     [\"Do you doubt {0}?\",\n",
    "      \"Do you really think so?\",\n",
    "      \"But you're not sure {0}?\"]],\n",
    "\n",
    "    [r'(.*) friend (.*)',\n",
    "     [\"Tell me more about your friends.\",\n",
    "      \"When you think of a friend, what comes to mind?\",\n",
    "      \"Why don't you tell me about a childhood friend?\"]],\n",
    "\n",
    "    [r'yes',\n",
    "     [\"You seem quite sure.\",\n",
    "      \"OK, but can you elaborate a bit?\"]],\n",
    "\n",
    "    [r'(.*) computer(.*)',\n",
    "     [\"Are you really talking about me?\",\n",
    "      \"Does it seem strange to talk to a computer?\",\n",
    "      \"How do computers make you feel?\",\n",
    "      \"Do you feel threatened by computers?\"]],\n",
    "\n",
    "    [r'is it (.*)',\n",
    "     [\"Do you think it is {0}?\",\n",
    "      \"Perhaps it's {0} -- what do you think?\",\n",
    "      \"If it were {0}, what would you do?\",\n",
    "      \"It could well be that {0}.\"]],\n",
    "\n",
    "    [r'it is (.*)',\n",
    "     [\"You seem very certain.\",\n",
    "      \"If I told you that it probably isn't {0}, what would you feel?\"]],\n",
    "\n",
    "    [r'can you ([^\\?]*)\\??',\n",
    "     [\"What makes you think I can't {0}?\",\n",
    "      \"If I could {0}, then what?\",\n",
    "      \"Why do you ask if I can {0}?\"]],\n",
    "\n",
    "    [r'can I ([^\\?]*)\\??',\n",
    "     [\"Perhaps you don't want to {0}.\",\n",
    "      \"Do you want to be able to {0}?\",\n",
    "      \"If you could {0}, would you?\"]],\n",
    "\n",
    "    [r'you are (.*)',\n",
    "     [\"Why do you think I am {0}?\",\n",
    "      \"Does it please you to think that I'm {0}?\",\n",
    "      \"Perhaps you would like me to be {0}.\",\n",
    "      \"Perhaps you're really talking about yourself?\"]],\n",
    "\n",
    "    [r'you\\'?re (.*)',\n",
    "     [\"Why do you say I am {0}?\",\n",
    "      \"Why do you think I am {0}?\",\n",
    "      \"Are we talking about you, or me?\"]],\n",
    "\n",
    "    [r'i don\\'?t (.*)',\n",
    "     [\"Don't you really {0}?\",\n",
    "      \"Why don't you {0}?\",\n",
    "      \"Do you want to {0}?\"]],\n",
    "\n",
    "    [r'i feel (.*)',\n",
    "     [\"Good, tell me more about these feelings.\",\n",
    "      \"Do you often feel {0}?\",\n",
    "      \"When do you usually feel {0}?\",\n",
    "      \"When you feel {0}, what do you do?\"]],\n",
    "\n",
    "    [r'i have (.*)',\n",
    "     [\"Why do you tell me that you've {0}?\",\n",
    "      \"Have you really {0}?\",\n",
    "      \"Now that you have {0}, what will you do next?\"]],\n",
    "\n",
    "    [r'i would (.*)',\n",
    "     [\"Could you explain why you would {0}?\",\n",
    "      \"Why would you {0}?\",\n",
    "      \"Who else knows that you would {0}?\"]],\n",
    "\n",
    "    [r'is there (.*)',\n",
    "     [\"Do you think there is {0}?\",\n",
    "      \"It's likely that there is {0}.\",\n",
    "      \"Would you like there to be {0}?\"]],\n",
    "\n",
    "    [r'my (.*)',\n",
    "     [\"I see, your {0}.\",\n",
    "      \"Why do you say that your {0}?\",\n",
    "      \"When your {0}, how do you feel?\"]],\n",
    "\n",
    "    [r'you (.*)',\n",
    "     [\"We should be discussing you, not me.\",\n",
    "      \"Why do you say that about me?\",\n",
    "      \"Why do you care whether I {0}?\"]],\n",
    "\n",
    "    [r'why (.*)',\n",
    "     [\"Why don't you tell me the reason why {0}?\",\n",
    "      \"Why do you think {0}?\"]],\n",
    "\n",
    "    [r'i want (.*)',\n",
    "     [\"What would it mean to you if you got {0}?\",\n",
    "      \"Why do you want {0}?\",\n",
    "      \"What would you do if you got {0}?\",\n",
    "      \"If you got {0}, then what would you do?\"]],\n",
    "\n",
    "    [r'(.*) mother(.*)',\n",
    "     [\"Tell me more about your mother.\",\n",
    "      \"What was your relationship with your mother like?\",\n",
    "      \"How do you feel about your mother?\",\n",
    "      \"How does this relate to your feelings today?\",\n",
    "      \"Good family relations are important.\"]],\n",
    "\n",
    "    [r'(.*) father(.*)',\n",
    "     [\"Tell me more about your father.\",\n",
    "      \"How did your father make you feel?\",\n",
    "      \"How do you feel about your father?\",\n",
    "      \"Does your relationship with your father relate to your feelings today?\",\n",
    "      \"Do you have trouble showing affection with your family?\"]],\n",
    "\n",
    "    [r'(.*) child(.*)',\n",
    "     [\"Did you have close friends as a child?\",\n",
    "      \"What is your favorite childhood memory?\",\n",
    "      \"Do you remember any dreams or nightmares from childhood?\",\n",
    "      \"Did the other children sometimes tease you?\",\n",
    "      \"How do you think your childhood experiences relate to your feelings today?\"]],\n",
    "\n",
    "    [r'(.*)\\?',\n",
    "     [\"Why do you ask that?\",\n",
    "      \"Please consider whether you can answer your own question.\",\n",
    "      \"Perhaps the answer lies within yourself?\",\n",
    "      \"Why don't you tell me?\"]],\n",
    "\n",
    "    [r'quit',\n",
    "     [\"Thank you for talking with me.\",\n",
    "      \"Good-bye.\",\n",
    "      \"Thank you, that will be $150.  Have a good day!\"]],\n",
    "\n",
    "    [r'(.*)',\n",
    "     [\"Please tell me more.\",\n",
    "      \"Let's change focus a bit... Tell me about your family.\",\n",
    "      \"Can you elaborate on that?\",\n",
    "      \"Why do you say that {0}?\",\n",
    "      \"I see.\",\n",
    "      \"Very interesting.\",\n",
    "      \"{0}.\",\n",
    "      \"I see.  And what does that tell you?\",\n",
    "      \"How does that make you feel?\",\n",
    "      \"How do you feel when you say that?\"]]\n",
    "]\n",
    "\n",
    "\n",
    "def reflect(fragment):\n",
    "    tokens = fragment.lower().split()\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in reflections:\n",
    "            tokens[i] = reflections[token]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def analyze(statement):\n",
    "    for pattern, responses in psychobabble:\n",
    "        match = re.match(pattern, statement.rstrip(\".!\"))\n",
    "        if match:\n",
    "            response = random.choice(responses)\n",
    "            return response.format(*[reflect(g) for g in match.groups()])\n",
    "\n",
    "def say(s):\n",
    "    s = s.replace(\"'\",\"\")\n",
    "    os.system(\"say '{}'\".format(s))\n",
    "\n",
    "def main():\n",
    "    say(\"Hello. How are you feeling today?\")\n",
    "\n",
    "    r = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            audio = r.listen(source)\n",
    "\n",
    "            # recognize speech using Google Speech Recognition\n",
    "            try:\n",
    "                # for testing purposes, we're just using the default API key\n",
    "                # to use another API key, use `r.recognize_google(audio, key=\"GOOGLE_SPEECH_RECOGNITION_API_KEY\")`\n",
    "                # instead of `r.recognize_google(audio)`\n",
    "                statement = r.recognize_google(audio)\n",
    "                print(\"Human: {}\".format(statement))\n",
    "                response = analyze(statement)\n",
    "\n",
    "                if statement.lower() == 'quit':\n",
    "                    done = True\n",
    "\n",
    "                print(\"Eliza (computer): {}\".format(response))\n",
    "                say(response)\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"No input, or could not understand audio.\")\n",
    "            except sr.RequestError as e:\n",
    "                print(\"Error: Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Bots\n",
    "\n",
    "Using the above code you can create your own primitive chat bots.  A some what famous video on Youtube from Cornell University shows what happens [when two chat bots converse](https://www.youtube.com/watch?v=WnzlbyTZsQY).  Other interesting chat bot type technology:\n",
    "\n",
    "* [CleverBot](http://www.cleverbot.com/)\n",
    "* [Computer Science Paper Generator](https://pdos.csail.mit.edu/archive/scigen/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on LSTM\n",
    "\n",
    "* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "* [LSTM Music](https://www.youtube.com/watch?v=0VTI1BBLydE)\n",
    "* [Natural Language Processing from Scratch](https://arxiv.org/abs/1103.0398)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
