{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 7: Generative Adversarial Networks**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Video Material\n",
    "\n",
    "Main video lecture:\n",
    "\n",
    "* [Part 7.1: Introduction to GANS for Image and Data Generation](https://www.youtube.com/watch?v=u8xn393mDPM&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN&index=21)\n",
    "* [Part 7.2: Implementing a GAN in Keras](https://www.youtube.com/watch?v=cf6FDLFNWEk&index=22&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN)\n",
    "* [Part 7.3: Face Generation with StyleGAN and Python](https://www.youtube.com/watch?v=LSSH_NdXwhU&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN&index=23)\n",
    "* [Part 7.4: GANS for Semi-Supervised Learning in Keras](https://www.youtube.com/watch?v=LSSH_NdXwhU&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN&index=23)\n",
    "* [Part 7.5: An Overview of GAN Research](https://www.youtube.com/watch?v=LSSH_NdXwhU&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN&index=23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7.1: Introduction to GANS for Image and Data Generation\n",
    "\n",
    "A generative adversarial network (GAN) is a class of machine learning systems invented by Ian Goodfellow in 2014. Two neural networks contest with each other in a game. Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised learning, GANs have also proven useful for semi-supervised learning, fully supervised learning, and reinforcement learning.  GANs were introduced in the following paper: \n",
    "\n",
    "* Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2014). [Generative adversarial nets](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf). In *Advances in neural information processing systems* (pp. 2672-2680).\n",
    "\n",
    "This paper used neural networks to automatically generate images for several datasets that we've seen previously: MINST and CIFAR.  However, it also included the Toronto Face Dataset (a private dataset used by some researchers).\n",
    "\n",
    "![GAN](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan-2.png \"GAN\")\n",
    "\n",
    "Only sub-figure D made use of convolutional neural networks.  Figures A-C make use of fully connected neural networks.  As we will see in this module, the role of convolutional neural networks with GANs was greatly increased.\n",
    "\n",
    "A GAN is called a generative model because it generates new data.  The overall process of a GAN is given by the following diagram.\n",
    "\n",
    "![GAN](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan-1.png \"GAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7.2: Implementing DCGANs in Keras\n",
    "\n",
    "Paper that described the type of DCGAN that we will create in this module.\n",
    "\n",
    "* Radford, A., Metz, L., & Chintala, S. (2015). [Unsupervised representation learning with deep convolutional generative adversarial networks](https://arxiv.org/abs/1511.06434). *arXiv preprint arXiv:1511.06434*.\n",
    "\n",
    "This paper implements a DCGAN as follows:\n",
    "\n",
    "* No pre-processing was applied to training images besides scaling to the range of the tanh activation function [-1, 1]. \n",
    "* All models were trained with mini-batch stochastic gradient descent (SGD) with a mini-batch size of 128. \n",
    "* All weights were initialized from a zero-centered Normal distribution with standard deviation 0.02. \n",
    "* In the LeakyReLU, the slope of the leak was set to 0.2 in all models.\n",
    "* we used the Adam optimizer(Kingma & Ba, 2014) with tuned hyperparameters. We found the suggested learning rate of 0.001, to be too high, using 0.0002 instead. \n",
    "* Additionally, we found leaving the momentum term $\\beta{1}$ at the suggested value of 0.9 resulted in training oscillation and instability while reducing it to 0.5 helped stabilize training.\n",
    "\n",
    "The paper also provides the following architecture guidelines for stable Deep Convolutional GANs:\n",
    "\n",
    "* Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator).\n",
    "* Use batchnorm in both the generator and the discriminator.\n",
    "* Remove fully connected hidden layers for deeper architectures.\n",
    "* Use ReLU activation in generator for all layers except for the output, which uses Tanh.\n",
    "* Use LeakyReLU activation in the discriminator for all layers.\n",
    "\n",
    "While creating the material for this module I used a number of Internet resources, some of the most helpful were:\n",
    "\n",
    "* [Keep Calm and train a GAN. Pitfalls and Tips on training Generative Adversarial Networks](https://medium.com/@utk.is.here/keep-calm-and-train-a-gan-pitfalls-and-tips-on-training-generative-adversarial-networks-edd529764aa9)\n",
    "* [Collection of Keras implementations of Generative Adversarial Networks GANs](https://github.com/eriklindernoren/Keras-GAN)\n",
    "* [dcgan-facegenerator](https://github.com/platonovsimeon/dcgan-facegenerator), [Semi-Paywalled Article by GitHub Author](https://medium.com/datadriveninvestor/generating-human-faces-with-keras-3ccd54c17f16)\n",
    "\n",
    "The source data (faces) used in this module can be found here:\n",
    "\n",
    "* [Kaggle Faces Data New](https://www.kaggle.com/gasgallo/faces-data-new)\n",
    "* [Kaggle Lag Dataset: Dataset of faces, from more than 1k different subjects](https://www.kaggle.com/gasgallo/lag-dataset)\n",
    "\n",
    "![GAN](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan-3.png \"GAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Reshape, Dropout, Dense, Flatten, BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this for Google CoLab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation resolution - Must be square \n",
    "# Training data is also scaled to this.\n",
    "# Note GENERATE_RES higher than 4 will blow Google CoLab's memory.\n",
    "GENERATE_RES = 2 # (1=32, 2=64, 3=96, etc.)\n",
    "GENERATE_SQUARE = 32 * GENERATE_RES # rows/cols (should be square)\n",
    "IMAGE_CHANNELS = 3\n",
    "\n",
    "# Preview image \n",
    "PREVIEW_ROWS = 4\n",
    "PREVIEW_COLS = 7\n",
    "PREVIEW_MARGIN = 16\n",
    "SAVE_FREQ = 100\n",
    "\n",
    "# Size vector to generate images from\n",
    "SEED_SIZE = 100\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = '/content/drive/My Drive/projects/faces'\n",
    "EPOCHS = 10000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(f\"Will generate {GENERATE_SQUARE}px square images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image set has 11,682 images.  Can take over an hour for initial preprocessing.\n",
    "# Because of this time needed, save a Numpy preprocessed file.\n",
    "# Note, that file is large enough to cause problems for sume verisons of Pickle,\n",
    "# so Numpy binary files are used.\n",
    "training_binary_path = os.path.join(DATA_PATH,f'training_data_{GENERATE_SQUARE}_{GENERATE_SQUARE}.npy')\n",
    "\n",
    "print(f\"Looking for file: {training_binary_path}\")\n",
    "\n",
    "if not os.path.isfile(training_binary_path):\n",
    "  print(\"Loading training images...\")\n",
    "\n",
    "  training_data = []\n",
    "  faces_path = os.path.join(DATA_PATH,'face_images')\n",
    "  for filename in tqdm(os.listdir(faces_path)):\n",
    "      path = os.path.join(faces_path,filename)\n",
    "      image = Image.open(path).resize((GENERATE_SQUARE,GENERATE_SQUARE),Image.ANTIALIAS)\n",
    "      training_data.append(np.asarray(image))\n",
    "  training_data = np.reshape(training_data,(-1,GENERATE_SQUARE,GENERATE_SQUARE,IMAGE_CHANNELS))\n",
    "  training_data = training_data / 127.5 - 1.\n",
    "\n",
    "  print(\"Saving training image binary...\")\n",
    "  np.save(training_binary_path,training_data)\n",
    "else:\n",
    "  print(\"Loading previous training pickle...\")\n",
    "  training_data = np.load(training_binary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(seed_size, channels):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(4*4*256,activation=\"relu\",input_dim=seed_size))\n",
    "    model.add(Reshape((4,4,256)))\n",
    "\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "   \n",
    "    # Output resolution, additional upsampling\n",
    "    for i in range(GENERATE_RES):\n",
    "      model.add(UpSampling2D())\n",
    "      model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n",
    "      model.add(BatchNormalization(momentum=0.8))\n",
    "      model.add(Activation(\"relu\"))\n",
    "\n",
    "    # Final CNN layer\n",
    "    model.add(Conv2D(channels,kernel_size=3,padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "\n",
    "    input = Input(shape=(seed_size,))\n",
    "    generated_image = model(input)\n",
    "\n",
    "    return Model(input,generated_image)\n",
    "\n",
    "\n",
    "def build_discriminator(image_shape):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=image_shape, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(512, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    input_image = Input(shape=image_shape)\n",
    "\n",
    "    validity = model(input_image)\n",
    "\n",
    "    return Model(input_image, validity)\n",
    "  \n",
    "def save_images(cnt,noise):\n",
    "  image_array = np.full(( \n",
    "      PREVIEW_MARGIN + (PREVIEW_ROWS * (GENERATE_SQUARE+PREVIEW_MARGIN)), \n",
    "      PREVIEW_MARGIN + (PREVIEW_COLS * (GENERATE_SQUARE+PREVIEW_MARGIN)), 3), \n",
    "      255, dtype=np.uint8)\n",
    "  \n",
    "  generated_images = generator.predict(noise)\n",
    "\n",
    "  generated_images = 0.5 * generated_images + 0.5\n",
    "\n",
    "  image_count = 0\n",
    "  for row in range(PREVIEW_ROWS):\n",
    "      for col in range(PREVIEW_COLS):\n",
    "        r = row * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n",
    "        c = col * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n",
    "        image_array[r:r+GENERATE_SQUARE,c:c+GENERATE_SQUARE] = generated_images[image_count] * 255\n",
    "        image_count += 1\n",
    "\n",
    "          \n",
    "  output_path = os.path.join(DATA_PATH,'output')\n",
    "  if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "  \n",
    "  filename = os.path.join(output_path,f\"train-{cnt}.png\")\n",
    "  im = Image.fromarray(image_array)\n",
    "  im.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (GENERATE_SQUARE,GENERATE_SQUARE,IMAGE_CHANNELS)\n",
    "optimizer = Adam(1.5e-4,0.5) # learning rate and momentum adjusted from paper\n",
    "\n",
    "discriminator = build_discriminator(image_shape)\n",
    "discriminator.compile(loss=\"binary_crossentropy\",optimizer=optimizer,metrics=[\"accuracy\"])\n",
    "generator = build_generator(SEED_SIZE,IMAGE_CHANNELS)\n",
    "\n",
    "random_input = Input(shape=(SEED_SIZE,))\n",
    "\n",
    "generated_image = generator(random_input)\n",
    "\n",
    "discriminator.trainable = False\n",
    "\n",
    "validity = discriminator(generated_image)\n",
    "\n",
    "combined = Model(random_input,validity)\n",
    "combined.compile(loss=\"binary_crossentropy\",optimizer=optimizer,metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_real = np.ones((BATCH_SIZE,1))\n",
    "y_fake = np.zeros((BATCH_SIZE,1))\n",
    "\n",
    "fixed_seed = np.random.normal(0, 1, (PREVIEW_ROWS * PREVIEW_COLS, SEED_SIZE))\n",
    "\n",
    "cnt = 1\n",
    "for epoch in range(EPOCHS):\n",
    "    idx = np.random.randint(0,training_data.shape[0],BATCH_SIZE)\n",
    "    x_real = training_data[idx]\n",
    "\n",
    "    # Generate some images\n",
    "    seed = np.random.normal(0,1,(BATCH_SIZE,SEED_SIZE))\n",
    "    x_fake = generator.predict(seed)\n",
    "\n",
    "    # Train discriminator on real and fake\n",
    "    discriminator_metric_real = discriminator.train_on_batch(x_real,y_real)\n",
    "    discriminator_metric_generated = discriminator.train_on_batch(x_fake,y_fake)\n",
    "    discriminator_metric = 0.5 * np.add(discriminator_metric_real,discriminator_metric_generated)\n",
    "    \n",
    "    # Train generator on Calculate losses\n",
    "    generator_metric = combined.train_on_batch(seed,y_real)\n",
    "    \n",
    "    # Time for an update?\n",
    "    if epoch % SAVE_FREQ == 0:\n",
    "        save_images(cnt, fixed_seed)\n",
    "        cnt += 1\n",
    "        print(f\"Epoch {epoch}, Discriminator accuarcy: {discriminator_metric[1]}, Generator accuracy: {generator_metric[1]}\")\n",
    "        \n",
    "generator.save(os.path.join(DATA_PATH,\"face_generator.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7.3: Face Generation with StyleGAN and Python\n",
    "\n",
    "GANs have appeared frequently in the media, showcasing their ability to generate extremely photorealistic faces.  One significant step forward for realistic face generation was nVidia StyleGAN, which was introduced in the following paper.\n",
    "\n",
    "* Karras, T., Laine, S., & Aila, T. (2018). [A style-based generator architecture for generative adversarial networks](https://arxiv.org/abs/1812.04948). *arXiv preprint arXiv:1812.04948*.\n",
    "\n",
    "In this part we will make use of StyleGAN.  We will also preload weights that nVidia trained on.  This will allow us to generate high resolution photorealistic looking faces, such as this one.\n",
    "\n",
    "![GAN](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan-example246.png \"GAN\")\n",
    "\n",
    "The above image was generated with StyleGAN, using Google CoLab.  Following the instructions in this section, you will be able to create faces like this of your own.  \n",
    "\n",
    "While the above image looks much more realistic than the previous set of images, it is not perfect.  There are usually a number of tell-tail signs that you are looking at a computer generated image.  One of the most obvious is usually the surreal, dream-like backgrounds.  The background does not look obviously fake, at first glance; however, upon closer inspection you usually can't quite discern exactly what a GAN generated background actually is.  Also look at the image character's left eye.  It is slightly unrealistic looking, especially near the eyelashes.\n",
    "\n",
    "Look at the following GAN face.  Can you spot any imperfections?\n",
    "\n",
    "![GAN](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan-example221.png \"GAN\")\n",
    "\n",
    "Notice the earrings?  GANs sometimes have problems with symmetry, particularly earrings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Sequence vs Functional Model API\n",
    "\n",
    "Most of the neural networks create in this course have made use of the Keras sequence object.  You might have noticed that we briefly made use of another type of neural network object for the ResNet, the Model.  These are the [two major means](https://keras.io/getting-started/functional-api-guide/) of constructing a neural network in Keras:\n",
    "\n",
    "* [Sequential](https://keras.io/getting-started/sequential-model-guide/) - Simplified interface to Keras that supports most models where the flow of information is a simple sequence from input to output. \n",
    "* [Keras Functional API](https://keras.io/getting-started/functional-api-guide/) - More complex interface that allows neural networks to be constructed of reused layers, multiple input layers, and supports building your own recurrent connections.\n",
    "\n",
    "It is important to point out that these are not two specific types of neural network.  Rather, they are two means of constructing neural networks in Keras.  Some types of neural network can be implemented in either, such as dense feedforward neural networks (like we used for the Iris and MPG datasets).  However, other types of neural network, like ResNet and GANs can only be used in the Functional Model API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating High Rez GAN Faces with Google CoLab\n",
    "\n",
    "This notebook demonstrates how to run [NVidia StyleGAN](https://github.com/NVlabs/stylegan) inside of a Google CoLab notebook.  I suggest you use this to generate GAN faces from a pretrained model.  If you try to train your own, you will run into compute limitations of Google CoLab.\n",
    "\n",
    "Make sure to run this code on a GPU instance.  GPU is assumed.\n",
    "\n",
    "First, map your G-Drive, this is where your GANs will be written to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this for Google CoLab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, clone StyleGAN from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVlabs/stylegan.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that StyleGAN has been cloned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /content/stylegan/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the StyleGAN folder to Python so that you can import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/content/stylegan\")\n",
    "\n",
    "import dnnlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is based on code from NVidia. This actually generates your images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n",
    "#\n",
    "# This work is licensed under the Creative Commons Attribution-NonCommercial\n",
    "# 4.0 International License. To view a copy of this license, visit\n",
    "# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n",
    "# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n",
    "\n",
    "\"\"\"Minimal script for generating an image using pre-trained StyleGAN generator.\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import dnnlib\n",
    "import dnnlib.tflib as tflib\n",
    "import config\n",
    "\n",
    "def main():\n",
    "    # Initialize TensorFlow.\n",
    "    tflib.init_tf()\n",
    "\n",
    "    # Load pre-trained network.\n",
    "    url = 'https://drive.google.com/uc?id=1MEGjdvVpUsu1jB4zrXZN7Y4kBBOzizDQ' # karras2019stylegan-ffhq-1024x1024.pkl\n",
    "    with dnnlib.util.open_url(url, cache_dir=config.cache_dir) as f:\n",
    "        _G, _D, Gs = pickle.load(f)\n",
    "        # _G = Instantaneous snapshot of the generator. Mainly useful for resuming a previous training run.\n",
    "        # _D = Instantaneous snapshot of the discriminator. Mainly useful for resuming a previous training run.\n",
    "        # Gs = Long-term average of the generator. Yields higher-quality results than the instantaneous snapshot.\n",
    "\n",
    "    # Print network details.\n",
    "    Gs.print_layers()\n",
    "\n",
    "    # Pick latent vector.\n",
    "    rnd = np.random.RandomState()\n",
    "    \n",
    "\n",
    "    latents = rnd.randn(1, Gs.input_shape[1])\n",
    "\n",
    "    # Generate image.\n",
    "    fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "    images = Gs.run(latents, None, truncation_psi=0.7, randomize_noise=True, output_transform=fmt)\n",
    "\n",
    "    # Save image.\n",
    "    os.makedirs(config.result_dir, exist_ok=True)\n",
    "    png_filename = os.path.join(config.result_dir, f'/content/drive/My Drive/images/example1.png')\n",
    "    PIL.Image.fromarray(images[0], 'RGB').save(png_filename)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7.4: GANS for Semi-Supervised Learning in Keras\n",
    "\n",
    "* [Odena, A. (2016). Semi-supervised learning with generative adversarial networks. *arXiv preprint* arXiv:1606.01583.](https://arxiv.org/abs/1606.01583)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\", \n",
    "    na_values=['NA', '?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet chevelle malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>buick skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>amc rebel sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>ford torino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>429.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>4341</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>ford galaxie 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>454.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>4354</td>\n",
       "      <td>9.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet impala</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>440.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>4312</td>\n",
       "      <td>8.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth fury iii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>455.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>4425</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>pontiac catalina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>390.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3850</td>\n",
       "      <td>8.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>amc ambassador dpl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
       "0  18.0          8         307.0       130.0    3504          12.0    70   \n",
       "1  15.0          8         350.0       165.0    3693          11.5    70   \n",
       "2  18.0          8         318.0       150.0    3436          11.0    70   \n",
       "3  16.0          8         304.0       150.0    3433          12.0    70   \n",
       "4  17.0          8         302.0       140.0    3449          10.5    70   \n",
       "5  15.0          8         429.0       198.0    4341          10.0    70   \n",
       "6  14.0          8         454.0       220.0    4354           9.0    70   \n",
       "7  14.0          8         440.0       215.0    4312           8.5    70   \n",
       "8  14.0          8         455.0       225.0    4425          10.0    70   \n",
       "9  15.0          8         390.0       190.0    3850           8.5    70   \n",
       "\n",
       "   origin                       name  \n",
       "0       1  chevrolet chevelle malibu  \n",
       "1       1          buick skylark 320  \n",
       "2       1         plymouth satellite  \n",
       "3       1              amc rebel sst  \n",
       "4       1                ford torino  \n",
       "5       1           ford galaxie 500  \n",
       "6       1           chevrolet impala  \n",
       "7       1          plymouth fury iii  \n",
       "8       1           pontiac catalina  \n",
       "9       1         amc ambassador dpl  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   8. ,  307. ,  130. , 3504. ,   12. ,   70. ],\n",
       "       [   8. ,  350. ,  165. , 3693. ,   11.5,   70. ],\n",
       "       [   8. ,  318. ,  150. , 3436. ,   11. ,   70. ],\n",
       "       ...,\n",
       "       [   4. ,  135. ,   84. , 2295. ,   11.6,   82. ],\n",
       "       [   4. ,  120. ,   79. , 2625. ,   18.6,   82. ],\n",
       "       [   4. ,  119. ,   82. , 2720. ,   19.4,   82. ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df[['cylinders','displacement','horsepower','weight','acceleration','year']].values\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Reshape, Dropout, Dense, Flatten, BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size vector to generate autos from\n",
    "SEED_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "SAVE_FREQ = 1\n",
    "DATA_PATH = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(seed_size,auto_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(4*4*256,activation=\"relu\",input_dim=seed_size))\n",
    "    \n",
    "    model.add(Dense(25, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "    model.add(Dense(11, activation='relu')) # Hidden 2\n",
    "    model.add(Dense(auto_size, activation='relu')) # Hidden 2\n",
    "\n",
    "    input = Input(shape=(seed_size,))\n",
    "    generated_auto = model(input)\n",
    "\n",
    "    return Model(input,generated_auto)\n",
    "\n",
    "\n",
    "def build_discriminator(image_shape):\n",
    "    model = Sequential()\n",
    "    \n",
    "    print(image_shape)\n",
    "    model.add(Dense(25, input_dim=image_shape, activation='relu')) # Hidden 1\n",
    "    model.add(Dense(10, activation='relu')) # Hidden 2\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    input_image = Input(shape= (image_shape,) )\n",
    "\n",
    "    validity = model(input_image)\n",
    "\n",
    "    return Model(input_image, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(1.5e-4,0.5)\n",
    "\n",
    "discriminator = build_discriminator(x.shape[1])\n",
    "discriminator.compile(loss=\"binary_crossentropy\",optimizer=optimizer,metrics=[\"accuracy\"])\n",
    "generator = build_generator(SEED_SIZE,x.shape[1])\n",
    "\n",
    "random_input = Input(shape=(SEED_SIZE,))\n",
    "\n",
    "generated_image = generator(random_input)\n",
    "\n",
    "discriminator.trainable = False\n",
    "\n",
    "validity = discriminator(generated_image)\n",
    "\n",
    "combined = Model(random_input,validity)\n",
    "combined.compile(loss=\"binary_crossentropy\",optimizer=optimizer,metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 1, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 2, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 3, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 4, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 5, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 6, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 7, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 8, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 9, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 10, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 11, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 12, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 13, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 14, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 15, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 16, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 17, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 18, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 19, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 20, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 21, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 22, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 23, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 24, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 25, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 26, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 27, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 28, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 29, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 30, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 31, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 32, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 33, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 34, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 35, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 36, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 37, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 38, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 39, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 40, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 41, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 42, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 43, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 44, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 45, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 46, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 47, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 48, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 49, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 50, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 51, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 52, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 53, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 54, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 55, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 56, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 57, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 58, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 59, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 60, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 61, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 62, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 63, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 64, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 65, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 66, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 67, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 68, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 69, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 70, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 71, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 72, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 73, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 74, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 75, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 76, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 77, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 78, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 79, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 80, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 81, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 82, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 83, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 84, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 85, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 86, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 87, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 88, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 89, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 90, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 91, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 92, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 93, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 94, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 95, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 96, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 97, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 98, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n",
      "Epoch 99, Discriminator accuarcy: 0.0, Generator accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "y_real = np.ones((BATCH_SIZE,1))\n",
    "y_fake = np.zeros((BATCH_SIZE,1))\n",
    "\n",
    "cnt = 1\n",
    "for epoch in range(EPOCHS):\n",
    "    idx = np.random.randint(0,x.shape[0],BATCH_SIZE)\n",
    "    x_real = x[idx]\n",
    "\n",
    "    # Generate some images\n",
    "    seed = np.random.normal(0,1,(BATCH_SIZE,SEED_SIZE))\n",
    "    x_fake = generator.predict(seed)\n",
    "\n",
    "    # Train discriminator on real and fake\n",
    "    discriminator_metric_real = discriminator.train_on_batch(x_real,y_real)\n",
    "    discriminator_metric_generated = discriminator.train_on_batch(x_fake,y_fake)\n",
    "    discriminator_metric = 0.5 * np.add(discriminator_metric_real,discriminator_metric_generated)\n",
    "    \n",
    "    # Train generator on Calculate losses\n",
    "    generator_metric = combined.train_on_batch(seed,y_real)\n",
    "    \n",
    "    # Time for an update?\n",
    "    if epoch % SAVE_FREQ == 0:\n",
    "        #save_images(cnt, fixed_seed)\n",
    "        cnt += 1\n",
    "        print(f\"Epoch {epoch}, Discriminator accuarcy: {discriminator_metric[1]}, Generator accuracy: {generator_metric[1]}\")\n",
    "        \n",
    "generator.save(os.path.join(DATA_PATH,\"automobile_generator.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7.5: An Overview of GAN Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Keras Implementations of Generative Adversarial Networks](https://github.com/eriklindernoren/Keras-GAN)\n",
    "* [Curated List of Awesome GAN Applications and Demo](https://github.com/nashory/gans-awesome-applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Module 7 Assignment\n",
    "\n",
    "You can find the first assignment here: [assignment 7](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class7.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.6 (wustl)",
   "language": "python",
   "name": "wustl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
