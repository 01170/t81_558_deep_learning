{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 12: Deep Learning and Security**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Video Material\n",
    "\n",
    "Main video lecture:\n",
    "\n",
    "* Part 12.1: Introduction to the OpenAI Gym [[Video]](https://www.youtube.com/playlist?list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_12_reinforcement.ipynb)\n",
    "* Part 12.2: Introduction to Q-Learning for Keras [[Video]](https://www.youtube.com/playlist?list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_12_reinforcement.ipynb)\n",
    "* Part 12.3: Keras Q-Learning in the OpenAI Gym [[Video]](https://www.youtube.com/playlist?list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_12_reinforcement.ipynb)\n",
    "* Part 12.4: Atari Games with Keras Neural Networks [[Video]](https://www.youtube.com/playlist?list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_12_reinforcement.ipynb)\n",
    "* **Part 12.5: How Alpha Zero used Reinforcement Learning to Master Chess** [[Video]](https://www.youtube.com/playlist?list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_12_reinforcement.ipynb)\n",
    "\n",
    "\n",
    "# Part 12.5: How Alpha Zero used Reinforcement Learning to Master Chess\n",
    "\n",
    "Google AlphaZero is an exciting technology that was developed by Google to master several different games with no previous human knowledge, entirely by self-play.  This part will present a high-level overview of AlphaZero and how its component technologies relate to this class.  For additional information about AlphaZero, refer to the following sources:\n",
    "\n",
    "* [Chess, a Drosophila of reasoning](https://science.sciencemag.org/content/362/6419/1087) by [Garry Kasparov](https://en.wikipedia.org/wiki/Garry_Kasparov)\n",
    "* [AlphaZero: Shedding new light on the grand games of chess, shogi and Go](https://deepmind.com/blog/alphazero-shedding-new-light-grand-games-chess-shogi-and-go/)\n",
    "* [Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/abs/1712.01815)\n",
    "* [Deepmind AlphaZero - Mastering Games Without Human Knowledge](https://www.youtube.com/watch?v=Wujy7OzvdJk)\n",
    "\n",
    "AlphaZero in many was is a culmination of the following achievements by Google.\n",
    "\n",
    "* **AlphaGo** - March 15, 2016, AlphaGo [beats](https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol) 18-time world champion Lee Sedol.\n",
    "* **AlphaGo Master** - May 27, 2017, AlphaGo Master [beats](https://en.wikipedia.org/wiki/AlphaGo_versus_Ke_Jie) current world No. 1 ranking player Ke Jie.  Also won 60 on-line games against top Go players.\n",
    "* **AlphaGo Zero** - October 19, 2017 introduced AlphaGo Zero, a version created without using data from human games, and stronger than any previous version. AlphaGo Zero surpassed the strength of AlphaGo Lee in three days by winning 100 games to 0, reached the level of AlphaGo Master in 21 days, and exceeded all the old versions in 40 days.\n",
    "* **AlphaZero** - December 5, 2017 AlphaGo Zero exceeded Stockfish not losing .\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlphaGo\n",
    "\n",
    "Game of Go is:\n",
    "* 3,000 years old\n",
    "* 40M Players\n",
    "* $10^{170}$ positions\n",
    "\n",
    "![AlphaGo Two Networks](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/alpha-zero-1.png \"AlphaGo Two Networks\")\n",
    "\n",
    "2 Convolutional neural networks\n",
    "* Policy Network - Need pic\n",
    "* Value Network - Positional Evaluation Knowldge , predicts winner or game -1 and +1 (win/loss)\n",
    "\n",
    "Supervised learning based on human games.  Reproduce (policy network) \n",
    "Policy network plays itself\n",
    "\n",
    "Train value network by reinforcement learning\n",
    "\n",
    "Narrow search space to reduce width of search\n",
    "Value network needs less depth\n",
    "\n",
    "Monte-Carlo tree search\n",
    "1. Traverse \n",
    "\n",
    "Lost one game.  Delusions\n",
    "\n",
    "Random rollouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlphaGo Master\n",
    "\n",
    "Ke Jia, AlphaGo Master Won 3-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlphaGo Zero\n",
    "\n",
    "* No human data\n",
    "* No human feature engineering\n",
    "* Single neural network (combine policy and value networks)\n",
    "* Simpler search\n",
    "\n",
    "State of the art residual network\n",
    "\n",
    "Random rollouts removed, only used neural network to evaluate.  More general, maybe apply to other games.\n",
    "\n",
    "MCTS - Lookahead, train neural network (policy) to come up with same result.\n",
    "Train value net to better predict winner\n",
    "\n",
    "Iterate over:\n",
    "* Search-Based Policy Improvement\n",
    "* Search-Based Policy Evaluation (key feature)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlphaZero\n",
    "\n",
    "* chess\n",
    "* shogi\n",
    "* Go\n",
    "\n",
    "Chess is:\n",
    "\n",
    "* Most studied domain in the history of AI\n",
    "* Highly specialized systems have been successful in chess\n",
    "* Shogi (Japanese Chess) is computationally harder than chess\n",
    "* State of the art engines are based on alpha-beta search (form of minimax)\n",
    "\n",
    "2016 TCEC - Stockfish\n",
    "\n",
    "* Board Representation\n",
    "* Search\n",
    "* Transposition Table\n",
    "* Move Ordering\n",
    "* Selectivity\n",
    "* Evaluation\n",
    "* End Game Tablebases\n",
    "\n",
    "Elo rating\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.6 (wustl)",
   "language": "python",
   "name": "wustl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
