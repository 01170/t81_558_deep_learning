{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 8: Kaggle Data Sets**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Video Material\n",
    "\n",
    "Main video lecture:\n",
    "\n",
    "* [Part 8.1: Introduction to Kaggle](https://www.youtube.com/watch?v=XpGI4engRjQ&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN&index=24)\n",
    "* [Part 8.2: Building Ensembles with Scikit-Learn and Keras](https://www.youtube.com/watch?v=AA3KFxjPxCo&index=25&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN)\n",
    "* [Part 8.3: How Should you Architect Your Keras Neural Network: Hyperparameters](https://www.youtube.com/watch?v=GaKo-9c532c)\n",
    "* [Part 8.4: Bayesian Hyperparameter Optimization for Keras](https://www.youtube.com/watch?v=GaKo-9c532c)\n",
    "* [Part 8.5: Current Semester's Kaggle](https://www.youtube.com/watch?v=GaKo-9c532c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 8.1: Introduction to Kaggle\n",
    "\n",
    "[Kaggle](http://www.kaggle.com) runs competitions in which data scientists compete in order to provide the best model to fit the data. A common project to get started with Kaggle is the [Titanic data set](https://www.kaggle.com/c/titanic-gettingStarted). Most Kaggle competitions end on a specific date. Website organizers have currently scheduled the Titanic competition to end on December 31, 20xx (with the year usually rolling forward). However, they have already extended the deadline several times, and an extension beyond 2014 is also possible. Second, the Titanic data set is considered a tutorial data set. In other words, there is no prize, and your score in the competition does not count towards becoming a Kaggle Master. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Ranks\n",
    "\n",
    "Kaggle ranks are achieved by earning gold, silver and bronze medals.\n",
    "\n",
    "* [Kaggle Top Users](https://www.kaggle.com/rankings)\n",
    "* [Current Top Kaggle User's Profile Page](https://www.kaggle.com/stasg7)\n",
    "* [Jeff Heaton's (your instructor) Kaggle Profile](https://www.kaggle.com/jeffheaton)\n",
    "* [Current Kaggle Ranking System](https://www.kaggle.com/progression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical Kaggle Competition\n",
    "\n",
    "A typical Kaggle competition will have several components.  Consider the Titanic tutorial:\n",
    "\n",
    "* [Competition Summary Page](https://www.kaggle.com/c/titanic)\n",
    "* [Data Page](https://www.kaggle.com/c/titanic/data)\n",
    "* [Evaluation Description Page](https://www.kaggle.com/c/titanic/details/evaluation)\n",
    "* [Leaderboard](https://www.kaggle.com/c/titanic/leaderboard)\n",
    "\n",
    "### How Kaggle Competitions are Scored\n",
    "\n",
    "Kaggle is provided with a data set by the competition sponsor.  This data set is divided up as follows:\n",
    "\n",
    "* **Complete Data Set** - This is the complete data set.\n",
    "    * **Training Data Set** - You are provided both the inputs and the outcomes for the training portion of the data set.\n",
    "    * **Test Data Set** - You are provided the complete test data set; however, you are not given the outcomes.  Your submission is  your predicted outcomes for this data set.\n",
    "        * **Public Leaderboard** - You are not told what part of the test data set contributes to the public leaderboard.  Your public score is calculated based on this part of the data set.\n",
    "        * **Private Leaderboard** - You are not told what part of the test data set contributes to the public leaderboard.  Your final score/rank is calculated based on this part.  You do not see your private leaderboard score until the end.\n",
    "\n",
    "![How Kaggle Competitions are Scored](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_3_kaggle.png \"How Kaggle Competitions are Scored\")\n",
    "\n",
    "### Preparing a Kaggle Submission\n",
    "\n",
    "Code need not be submitted to Kaggle.  For competitions, you are scored entirely on the accuracy of your sbmission file.  A Kaggle submission file is always a CSV file that contains the **Id** of the row you are predicting and the answer.  For the titanic competition, a submission file looks something like this:\n",
    "\n",
    "```\n",
    "PassengerId,Survived\n",
    "892,0\n",
    "893,1\n",
    "894,1\n",
    "895,0\n",
    "896,0\n",
    "897,1\n",
    "...\n",
    "```\n",
    "\n",
    "The above file states the prediction for each of various passengers.  You should only predict on ID's that are in the test file.  Likewise, you should render a prediction for every row in the test file.  Some competitions will have different formats for their answers.  For example, a multi-classification will usually have a column for each class and your predictions for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Kaggle Competitions\n",
    "\n",
    "There have been many interesting competitions on Kaggle, these are some of my favorites.\n",
    "\n",
    "## Predictive Modeling\n",
    "\n",
    "* [Otto Group Product Classification Challenge](https://www.kaggle.com/c/otto-group-product-classification-challenge)\n",
    "* [Galaxy Zoo - The Galaxy Challenge](https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge)\n",
    "* [Practice Fusion Diabetes Classification](https://www.kaggle.com/c/pf2012-diabetes)\n",
    "* [Predicting a Biological Response](https://www.kaggle.com/c/bioresponse)\n",
    "\n",
    "## Computer Vision\n",
    "\n",
    "* [Diabetic Retinopathy Detection](https://www.kaggle.com/c/diabetic-retinopathy-detection)\n",
    "* [Cats vs Dogs](https://www.kaggle.com/c/dogs-vs-cats)\n",
    "* [State Farm Distracted Driver Detection](https://www.kaggle.com/c/state-farm-distracted-driver-detection)\n",
    "\n",
    "## Time Series\n",
    "\n",
    "* [The Marinexplore and Cornell University Whale Detection Challenge](https://www.kaggle.com/c/whale-detection-challenge)\n",
    "\n",
    "## Other\n",
    "\n",
    "* [Helping Santa's Helpers](https://www.kaggle.com/c/helping-santas-helpers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris as a Kaggle Competition\n",
    "\n",
    "If the Iris data were used as a Kaggle, you would be given the following three files:\n",
    "\n",
    "* [kaggle_iris_test.csv](https://data.heatonresearch.com/data/t81-558/datasets/kaggle_iris_test.csv) - The data that Kaggle will evaluate you on.  Contains only input, you must provide answers.  (contains x)\n",
    "* [kaggle_iris_train.csv](https://data.heatonresearch.com/data/t81-558/datasets/kaggle_iris_train.csv) - The data that you will use to train. (contains x and y)\n",
    "* [kaggle_iris_sample.csv](https://data.heatonresearch.com/data/t81-558/datasets/kaggle_iris_sample.csv) - A sample submission for Kaggle. (contains x and y)\n",
    "\n",
    "Important features of the Kaggle iris files (that differ from how we've previously seen files):\n",
    "\n",
    "* The iris species is already index encoded.\n",
    "* Your training data is in a separate file.\n",
    "* You will load the test data to generate a submission file.\n",
    "\n",
    "The following program generates a submission file for \"Iris Kaggle\".  You can use it as a starting point for assignment 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 3\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a2f13b160>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "df_train = pd.read_csv(\"https://data.heatonresearch.com/data/t81-558/datasets/kaggle_iris_train.csv\",\n",
    "                       na_values=['NA','?'])\n",
    "\n",
    "# Encode feature vector\n",
    "df_train.drop('id', axis=1, inplace=True)\n",
    "\n",
    "num_classes = len(df_train.groupby('species').species.nunique())\n",
    "\n",
    "print(\"Number of classes: {}\".format(num_classes))\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "x = df_train[['sepal_l', 'sepal_w', 'petal_l', 'petal_w']].values\n",
    "dummies = pd.get_dummies(df_train['species']) # Classification\n",
    "species = dummies.columns\n",
    "y = dummies.values\n",
    "    \n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=45)\n",
    "\n",
    "# Train, with early stopping\n",
    "model = Sequential()\n",
    "model.add(Dense(0, input_dim=x.shape[1], activation='relu'))\n",
    "model.add(Dense(20))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto',\n",
    "                       restore_best_weights=True)\n",
    "\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=0,epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss score: 1.0993138504028321\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Calculate multi log loss error\n",
    "pred = model.predict(x_test)\n",
    "score = metrics.log_loss(y_test, pred)\n",
    "print(\"Log loss score: {}\".format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id  species-0  species-1  species-2\n",
      "0   100   0.330798   0.335996   0.333206\n",
      "1   101   0.330798   0.335996   0.333206\n",
      "2   102   0.330798   0.335996   0.333206\n",
      "3   103   0.330798   0.335996   0.333206\n",
      "4   104   0.330798   0.335996   0.333206\n",
      "5   105   0.330798   0.335996   0.333206\n",
      "6   106   0.330798   0.335996   0.333206\n",
      "7   107   0.330798   0.335996   0.333206\n",
      "8   108   0.330798   0.335996   0.333206\n",
      "9   109   0.330798   0.335996   0.333206\n",
      "10  110   0.330798   0.335996   0.333206\n",
      "11  111   0.330798   0.335996   0.333206\n",
      "12  112   0.330798   0.335996   0.333206\n",
      "13  113   0.330798   0.335996   0.333206\n",
      "14  114   0.330798   0.335996   0.333206\n",
      "15  115   0.330798   0.335996   0.333206\n",
      "16  116   0.330798   0.335996   0.333206\n",
      "17  117   0.330798   0.335996   0.333206\n",
      "18  118   0.330798   0.335996   0.333206\n",
      "19  119   0.330798   0.335996   0.333206\n",
      "20  120   0.330798   0.335996   0.333206\n",
      "21  121   0.330798   0.335996   0.333206\n",
      "22  122   0.330798   0.335996   0.333206\n",
      "23  123   0.330798   0.335996   0.333206\n",
      "24  124   0.330798   0.335996   0.333206\n",
      "25  125   0.330798   0.335996   0.333206\n",
      "26  126   0.330798   0.335996   0.333206\n",
      "27  127   0.330798   0.335996   0.333206\n",
      "28  128   0.330798   0.335996   0.333206\n",
      "29  129   0.330798   0.335996   0.333206\n",
      "30  130   0.330798   0.335996   0.333206\n",
      "31  131   0.330798   0.335996   0.333206\n",
      "32  132   0.330798   0.335996   0.333206\n",
      "33  133   0.330798   0.335996   0.333206\n",
      "34  134   0.330798   0.335996   0.333206\n",
      "35  135   0.330798   0.335996   0.333206\n",
      "36  136   0.330798   0.335996   0.333206\n",
      "37  137   0.330798   0.335996   0.333206\n",
      "38  138   0.330798   0.335996   0.333206\n",
      "39  139   0.330798   0.335996   0.333206\n",
      "40  140   0.330798   0.335996   0.333206\n",
      "41  141   0.330798   0.335996   0.333206\n",
      "42  142   0.330798   0.335996   0.333206\n",
      "43  143   0.330798   0.335996   0.333206\n",
      "44  144   0.330798   0.335996   0.333206\n",
      "45  145   0.330798   0.335996   0.333206\n",
      "46  146   0.330798   0.335996   0.333206\n",
      "47  147   0.330798   0.335996   0.333206\n",
      "48  148   0.330798   0.335996   0.333206\n",
      "49  149   0.330798   0.335996   0.333206\n",
      "50  150   0.330798   0.335996   0.333206\n"
     ]
    }
   ],
   "source": [
    "# Generate Kaggle submit file\n",
    "\n",
    "# Encode feature vector\n",
    "df_test = pd.read_csv(\"https://data.heatonresearch.com/data/t81-558/datasets/kaggle_iris_test.csv\",\n",
    "                      na_values=['NA','?'])\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "ids = df_test['id']\n",
    "df_test.drop('id', axis=1, inplace=True)\n",
    "x = df_test[['sepal_l', 'sepal_w', 'petal_l', 'petal_w']].values\n",
    "y = dummies.values\n",
    "\n",
    "# Generate predictions\n",
    "pred = model.predict(x)\n",
    "#pred\n",
    "\n",
    "# Create submission data set\n",
    "\n",
    "df_submit = pd.DataFrame(pred)\n",
    "df_submit.insert(0,'id',ids)\n",
    "df_submit.columns = ['id','species-0','species-1','species-2']\n",
    "\n",
    "df_submit.to_csv(\"iris_submit.csv\", index=False) # Write submit file locally\n",
    "\n",
    "print(df_submit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPG as a Kaggle Competition (Regression)\n",
    "\n",
    "If the Auto MPG data were used as a Kaggle, you would be given the following three files:\n",
    "\n",
    "* [kaggle_mpg_test.csv](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/data/kaggle_mpg_test.csv) - The data that Kaggle will evaluate you on.  Contains only input, you must provide answers.  (contains x)\n",
    "* [kaggle_mpg_train.csv](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/data/kaggle_mpg_train.csv) - The data that you will use to train. (contains x and y)\n",
    "* [kaggle_mpg_sample.csv](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/data/kaggle_mpg_sample.csv) - A sample submission for Kaggle. (contains x and y)\n",
    "\n",
    "Important features of the Kaggle iris files (that differ from how we've previously seen files):\n",
    "\n",
    "The following program generates a submission file for \"MPG Kaggle\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8.2: Building Ensembles with Scikit-Learn and Keras\n",
    "\n",
    "### Evaluating Feature Importance\n",
    "\n",
    "Feature importance tells us how important each of the features (from the feature/import vector are to the prediction of a neural network, or other model.  There are many different ways to evaluate feature importance for neural networks.  The following paper presents a very good (and readable) overview of the various means of evaluating the importance of neural network inputs/features.\n",
    "\n",
    "Olden, J. D., Joy, M. K., & Death, R. G. (2004). [An accurate comparison of methods for quantifying variable importance in artificial neural networks using simulated data](http://depts.washington.edu/oldenlab/wordpress/wp-content/uploads/2013/03/EcologicalModelling_2004.pdf). *Ecological Modelling*, 178(3), 389-397.\n",
    "\n",
    "In summary, the following methods are available to neural networks:\n",
    "\n",
    "* Connection Weights Algorithm\n",
    "* Partial Derivatives\n",
    "* Input Perturbation\n",
    "* Sensitivity Analysis\n",
    "* Forward Stepwise Addition \n",
    "* Improved Stepwise Selection 1\n",
    "* Backward Stepwise Elimination\n",
    "* Improved Stepwise Selection\n",
    "\n",
    "For this class we will use the **Input Perturbation** feature ranking algorithm.  This algorithm will work with any regression or classification network.  implementation of the input perturbation algorithm for scikit-learn is given in the next section. This algorithm is implemented in a function below that will work with any scikit-learn model.\n",
    "\n",
    "This algorithm was introduced by [Breiman](https://en.wikipedia.org/wiki/Leo_Breiman) in his seminal paper on random forests.  Although he presented this algorithm in conjunction with random forests, it is model-independent and appropriate for any supervised learning model.  This algorithm, known as the input perturbation algorithm, works by evaluating a trained model’s accuracy with each of the inputs individually shuffled from a data set.  Shuffling an input causes it to become useless—effectively removing it from the model. More important inputs will produce a less accurate score when they are removed by shuffling them. This process makes sense, because important features will contribute to the accuracy of the model.  The TensorFlow version of this algorithm is taken from the following paper.\n",
    "\n",
    "Heaton, J., McElwee, S., & Cannady, J. (May 2017). Early stabilizing feature importance for TensorFlow deep neural networks. In *International Joint Conference on Neural Networks (IJCNN 2017)* (accepted for publication). IEEE.\n",
    "\n",
    "This algorithm will use logloss to evaluate a classification problem and RMSE for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn import metrics\n",
    "\n",
    "def perturbation_rank(model, x, y, names, regression):\n",
    "    errors = []\n",
    "\n",
    "    for i in range(x.shape[1]):\n",
    "        hold = np.array(x[:, i])\n",
    "        np.random.shuffle(x[:, i])\n",
    "        \n",
    "        if regression:\n",
    "            pred = model.predict(x)\n",
    "            error = metrics.mean_squared_error(y, pred)\n",
    "        else:\n",
    "            pred = model.predict_proba(x)\n",
    "            error = metrics.log_loss(y, pred)\n",
    "            \n",
    "        errors.append(error)\n",
    "        x[:, i] = hold\n",
    "        \n",
    "    max_error = np.max(errors)\n",
    "    importance = [e/max_error for e in errors]\n",
    "\n",
    "    data = {'name':names,'error':errors,'importance':importance}\n",
    "    result = pd.DataFrame(data, columns = ['name','error','importance'])\n",
    "    result.sort_values(by=['importance'], ascending=[0], inplace=True)\n",
    "    result.reset_index(inplace=True, drop=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and Input Perturbation Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ce5cfa88d2b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Split into train/test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m x_train, x_test, y_train, y_test = train_test_split(    \n\u001b[0m\u001b[1;32m     22\u001b[0m     x, y, test_size=0.25, random_state=42)\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/iris.csv\", \n",
    "    na_values=['NA', '?'])\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "x = df[['sepal_l', 'sepal_w', 'petal_l', 'petal_w']].values\n",
    "dummies = pd.get_dummies(df['species']) # Classification\n",
    "species = dummies.columns\n",
    "y = dummies.values\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Build neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "model.add(Dense(25, activation='relu')) # Hidden 2\n",
    "model.add(Dense(y.shape[1],activation='softmax')) # Output\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(x_train,y_train,verbose=2,epochs=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "predict_classes = np.argmax(pred,axis=1)\n",
    "expected_classes = np.argmax(y_test,axis=1)\n",
    "correct = accuracy_score(expected_classes,predict_classes)\n",
    "print(f\"Accuracy: {correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>petal_l</td>\n",
       "      <td>1.761093</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>petal_w</td>\n",
       "      <td>0.792795</td>\n",
       "      <td>0.450172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sepal_w</td>\n",
       "      <td>0.107407</td>\n",
       "      <td>0.060989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sepal_l</td>\n",
       "      <td>0.094764</td>\n",
       "      <td>0.053810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name     error  importance\n",
       "0  petal_l  1.761093    1.000000\n",
       "1  petal_w  0.792795    0.450172\n",
       "2  sepal_w  0.107407    0.060989\n",
       "3  sepal_l  0.094764    0.053810"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rank the features\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = list(df.columns) # x+y column names\n",
    "names.remove(\"species\") # remove the target(y)\n",
    "rank = perturbation_rank(model, x_test, y_test, names, False)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression and Input Perturbation Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "298/298 - 0s - loss: 32606.5345\n",
      "Epoch 2/100\n",
      "298/298 - 0s - loss: 2778.6739\n",
      "Epoch 3/100\n",
      "298/298 - 0s - loss: 2764.0245\n",
      "Epoch 4/100\n",
      "298/298 - 0s - loss: 2036.1486\n",
      "Epoch 5/100\n",
      "298/298 - 0s - loss: 1119.0596\n",
      "Epoch 6/100\n",
      "298/298 - 0s - loss: 1095.0673\n",
      "Epoch 7/100\n",
      "298/298 - 0s - loss: 971.4769\n",
      "Epoch 8/100\n",
      "298/298 - 0s - loss: 924.0342\n",
      "Epoch 9/100\n",
      "298/298 - 0s - loss: 884.0933\n",
      "Epoch 10/100\n",
      "298/298 - 0s - loss: 851.9972\n",
      "Epoch 11/100\n",
      "298/298 - 0s - loss: 815.1204\n",
      "Epoch 12/100\n",
      "298/298 - 0s - loss: 761.1936\n",
      "Epoch 13/100\n",
      "298/298 - 0s - loss: 715.6020\n",
      "Epoch 14/100\n",
      "298/298 - 0s - loss: 635.3474\n",
      "Epoch 15/100\n",
      "298/298 - 0s - loss: 516.0050\n",
      "Epoch 16/100\n",
      "298/298 - 0s - loss: 309.9671\n",
      "Epoch 17/100\n",
      "298/298 - 0s - loss: 159.7871\n",
      "Epoch 18/100\n",
      "298/298 - 0s - loss: 125.9634\n",
      "Epoch 19/100\n",
      "298/298 - 0s - loss: 116.7699\n",
      "Epoch 20/100\n",
      "298/298 - 0s - loss: 115.4136\n",
      "Epoch 21/100\n",
      "298/298 - 0s - loss: 115.0016\n",
      "Epoch 22/100\n",
      "298/298 - 0s - loss: 112.7934\n",
      "Epoch 23/100\n",
      "298/298 - 0s - loss: 112.0152\n",
      "Epoch 24/100\n",
      "298/298 - 0s - loss: 110.6687\n",
      "Epoch 25/100\n",
      "298/298 - 0s - loss: 109.4967\n",
      "Epoch 26/100\n",
      "298/298 - 0s - loss: 108.5609\n",
      "Epoch 27/100\n",
      "298/298 - 0s - loss: 107.5834\n",
      "Epoch 28/100\n",
      "298/298 - 0s - loss: 106.1576\n",
      "Epoch 29/100\n",
      "298/298 - 0s - loss: 106.0623\n",
      "Epoch 30/100\n",
      "298/298 - 0s - loss: 104.2844\n",
      "Epoch 31/100\n",
      "298/298 - 0s - loss: 103.4108\n",
      "Epoch 32/100\n",
      "298/298 - 0s - loss: 102.0935\n",
      "Epoch 33/100\n",
      "298/298 - 0s - loss: 101.2474\n",
      "Epoch 34/100\n",
      "298/298 - 0s - loss: 100.4001\n",
      "Epoch 35/100\n",
      "298/298 - 0s - loss: 99.3460\n",
      "Epoch 36/100\n",
      "298/298 - 0s - loss: 98.6894\n",
      "Epoch 37/100\n",
      "298/298 - 0s - loss: 97.7879\n",
      "Epoch 38/100\n",
      "298/298 - 0s - loss: 97.6663\n",
      "Epoch 39/100\n",
      "298/298 - 0s - loss: 95.6673\n",
      "Epoch 40/100\n",
      "298/298 - 0s - loss: 94.8393\n",
      "Epoch 41/100\n",
      "298/298 - 0s - loss: 93.6464\n",
      "Epoch 42/100\n",
      "298/298 - 0s - loss: 93.1949\n",
      "Epoch 43/100\n",
      "298/298 - 0s - loss: 91.7112\n",
      "Epoch 44/100\n",
      "298/298 - 0s - loss: 91.1553\n",
      "Epoch 45/100\n",
      "298/298 - 0s - loss: 91.1571\n",
      "Epoch 46/100\n",
      "298/298 - 0s - loss: 90.2418\n",
      "Epoch 47/100\n",
      "298/298 - 0s - loss: 88.5966\n",
      "Epoch 48/100\n",
      "298/298 - 0s - loss: 88.6587\n",
      "Epoch 49/100\n",
      "298/298 - 0s - loss: 86.9323\n",
      "Epoch 50/100\n",
      "298/298 - 0s - loss: 87.5602\n",
      "Epoch 51/100\n",
      "298/298 - 0s - loss: 86.4559\n",
      "Epoch 52/100\n",
      "298/298 - 0s - loss: 84.5946\n",
      "Epoch 53/100\n",
      "298/298 - 0s - loss: 84.0452\n",
      "Epoch 54/100\n",
      "298/298 - 0s - loss: 83.2083\n",
      "Epoch 55/100\n",
      "298/298 - 0s - loss: 82.5072\n",
      "Epoch 56/100\n",
      "298/298 - 0s - loss: 81.9994\n",
      "Epoch 57/100\n",
      "298/298 - 0s - loss: 81.7953\n",
      "Epoch 58/100\n",
      "298/298 - 0s - loss: 80.4082\n",
      "Epoch 59/100\n",
      "298/298 - 0s - loss: 80.1825\n",
      "Epoch 60/100\n",
      "298/298 - 0s - loss: 79.5632\n",
      "Epoch 61/100\n",
      "298/298 - 0s - loss: 78.6181\n",
      "Epoch 62/100\n",
      "298/298 - 0s - loss: 78.5715\n",
      "Epoch 63/100\n",
      "298/298 - 0s - loss: 77.9867\n",
      "Epoch 64/100\n",
      "298/298 - 0s - loss: 77.6277\n",
      "Epoch 65/100\n",
      "298/298 - 0s - loss: 76.6454\n",
      "Epoch 66/100\n",
      "298/298 - 0s - loss: 75.4906\n",
      "Epoch 67/100\n",
      "298/298 - 0s - loss: 74.9005\n",
      "Epoch 68/100\n",
      "298/298 - 0s - loss: 75.5342\n",
      "Epoch 69/100\n",
      "298/298 - 0s - loss: 72.8821\n",
      "Epoch 70/100\n",
      "298/298 - 0s - loss: 73.2473\n",
      "Epoch 71/100\n",
      "298/298 - 0s - loss: 72.0113\n",
      "Epoch 72/100\n",
      "298/298 - 0s - loss: 71.6630\n",
      "Epoch 73/100\n",
      "298/298 - 0s - loss: 71.0266\n",
      "Epoch 74/100\n",
      "298/298 - 0s - loss: 70.8941\n",
      "Epoch 75/100\n",
      "298/298 - 0s - loss: 70.2492\n",
      "Epoch 76/100\n",
      "298/298 - 0s - loss: 69.3485\n",
      "Epoch 77/100\n",
      "298/298 - 0s - loss: 69.3185\n",
      "Epoch 78/100\n",
      "298/298 - 0s - loss: 69.0729\n",
      "Epoch 79/100\n",
      "298/298 - 0s - loss: 68.5890\n",
      "Epoch 80/100\n",
      "298/298 - 0s - loss: 67.5585\n",
      "Epoch 81/100\n",
      "298/298 - 0s - loss: 67.0749\n",
      "Epoch 82/100\n",
      "298/298 - 0s - loss: 66.5255\n",
      "Epoch 83/100\n",
      "298/298 - 0s - loss: 66.7694\n",
      "Epoch 84/100\n",
      "298/298 - 0s - loss: 67.3678\n",
      "Epoch 85/100\n",
      "298/298 - 0s - loss: 66.5568\n",
      "Epoch 86/100\n",
      "298/298 - 0s - loss: 64.9722\n",
      "Epoch 87/100\n",
      "298/298 - 0s - loss: 65.0336\n",
      "Epoch 88/100\n",
      "298/298 - 0s - loss: 64.6684\n",
      "Epoch 89/100\n",
      "298/298 - 0s - loss: 64.0633\n",
      "Epoch 90/100\n",
      "298/298 - 0s - loss: 63.6686\n",
      "Epoch 91/100\n",
      "298/298 - 0s - loss: 63.2629\n",
      "Epoch 92/100\n",
      "298/298 - 0s - loss: 63.9764\n",
      "Epoch 93/100\n",
      "298/298 - 0s - loss: 65.8389\n",
      "Epoch 94/100\n",
      "298/298 - 0s - loss: 61.9319\n",
      "Epoch 95/100\n",
      "298/298 - 0s - loss: 61.6919\n",
      "Epoch 96/100\n",
      "298/298 - 0s - loss: 61.1559\n",
      "Epoch 97/100\n",
      "298/298 - 0s - loss: 62.1910\n",
      "Epoch 98/100\n",
      "298/298 - 0s - loss: 62.7558\n",
      "Epoch 99/100\n",
      "298/298 - 0s - loss: 60.1124\n",
      "Epoch 100/100\n",
      "298/298 - 0s - loss: 59.6782\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "save_path = \".\"\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\", \n",
    "    na_values=['NA', '?'])\n",
    "\n",
    "cars = df['name']\n",
    "\n",
    "# Handle missing value\n",
    "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n",
    "\n",
    "# Pandas to Numpy\n",
    "x = df[['cylinders', 'displacement', 'horsepower', 'weight',\n",
    "       'acceleration', 'year', 'origin']].values\n",
    "y = df['mpg'].values # regression\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Build the neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "model.add(Dense(10, activation='relu')) # Hidden 2\n",
    "model.add(Dense(1)) # Output\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(x_train,y_train,verbose=2,epochs=100)\n",
    "\n",
    "# Predict\n",
    "pred = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>displacement</td>\n",
       "      <td>394.341258</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weight</td>\n",
       "      <td>345.884603</td>\n",
       "      <td>0.877120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>horsepower</td>\n",
       "      <td>53.142493</td>\n",
       "      <td>0.134763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>year</td>\n",
       "      <td>52.398014</td>\n",
       "      <td>0.132875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>origin</td>\n",
       "      <td>49.303057</td>\n",
       "      <td>0.125026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cylinders</td>\n",
       "      <td>49.274154</td>\n",
       "      <td>0.124953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>acceleration</td>\n",
       "      <td>48.957142</td>\n",
       "      <td>0.124149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name       error  importance\n",
       "0  displacement  394.341258    1.000000\n",
       "1        weight  345.884603    0.877120\n",
       "2    horsepower   53.142493    0.134763\n",
       "3          year   52.398014    0.132875\n",
       "4        origin   49.303057    0.125026\n",
       "5     cylinders   49.274154    0.124953\n",
       "6  acceleration   48.957142    0.124149"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rank the features\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = list(df.columns) # x+y column names\n",
    "names.remove(\"name\")\n",
    "names.remove(\"mpg\") # remove the target(y)\n",
    "rank = perturbation_rank(model, x_test, y_test, names, True)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biological Response with Neural Network\n",
    "\n",
    "* [Predicting a Biological Response](https://www.kaggle.com/c/bioresponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import KFold\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "filename_train = os.path.join(path,\"bio_train.csv\")\n",
    "filename_test = os.path.join(path,\"bio_test.csv\")\n",
    "filename_submit = os.path.join(path,\"bio_submit.csv\")\n",
    "\n",
    "df_train = pd.read_csv(filename_train,na_values=['NA','?'])\n",
    "df_test = pd.read_csv(filename_test,na_values=['NA','?'])\n",
    "\n",
    "activity_classes = df_train['Activity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3751, 1777)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting/Training...\n",
      "Epoch 00009: early stopping\n",
      "Fitting done...\n",
      "Validation logloss: 0.563175779301549\n",
      "Validation accuracy score: 0.7750533049040512\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "# Encode feature vector\n",
    "# Convert to numpy - Classification\n",
    "x_columns = df_train.columns.drop('Activity')\n",
    "x = df_train[x_columns].values\n",
    "y = df_train['Activity'].values # Classification\n",
    "x_submit = df_test[x_columns].values.astype(np.float32)\n",
    "\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=42) \n",
    "\n",
    "print(\"Fitting/Training...\")\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=x.shape[1], activation='relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=0,epochs=1000)\n",
    "print(\"Fitting done...\")\n",
    "\n",
    "# Predict\n",
    "pred = model.predict(x_test).flatten()\n",
    "\n",
    "\n",
    "# Clip so that min is never exactly 0, max never 1\n",
    "pred = np.clip(pred,a_min=1e-6,a_max=(1-1e-6)) \n",
    "print(\"Validation logloss: {}\".format(sklearn.metrics.log_loss(y_test,pred)))\n",
    "\n",
    "# Evaluate success using accuracy\n",
    "pred = pred>0.5 # If greater than 0.5 probability, then true\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"Validation accuracy score: {}\".format(score))\n",
    "\n",
    "# Build real submit file\n",
    "pred_submit = model.predict(x_submit)\n",
    "\n",
    "# Clip so that min is never exactly 0, max never 1 (would be a NaN score)\n",
    "pred = np.clip(pred,a_min=1e-6,a_max=(1-1e-6)) \n",
    "submit_df = pd.DataFrame({'MoleculeId':[x+1 for x in range(len(pred_submit))],'PredictedProbability':pred_submit.flatten()})\n",
    "submit_df.to_csv(filename_submit, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Features/Columns are Important\n",
    "The following uses perturbation ranking to evaluate the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D27</td>\n",
       "      <td>0.618613</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D51</td>\n",
       "      <td>0.575680</td>\n",
       "      <td>0.930598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D1159</td>\n",
       "      <td>0.571298</td>\n",
       "      <td>0.923514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D1049</td>\n",
       "      <td>0.570479</td>\n",
       "      <td>0.922191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D1241</td>\n",
       "      <td>0.570324</td>\n",
       "      <td>0.921939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>D1167</td>\n",
       "      <td>0.570319</td>\n",
       "      <td>0.921931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>D1059</td>\n",
       "      <td>0.569944</td>\n",
       "      <td>0.921326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>D119</td>\n",
       "      <td>0.569884</td>\n",
       "      <td>0.921229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>D1100</td>\n",
       "      <td>0.568941</td>\n",
       "      <td>0.919704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>D1188</td>\n",
       "      <td>0.568908</td>\n",
       "      <td>0.919650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>D1115</td>\n",
       "      <td>0.568722</td>\n",
       "      <td>0.919350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>D1128</td>\n",
       "      <td>0.568721</td>\n",
       "      <td>0.919348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>D117</td>\n",
       "      <td>0.568668</td>\n",
       "      <td>0.919262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>D1271</td>\n",
       "      <td>0.568464</td>\n",
       "      <td>0.918932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>D1432</td>\n",
       "      <td>0.568267</td>\n",
       "      <td>0.918614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>D201</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.918477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>D1009</td>\n",
       "      <td>0.568157</td>\n",
       "      <td>0.918437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>D1116</td>\n",
       "      <td>0.567976</td>\n",
       "      <td>0.918143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>D1256</td>\n",
       "      <td>0.567937</td>\n",
       "      <td>0.918080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>D1111</td>\n",
       "      <td>0.567786</td>\n",
       "      <td>0.917836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>D1194</td>\n",
       "      <td>0.567768</td>\n",
       "      <td>0.917807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>D960</td>\n",
       "      <td>0.567677</td>\n",
       "      <td>0.917661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>D82</td>\n",
       "      <td>0.567668</td>\n",
       "      <td>0.917647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>D1032</td>\n",
       "      <td>0.567659</td>\n",
       "      <td>0.917632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>D1217</td>\n",
       "      <td>0.567617</td>\n",
       "      <td>0.917563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>D958</td>\n",
       "      <td>0.567594</td>\n",
       "      <td>0.917526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>D1067</td>\n",
       "      <td>0.567519</td>\n",
       "      <td>0.917404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>D1287</td>\n",
       "      <td>0.567414</td>\n",
       "      <td>0.917236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>D998</td>\n",
       "      <td>0.567393</td>\n",
       "      <td>0.917201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>D1142</td>\n",
       "      <td>0.567364</td>\n",
       "      <td>0.917155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>D1170</td>\n",
       "      <td>0.560571</td>\n",
       "      <td>0.906174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>D1294</td>\n",
       "      <td>0.560497</td>\n",
       "      <td>0.906054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>D1146</td>\n",
       "      <td>0.560473</td>\n",
       "      <td>0.906016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>D1354</td>\n",
       "      <td>0.560411</td>\n",
       "      <td>0.905915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>D1189</td>\n",
       "      <td>0.560405</td>\n",
       "      <td>0.905905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>D1369</td>\n",
       "      <td>0.560399</td>\n",
       "      <td>0.905895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>D1014</td>\n",
       "      <td>0.560330</td>\n",
       "      <td>0.905784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>D1179</td>\n",
       "      <td>0.560270</td>\n",
       "      <td>0.905687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754</th>\n",
       "      <td>D1353</td>\n",
       "      <td>0.560257</td>\n",
       "      <td>0.905666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755</th>\n",
       "      <td>D1431</td>\n",
       "      <td>0.560249</td>\n",
       "      <td>0.905653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>D997</td>\n",
       "      <td>0.560222</td>\n",
       "      <td>0.905609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>D1290</td>\n",
       "      <td>0.560158</td>\n",
       "      <td>0.905507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>D1681</td>\n",
       "      <td>0.560145</td>\n",
       "      <td>0.905484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>D1219</td>\n",
       "      <td>0.560136</td>\n",
       "      <td>0.905470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>D1033</td>\n",
       "      <td>0.560089</td>\n",
       "      <td>0.905395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>D1415</td>\n",
       "      <td>0.559966</td>\n",
       "      <td>0.905196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>D1427</td>\n",
       "      <td>0.559792</td>\n",
       "      <td>0.904915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>D1190</td>\n",
       "      <td>0.559585</td>\n",
       "      <td>0.904580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>D1121</td>\n",
       "      <td>0.559507</td>\n",
       "      <td>0.904454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>D1286</td>\n",
       "      <td>0.559497</td>\n",
       "      <td>0.904437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>D1404</td>\n",
       "      <td>0.559465</td>\n",
       "      <td>0.904385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>D1205</td>\n",
       "      <td>0.559408</td>\n",
       "      <td>0.904293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>D1130</td>\n",
       "      <td>0.559293</td>\n",
       "      <td>0.904107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>D1154</td>\n",
       "      <td>0.559225</td>\n",
       "      <td>0.903998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>D1200</td>\n",
       "      <td>0.559185</td>\n",
       "      <td>0.903934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>D1275</td>\n",
       "      <td>0.559157</td>\n",
       "      <td>0.903888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>D1221</td>\n",
       "      <td>0.558856</td>\n",
       "      <td>0.903402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>D1095</td>\n",
       "      <td>0.558605</td>\n",
       "      <td>0.902995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>D1144</td>\n",
       "      <td>0.558090</td>\n",
       "      <td>0.902163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>D1421</td>\n",
       "      <td>0.557622</td>\n",
       "      <td>0.901407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1776 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       name     error  importance\n",
       "0       D27  0.618613    1.000000\n",
       "1       D51  0.575680    0.930598\n",
       "2     D1159  0.571298    0.923514\n",
       "3     D1049  0.570479    0.922191\n",
       "4     D1241  0.570324    0.921939\n",
       "5     D1167  0.570319    0.921931\n",
       "6     D1059  0.569944    0.921326\n",
       "7      D119  0.569884    0.921229\n",
       "8     D1100  0.568941    0.919704\n",
       "9     D1188  0.568908    0.919650\n",
       "10    D1115  0.568722    0.919350\n",
       "11    D1128  0.568721    0.919348\n",
       "12     D117  0.568668    0.919262\n",
       "13    D1271  0.568464    0.918932\n",
       "14    D1432  0.568267    0.918614\n",
       "15     D201  0.568182    0.918477\n",
       "16    D1009  0.568157    0.918437\n",
       "17    D1116  0.567976    0.918143\n",
       "18    D1256  0.567937    0.918080\n",
       "19    D1111  0.567786    0.917836\n",
       "20    D1194  0.567768    0.917807\n",
       "21     D960  0.567677    0.917661\n",
       "22      D82  0.567668    0.917647\n",
       "23    D1032  0.567659    0.917632\n",
       "24    D1217  0.567617    0.917563\n",
       "25     D958  0.567594    0.917526\n",
       "26    D1067  0.567519    0.917404\n",
       "27    D1287  0.567414    0.917236\n",
       "28     D998  0.567393    0.917201\n",
       "29    D1142  0.567364    0.917155\n",
       "...     ...       ...         ...\n",
       "1746  D1170  0.560571    0.906174\n",
       "1747  D1294  0.560497    0.906054\n",
       "1748  D1146  0.560473    0.906016\n",
       "1749  D1354  0.560411    0.905915\n",
       "1750  D1189  0.560405    0.905905\n",
       "1751  D1369  0.560399    0.905895\n",
       "1752  D1014  0.560330    0.905784\n",
       "1753  D1179  0.560270    0.905687\n",
       "1754  D1353  0.560257    0.905666\n",
       "1755  D1431  0.560249    0.905653\n",
       "1756   D997  0.560222    0.905609\n",
       "1757  D1290  0.560158    0.905507\n",
       "1758  D1681  0.560145    0.905484\n",
       "1759  D1219  0.560136    0.905470\n",
       "1760  D1033  0.560089    0.905395\n",
       "1761  D1415  0.559966    0.905196\n",
       "1762  D1427  0.559792    0.904915\n",
       "1763  D1190  0.559585    0.904580\n",
       "1764  D1121  0.559507    0.904454\n",
       "1765  D1286  0.559497    0.904437\n",
       "1766  D1404  0.559465    0.904385\n",
       "1767  D1205  0.559408    0.904293\n",
       "1768  D1130  0.559293    0.904107\n",
       "1769  D1154  0.559225    0.903998\n",
       "1770  D1200  0.559185    0.903934\n",
       "1771  D1275  0.559157    0.903888\n",
       "1772  D1221  0.558856    0.903402\n",
       "1773  D1095  0.558605    0.902995\n",
       "1774  D1144  0.558090    0.902163\n",
       "1775  D1421  0.557622    0.901407\n",
       "\n",
       "[1776 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rank the features\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = list(df_train.columns) # x+y column names\n",
    "names.remove(\"Activity\") # remove the target(y)\n",
    "rank = perturbation_rank(model, x_test, y_test, names, False)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Ensemble\n",
    "\n",
    "A neural network ensemble combines neural network predictions with other models. The exact blend of all of these models is determined by logistic regression. The following code performs this blend for a classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Model: 0 : <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x1a337c5f28>\n",
      "3375/3375 [==============================] - 0s 72us/sample - loss: 0.5925\n",
      "Fold #0: loss=0.539976977570035\n",
      "3375/3375 [==============================] - 0s 74us/sample - loss: 0.6046\n",
      "Fold #1: loss=0.546293967133579\n",
      "3375/3375 [==============================] - 0s 72us/sample - loss: 0.5886\n",
      "Fold #2: loss=0.5752999891281394\n",
      "3375/3375 [==============================] - 0s 73us/sample - loss: 0.5924\n",
      "Fold #3: loss=0.48768840328486535\n",
      "3376/3376 [==============================] - 0s 72us/sample - loss: 0.5894\n",
      "Fold #4: loss=0.5052750905790468\n",
      "3376/3376 [==============================] - 0s 75us/sample - loss: 0.6035\n",
      "Fold #5: loss=0.587056862766497\n",
      "3376/3376 [==============================] - 0s 74us/sample - loss: 0.6009\n",
      "Fold #6: loss=0.5225903315974081\n",
      "3377/3377 [==============================] - 0s 73us/sample - loss: 0.5964\n",
      "Fold #7: loss=0.5367141641000998\n",
      "3377/3377 [==============================] - 0s 73us/sample - loss: 0.6114\n",
      "Fold #8: loss=0.5436581046397939\n",
      "3377/3377 [==============================] - 0s 74us/sample - loss: 0.5970\n",
      "Fold #9: loss=0.5408651016235858\n",
      "KerasClassifier: Mean loss=0.538541899242305\n",
      "Model: 1 : KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "Fold #0: loss=3.606678388314123\n",
      "Fold #1: loss=2.2197228940978317\n",
      "Fold #2: loss=3.6717523663107237\n",
      "Fold #3: loss=2.5045156203944594\n",
      "Fold #4: loss=4.443553550438037\n",
      "Fold #5: loss=4.410524301688227\n",
      "Fold #6: loss=3.400455469543658\n",
      "Fold #7: loss=3.0885474338547683\n",
      "Fold #8: loss=2.1219335323249253\n",
      "Fold #9: loss=3.0613772690497245\n",
      "KNeighborsClassifier: Mean loss=3.2529060826016476\n",
      "Model: 2 : RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fold #0: loss=0.45367647860510457\n",
      "Fold #1: loss=0.4291285133546661\n",
      "Fold #2: loss=0.47807823984566766\n",
      "Fold #3: loss=0.4297027376520038\n",
      "Fold #4: loss=0.47754586833400503\n",
      "Fold #5: loss=0.49680256750180235\n",
      "Fold #6: loss=0.40838845898704956\n",
      "Fold #7: loss=0.4588990265492762\n",
      "Fold #8: loss=0.44721528915044234\n",
      "Fold #9: loss=0.4663937747948781\n",
      "RandomForestClassifier: Mean loss=0.4545830954774896\n",
      "Model: 3 : RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fold #0: loss=0.4566961345594908\n",
      "Fold #1: loss=0.42066496699999656\n",
      "Fold #2: loss=0.5538934062334627\n",
      "Fold #3: loss=0.422674022246857\n",
      "Fold #4: loss=0.4673662326813632\n",
      "Fold #5: loss=0.4840021406592415\n",
      "Fold #6: loss=0.40385621574238284\n",
      "Fold #7: loss=0.464356660292751\n",
      "Fold #8: loss=0.44704938343474143\n",
      "Fold #9: loss=0.46845783082653614\n",
      "RandomForestClassifier: Mean loss=0.4589016993676823\n",
      "Model: 4 : ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Fold #0: loss=0.4527675035067471\n",
      "Fold #1: loss=0.4110305935734117\n",
      "Fold #2: loss=0.5888803130359166\n",
      "Fold #3: loss=0.4306328240093986\n",
      "Fold #4: loss=0.48225861790742525\n",
      "Fold #5: loss=0.4948091627409796\n",
      "Fold #6: loss=0.5012859393937282\n",
      "Fold #7: loss=0.6532547466122728\n",
      "Fold #8: loss=0.45542964992222223\n",
      "Fold #9: loss=0.46367562685959457\n",
      "ExtraTreesClassifier: Mean loss=0.4934024977561696\n",
      "Model: 5 : ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Fold #0: loss=0.4613198816136302\n",
      "Fold #1: loss=0.4176531827411692\n",
      "Fold #2: loss=0.5027669500720876\n",
      "Fold #3: loss=0.4283207247463087\n",
      "Fold #4: loss=0.4881904944765979\n",
      "Fold #5: loss=0.5698193040126477\n",
      "Fold #6: loss=0.49206225346038845\n",
      "Fold #7: loss=0.6452376812723384\n",
      "Fold #8: loss=0.5280402633754961\n",
      "Fold #9: loss=0.5501752319299249\n",
      "ExtraTreesClassifier: Mean loss=0.5083585967700589\n",
      "Model: 6 : GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.05, loss='deviance', max_depth=6,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "              n_iter_no_change=None, presort='auto', random_state=None,\n",
      "              subsample=0.5, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)\n",
      "Fold #0: loss=0.49791198572991924\n",
      "Fold #1: loss=0.4579863943334928\n",
      "Fold #2: loss=0.4805922791532558\n",
      "Fold #3: loss=0.4531618329064234\n",
      "Fold #4: loss=0.4987153653806443\n",
      "Fold #5: loss=0.49104909520481993\n",
      "Fold #6: loss=0.44836607002257406\n",
      "Fold #7: loss=0.45681922814619896\n",
      "Fold #8: loss=0.45792139879070043\n",
      "Fold #9: loss=0.47836253203362916\n",
      "GradientBoostingClassifier: Mean loss=0.47208861817016584\n",
      "\n",
      "Blending models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jheaton/miniconda3/envs/tensorflow-2.0/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "PATH = \"./data/\"\n",
    "SHUFFLE = False\n",
    "FOLDS = 10\n",
    "\n",
    "def build_ann(input_size,classes,neurons):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=input_size, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Dense(classes,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def mlogloss(y_test, preds):\n",
    "    epsilon = 1e-15\n",
    "    sum = 0\n",
    "    for row in zip(preds,y_test):\n",
    "        x = row[0][row[1]]\n",
    "        x = max(epsilon,x)\n",
    "        x = min(1-epsilon,x)\n",
    "        sum+=math.log(x)\n",
    "    return( (-1/len(preds))*sum)\n",
    "\n",
    "def stretch(y):\n",
    "    return (y - y.min()) / (y.max() - y.min())\n",
    "\n",
    "\n",
    "def blend_ensemble(x, y, x_submit):\n",
    "    kf = StratifiedKFold(FOLDS)\n",
    "    folds = list(kf.split(x,y))\n",
    "\n",
    "    models = [\n",
    "        KerasClassifier(build_fn=build_ann,neurons=20,input_size=x.shape[1],classes=2),\n",
    "        KNeighborsClassifier(n_neighbors=3),\n",
    "        RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "        RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50)]\n",
    "\n",
    "    dataset_blend_train = np.zeros((x.shape[0], len(models)))\n",
    "    dataset_blend_test = np.zeros((x_submit.shape[0], len(models)))\n",
    "\n",
    "    for j, model in enumerate(models):\n",
    "        print(\"Model: {} : {}\".format(j, model) )\n",
    "        fold_sums = np.zeros((x_submit.shape[0], len(folds)))\n",
    "        total_loss = 0\n",
    "        for i, (train, test) in enumerate(folds):\n",
    "            x_train = x[train]\n",
    "            y_train = y[train]\n",
    "            x_test = x[test]\n",
    "            y_test = y[test]\n",
    "            model.fit(x_train, y_train)\n",
    "            pred = np.array(model.predict_proba(x_test))\n",
    "            # pred = model.predict_proba(x_test)\n",
    "            dataset_blend_train[test, j] = pred[:, 1]\n",
    "            pred2 = np.array(model.predict_proba(x_submit))\n",
    "            #fold_sums[:, i] = model.predict_proba(x_submit)[:, 1]\n",
    "            fold_sums[:, i] = pred2[:, 1]\n",
    "            loss = mlogloss(y_test, pred)\n",
    "            total_loss+=loss\n",
    "            print(\"Fold #{}: loss={}\".format(i,loss))\n",
    "        print(\"{}: Mean loss={}\".format(model.__class__.__name__,total_loss/len(folds)))\n",
    "        dataset_blend_test[:, j] = fold_sums.mean(1)\n",
    "\n",
    "    print()\n",
    "    print(\"Blending models.\")\n",
    "    blend = LogisticRegression(solver='lbfgs')\n",
    "    blend.fit(dataset_blend_train, y)\n",
    "    return blend.predict_proba(dataset_blend_test)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    np.random.seed(42)  # seed to shuffle the train set\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    filename_train = os.path.join(PATH, \"bio_train.csv\")\n",
    "    df_train = pd.read_csv(filename_train, na_values=['NA', '?'])\n",
    "\n",
    "    filename_submit = os.path.join(PATH, \"bio_test.csv\")\n",
    "    df_submit = pd.read_csv(filename_submit, na_values=['NA', '?'])\n",
    "\n",
    "    predictors = list(df_train.columns.values)\n",
    "    predictors.remove('Activity')\n",
    "    x = df_train[predictors].values\n",
    "    y = df_train['Activity']\n",
    "    x_submit = df_submit.values\n",
    "\n",
    "    if SHUFFLE:\n",
    "        idx = np.random.permutation(y.size)\n",
    "        x = x[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    submit_data = blend_ensemble(x, y, x_submit)\n",
    "    submit_data = stretch(submit_data)\n",
    "\n",
    "    ####################\n",
    "    # Build submit file\n",
    "    ####################\n",
    "    ids = [id+1 for id in range(submit_data.shape[0])]\n",
    "    submit_filename = os.path.join(PATH, \"bio_submit.csv\")\n",
    "    submit_df = pd.DataFrame({'MoleculeId': ids, 'PredictedProbability': submit_data[:, 1]},\n",
    "                             columns=['MoleculeId','PredictedProbability'])\n",
    "    submit_df.to_csv(submit_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and Input Perturbation Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8.3: How Should you Architect Your Keras Neural Network: Hyperparameters\n",
    "\n",
    "* [Guide to choosing Hyperparameters for your Neural Networks](https://towardsdatascience.com/guide-to-choosing-hyperparameters-for-your-neural-networks-38244e87dafe)\n",
    "\n",
    "### Number of Hidden Layers and Neuron Counts\n",
    "### Activation Functions\n",
    "### Regularization: L1, L2, Dropout\n",
    "### Batch Normalization\n",
    "### Training Parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Read the data set\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Generate dummies for job\n",
    "df = pd.concat([df,pd.get_dummies(df['job'],prefix=\"job\")],axis=1)\n",
    "df.drop('job', axis=1, inplace=True)\n",
    "\n",
    "# Generate dummies for area\n",
    "df = pd.concat([df,pd.get_dummies(df['area'],prefix=\"area\")],axis=1)\n",
    "df.drop('area', axis=1, inplace=True)\n",
    "\n",
    "# Missing values for income\n",
    "med = df['income'].median()\n",
    "df['income'] = df['income'].fillna(med)\n",
    "\n",
    "# Standardize ranges\n",
    "df['income'] = zscore(df['income'])\n",
    "df['aspect'] = zscore(df['aspect'])\n",
    "df['save_rate'] = zscore(df['save_rate'])\n",
    "df['age'] = zscore(df['age'])\n",
    "df['subscriptions'] = zscore(df['subscriptions'])\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "x_columns = df.columns.drop('product').drop('id')\n",
    "x = df[x_columns].values\n",
    "dummies = pd.get_dummies(df['product']) # Classification\n",
    "products = dummies.columns\n",
    "y = dummies.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6821130685589742\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow.keras.initializers\n",
    "import statistics\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tensorflow.keras.layers import LeakyReLU,PReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_network(dropout,lr,neuronPct,neuronShrink):\n",
    "    SPLITS = 2\n",
    "\n",
    "    # Bootstrap\n",
    "    boot = StratifiedShuffleSplit(n_splits=SPLITS, test_size=0.1)\n",
    "\n",
    "    # Track progress\n",
    "    mean_benchmark = []\n",
    "    epochs_needed = []\n",
    "    num = 0\n",
    "    neuronCount = int(neuronPct * 5000)\n",
    "\n",
    "    # Loop through samples\n",
    "    for train, test in boot.split(x,df['product']):\n",
    "        start_time = time.time()\n",
    "        num+=1\n",
    "\n",
    "        # Split train and test\n",
    "        x_train = x[train]\n",
    "        y_train = y[train]\n",
    "        x_test = x[test]\n",
    "        y_test = y[test]\n",
    "\n",
    "        # Construct neural network\n",
    "        # kernel_initializer = tensorflow.keras.initializers.he_uniform(seed=None)\n",
    "        model = Sequential()\n",
    "        \n",
    "        layer = 0\n",
    "        while neuronCount>25 and layer<10:\n",
    "            #print(neuronCount)\n",
    "            if layer==0:\n",
    "                model.add(Dense(neuronCount, \n",
    "                    input_dim=x.shape[1], \n",
    "                    activation=PReLU())) \n",
    "            else:\n",
    "                model.add(Dense(neuronCount, activation=PReLU())) \n",
    "            model.add(Dropout(dropout))\n",
    "        \n",
    "            neuronCount = neuronCount * neuronShrink\n",
    "        \n",
    "        model.add(Dense(y.shape[1],activation='softmax')) # Output\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=lr))\n",
    "        monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
    "            patience=100, verbose=0, mode='auto', restore_best_weights=True)\n",
    "\n",
    "        # Train on the bootstrap sample\n",
    "        model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=0,epochs=1000)\n",
    "        epochs = monitor.stopped_epoch\n",
    "        epochs_needed.append(epochs)\n",
    "\n",
    "        # Predict on the out of boot (validation)\n",
    "        pred = model.predict(x_test)\n",
    "\n",
    "        # Measure this bootstrap's log loss\n",
    "        y_compare = np.argmax(y_test,axis=1) # For log loss calculation\n",
    "        score = metrics.log_loss(y_compare, pred)\n",
    "        mean_benchmark.append(score)\n",
    "        m1 = statistics.mean(mean_benchmark)\n",
    "        m2 = statistics.mean(epochs_needed)\n",
    "        mdev = statistics.pstdev(mean_benchmark)\n",
    "\n",
    "        # Record this iteration\n",
    "        time_took = time.time() - start_time\n",
    "        #print(f\"#{num}: score={score:.6f}, mean score={m1:.6f}, stdev={mdev:.6f}, epochs={epochs}, mean epochs={int(m2)}, time={hms_string(time_took)}\")\n",
    "\n",
    "    return (-m1)\n",
    "    # https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404\n",
    "\n",
    "print(evaluate_network(\n",
    "    dropout=0.2,\n",
    "    lr=1e-3,\n",
    "    neuronPct=0.2,\n",
    "    neuronShrink=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8.4: Bayesian Hyperparameter Optimization for Keras\n",
    "\n",
    "Snoek, J., Larochelle, H., & Adams, R. P. (2012). [Practical bayesian optimization of machine learning algorithms](https://arxiv.org/pdf/1206.2944.pdf). In *Advances in neural information processing systems* (pp. 2951-2959).\n",
    "\n",
    "\n",
    "* [bayesian-optimization](https://github.com/fmfn/BayesianOptimization)\n",
    "* [hyperopt](https://github.com/hyperopt/hyperopt)\n",
    "* [spearmint](https://github.com/JasperSnoek/spearmint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |  dropout  |    lr     | neuronPct | neuron... |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.7325  \u001b[0m | \u001b[0m 0.2081  \u001b[0m | \u001b[0m 0.07203 \u001b[0m | \u001b[0m 0.01011 \u001b[0m | \u001b[0m 0.3093  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-0.7113  \u001b[0m | \u001b[95m 0.07323 \u001b[0m | \u001b[95m 0.009234\u001b[0m | \u001b[95m 0.1944  \u001b[0m | \u001b[95m 0.3521  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-9.384   \u001b[0m | \u001b[0m 0.198   \u001b[0m | \u001b[0m 0.05388 \u001b[0m | \u001b[0m 0.425   \u001b[0m | \u001b[0m 0.6884  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-9.389   \u001b[0m | \u001b[0m 0.102   \u001b[0m | \u001b[0m 0.08781 \u001b[0m | \u001b[0m 0.03711 \u001b[0m | \u001b[0m 0.6738  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-11.25   \u001b[0m | \u001b[0m 0.2082  \u001b[0m | \u001b[0m 0.05587 \u001b[0m | \u001b[0m 0.149   \u001b[0m | \u001b[0m 0.2061  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-9.38    \u001b[0m | \u001b[0m 0.3996  \u001b[0m | \u001b[0m 0.09683 \u001b[0m | \u001b[0m 0.3203  \u001b[0m | \u001b[0m 0.6954  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-11.25   \u001b[0m | \u001b[0m 0.4373  \u001b[0m | \u001b[0m 0.08946 \u001b[0m | \u001b[0m 0.09419 \u001b[0m | \u001b[0m 0.04866 \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-9.384   \u001b[0m | \u001b[0m 0.08475 \u001b[0m | \u001b[0m 0.08781 \u001b[0m | \u001b[0m 0.1074  \u001b[0m | \u001b[0m 0.4269  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-9.361   \u001b[0m | \u001b[0m 0.478   \u001b[0m | \u001b[0m 0.05332 \u001b[0m | \u001b[0m 0.695   \u001b[0m | \u001b[0m 0.3224  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-11.28   \u001b[0m | \u001b[0m 0.3426  \u001b[0m | \u001b[0m 0.08346 \u001b[0m | \u001b[0m 0.02811 \u001b[0m | \u001b[0m 0.7526  \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jheaton\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:1224: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current - self.min_delta, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-9.343   \u001b[0m | \u001b[0m 0.4934  \u001b[0m | \u001b[0m 0.07482 \u001b[0m | \u001b[0m 0.2876  \u001b[0m | \u001b[0m 0.7914  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-9.366   \u001b[0m | \u001b[0m 0.05151 \u001b[0m | \u001b[0m 0.04479 \u001b[0m | \u001b[0m 0.9095  \u001b[0m | \u001b[0m 0.3007  \u001b[0m |\n",
      "| \u001b[95m 13      \u001b[0m | \u001b[95m-0.6943  \u001b[0m | \u001b[95m 0.1436  \u001b[0m | \u001b[95m 0.013   \u001b[0m | \u001b[95m 0.02917 \u001b[0m | \u001b[95m 0.682   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-9.335   \u001b[0m | \u001b[0m 0.1056  \u001b[0m | \u001b[0m 0.02655 \u001b[0m | \u001b[0m 0.4967  \u001b[0m | \u001b[0m 0.06283 \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-11.24   \u001b[0m | \u001b[0m 0.2865  \u001b[0m | \u001b[0m 0.01467 \u001b[0m | \u001b[0m 0.5934  \u001b[0m | \u001b[0m 0.7028  \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-9.337   \u001b[0m | \u001b[0m 0.05106 \u001b[0m | \u001b[0m 0.04141 \u001b[0m | \u001b[0m 0.6975  \u001b[0m | \u001b[0m 0.42    \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-9.362   \u001b[0m | \u001b[0m 0.02493 \u001b[0m | \u001b[0m 0.05359 \u001b[0m | \u001b[0m 0.6672  \u001b[0m | \u001b[0m 0.5197  \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-9.358   \u001b[0m | \u001b[0m 0.4714  \u001b[0m | \u001b[0m 0.05866 \u001b[0m | \u001b[0m 0.9044  \u001b[0m | \u001b[0m 0.1461  \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-11.26   \u001b[0m | \u001b[0m 0.0695  \u001b[0m | \u001b[0m 0.08074 \u001b[0m | \u001b[0m 0.4037  \u001b[0m | \u001b[0m 0.1737  \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-11.26   \u001b[0m | \u001b[0m 0.4628  \u001b[0m | \u001b[0m 0.03478 \u001b[0m | \u001b[0m 0.7533  \u001b[0m | \u001b[0m 0.7287  \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-11.23   \u001b[0m | \u001b[0m 0.1799  \u001b[0m | \u001b[0m 0.07174 \u001b[0m | \u001b[0m 0.4539  \u001b[0m | \u001b[0m 0.6872  \u001b[0m |\n",
      "| \u001b[95m 22      \u001b[0m | \u001b[95m-0.6763  \u001b[0m | \u001b[95m 0.1463  \u001b[0m | \u001b[95m 0.008201\u001b[0m | \u001b[95m 0.02866 \u001b[0m | \u001b[95m 0.6826  \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-0.712   \u001b[0m | \u001b[0m 0.1388  \u001b[0m | \u001b[0m 0.006505\u001b[0m | \u001b[0m 0.0294  \u001b[0m | \u001b[0m 0.6795  \u001b[0m |\n",
      "| \u001b[95m 24      \u001b[0m | \u001b[95m-0.6753  \u001b[0m | \u001b[95m 0.1401  \u001b[0m | \u001b[95m 0.008654\u001b[0m | \u001b[95m 0.02142 \u001b[0m | \u001b[95m 0.6856  \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-0.718   \u001b[0m | \u001b[0m 0.1392  \u001b[0m | \u001b[0m 0.007567\u001b[0m | \u001b[0m 0.03131 \u001b[0m | \u001b[0m 0.6911  \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-0.7072  \u001b[0m | \u001b[0m 0.07152 \u001b[0m | \u001b[0m 0.002432\u001b[0m | \u001b[0m 0.2016  \u001b[0m | \u001b[0m 0.3469  \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-4.261   \u001b[0m | \u001b[0m 0.07106 \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.1886  \u001b[0m | \u001b[0m 0.3442  \u001b[0m |\n",
      "| \u001b[95m 28      \u001b[0m | \u001b[95m-0.6662  \u001b[0m | \u001b[95m 0.0732  \u001b[0m | \u001b[95m 0.009476\u001b[0m | \u001b[95m 0.204   \u001b[0m | \u001b[95m 0.3528  \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-0.7199  \u001b[0m | \u001b[0m 0.06879 \u001b[0m | \u001b[0m 0.005844\u001b[0m | \u001b[0m 0.2001  \u001b[0m | \u001b[0m 0.3535  \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m-0.7105  \u001b[0m | \u001b[0m 0.07674 \u001b[0m | \u001b[0m 0.004627\u001b[0m | \u001b[0m 0.2002  \u001b[0m | \u001b[0m 0.3527  \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-0.6948  \u001b[0m | \u001b[0m 0.073   \u001b[0m | \u001b[0m 0.009666\u001b[0m | \u001b[0m 0.2003  \u001b[0m | \u001b[0m 0.3474  \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m-0.6943  \u001b[0m | \u001b[0m 0.1401  \u001b[0m | \u001b[0m 0.008614\u001b[0m | \u001b[0m 0.02801 \u001b[0m | \u001b[0m 0.6856  \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-0.6759  \u001b[0m | \u001b[0m 0.1434  \u001b[0m | \u001b[0m 0.009636\u001b[0m | \u001b[0m 0.02197 \u001b[0m | \u001b[0m 0.6764  \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m-0.7198  \u001b[0m | \u001b[0m 0.1454  \u001b[0m | \u001b[0m 0.01032 \u001b[0m | \u001b[0m 0.02603 \u001b[0m | \u001b[0m 0.6938  \u001b[0m |\n",
      "| \u001b[95m 35      \u001b[0m | \u001b[95m-0.6628  \u001b[0m | \u001b[95m 0.1428  \u001b[0m | \u001b[95m 0.001356\u001b[0m | \u001b[95m 0.02556 \u001b[0m | \u001b[95m 0.6929  \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-0.7053  \u001b[0m | \u001b[0m 0.1425  \u001b[0m | \u001b[0m 0.000230\u001b[0m | \u001b[0m 0.02131 \u001b[0m | \u001b[0m 0.6811  \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m-0.69    \u001b[0m | \u001b[0m 0.1487  \u001b[0m | \u001b[0m 0.005591\u001b[0m | \u001b[0m 0.01808 \u001b[0m | \u001b[0m 0.6867  \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m-0.7174  \u001b[0m | \u001b[0m 0.07181 \u001b[0m | \u001b[0m 0.002015\u001b[0m | \u001b[0m 0.2097  \u001b[0m | \u001b[0m 0.3478  \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m-9.382   \u001b[0m | \u001b[0m 0.0751  \u001b[0m | \u001b[0m 0.01531 \u001b[0m | \u001b[0m 0.1953  \u001b[0m | \u001b[0m 0.3579  \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m-0.7024  \u001b[0m | \u001b[0m 0.1448  \u001b[0m | \u001b[0m 0.004864\u001b[0m | \u001b[0m 0.02343 \u001b[0m | \u001b[0m 0.6869  \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m-0.7179  \u001b[0m | \u001b[0m 0.07256 \u001b[0m | \u001b[0m 0.005286\u001b[0m | \u001b[0m 0.2044  \u001b[0m | \u001b[0m 0.3504  \u001b[0m |\n",
      "| \u001b[95m 42      \u001b[0m | \u001b[95m-0.6124  \u001b[0m | \u001b[95m 0.1392  \u001b[0m | \u001b[95m 0.002035\u001b[0m | \u001b[95m 0.02355 \u001b[0m | \u001b[95m 0.6707  \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m-0.6683  \u001b[0m | \u001b[0m 0.07257 \u001b[0m | \u001b[0m 0.00583 \u001b[0m | \u001b[0m 0.1967  \u001b[0m | \u001b[0m 0.3502  \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m-0.6661  \u001b[0m | \u001b[0m 0.1412  \u001b[0m | \u001b[0m 0.004371\u001b[0m | \u001b[0m 0.02413 \u001b[0m | \u001b[0m 0.677   \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m-0.7942  \u001b[0m | \u001b[0m 0.06888 \u001b[0m | \u001b[0m 0.004445\u001b[0m | \u001b[0m 0.2061  \u001b[0m | \u001b[0m 0.346   \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m-0.665   \u001b[0m | \u001b[0m 0.141   \u001b[0m | \u001b[0m 0.000586\u001b[0m | \u001b[0m 0.02047 \u001b[0m | \u001b[0m 0.6894  \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m-2.846   \u001b[0m | \u001b[0m 0.06934 \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.2058  \u001b[0m | \u001b[0m 0.3486  \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m-0.7025  \u001b[0m | \u001b[0m 0.06641 \u001b[0m | \u001b[0m 0.01247 \u001b[0m | \u001b[0m 0.1987  \u001b[0m | \u001b[0m 0.3408  \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m-0.7779  \u001b[0m | \u001b[0m 0.2082  \u001b[0m | \u001b[0m 0.0751  \u001b[0m | \u001b[0m 0.01003 \u001b[0m | \u001b[0m 0.3113  \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m-0.8038  \u001b[0m | \u001b[0m 0.2103  \u001b[0m | \u001b[0m 0.0718  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.3127  \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m-0.7649  \u001b[0m | \u001b[0m 0.1452  \u001b[0m | \u001b[0m 0.01018 \u001b[0m | \u001b[0m 0.02285 \u001b[0m | \u001b[0m 0.6824  \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m-0.7447  \u001b[0m | \u001b[0m 0.2059  \u001b[0m | \u001b[0m 0.07174 \u001b[0m | \u001b[0m 0.01001 \u001b[0m | \u001b[0m 0.3129  \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m-0.6854  \u001b[0m | \u001b[0m 0.1406  \u001b[0m | \u001b[0m 0.006628\u001b[0m | \u001b[0m 0.02458 \u001b[0m | \u001b[0m 0.6922  \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m-9.381   \u001b[0m | \u001b[0m 0.2019  \u001b[0m | \u001b[0m 0.08034 \u001b[0m | \u001b[0m 0.02351 \u001b[0m | \u001b[0m 0.3124  \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m-0.7364  \u001b[0m | \u001b[0m 0.06729 \u001b[0m | \u001b[0m 0.008855\u001b[0m | \u001b[0m 0.1999  \u001b[0m | \u001b[0m 0.3465  \u001b[0m |\n",
      "| \u001b[0m 56      \u001b[0m | \u001b[0m-0.6632  \u001b[0m | \u001b[0m 0.1419  \u001b[0m | \u001b[0m 0.009637\u001b[0m | \u001b[0m 0.02654 \u001b[0m | \u001b[0m 0.6792  \u001b[0m |\n",
      "| \u001b[0m 57      \u001b[0m | \u001b[0m-0.7014  \u001b[0m | \u001b[0m 0.1444  \u001b[0m | \u001b[0m 0.007293\u001b[0m | \u001b[0m 0.02954 \u001b[0m | \u001b[0m 0.6904  \u001b[0m |\n",
      "| \u001b[0m 58      \u001b[0m | \u001b[0m-0.6882  \u001b[0m | \u001b[0m 0.07762 \u001b[0m | \u001b[0m 0.006598\u001b[0m | \u001b[0m 0.2075  \u001b[0m | \u001b[0m 0.3394  \u001b[0m |\n",
      "| \u001b[0m 59      \u001b[0m | \u001b[0m-0.6882  \u001b[0m | \u001b[0m 0.1382  \u001b[0m | \u001b[0m 0.002685\u001b[0m | \u001b[0m 0.02563 \u001b[0m | \u001b[0m 0.675   \u001b[0m |\n",
      "| \u001b[0m 60      \u001b[0m | \u001b[0m-0.6571  \u001b[0m | \u001b[0m 0.06219 \u001b[0m | \u001b[0m 0.0117  \u001b[0m | \u001b[0m 0.1982  \u001b[0m | \u001b[0m 0.3446  \u001b[0m |\n",
      "| \u001b[0m 61      \u001b[0m | \u001b[0m-0.7055  \u001b[0m | \u001b[0m 0.1423  \u001b[0m | \u001b[0m 0.002345\u001b[0m | \u001b[0m 0.03659 \u001b[0m | \u001b[0m 0.6741  \u001b[0m |\n",
      "| \u001b[0m 62      \u001b[0m | \u001b[0m-0.8123  \u001b[0m | \u001b[0m 0.2047  \u001b[0m | \u001b[0m 0.07329 \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.3097  \u001b[0m |\n",
      "| \u001b[0m 63      \u001b[0m | \u001b[0m-0.6706  \u001b[0m | \u001b[0m 0.07476 \u001b[0m | \u001b[0m 0.005456\u001b[0m | \u001b[0m 0.2075  \u001b[0m | \u001b[0m 0.3446  \u001b[0m |\n",
      "| \u001b[0m 64      \u001b[0m | \u001b[0m-0.7178  \u001b[0m | \u001b[0m 0.1433  \u001b[0m | \u001b[0m 0.004861\u001b[0m | \u001b[0m 0.01754 \u001b[0m | \u001b[0m 0.6831  \u001b[0m |\n",
      "| \u001b[0m 65      \u001b[0m | \u001b[0m-0.7406  \u001b[0m | \u001b[0m 0.1302  \u001b[0m | \u001b[0m 0.004853\u001b[0m | \u001b[0m 0.03341 \u001b[0m | \u001b[0m 0.674   \u001b[0m |\n",
      "| \u001b[0m 66      \u001b[0m | \u001b[0m-0.6938  \u001b[0m | \u001b[0m 0.1378  \u001b[0m | \u001b[0m 0.004844\u001b[0m | \u001b[0m 0.0321  \u001b[0m | \u001b[0m 0.6722  \u001b[0m |\n",
      "| \u001b[0m 67      \u001b[0m | \u001b[0m-0.7554  \u001b[0m | \u001b[0m 0.1257  \u001b[0m | \u001b[0m 0.008696\u001b[0m | \u001b[0m 0.02735 \u001b[0m | \u001b[0m 0.6881  \u001b[0m |\n",
      "| \u001b[0m 68      \u001b[0m | \u001b[0m-0.7404  \u001b[0m | \u001b[0m 0.07945 \u001b[0m | \u001b[0m 0.006459\u001b[0m | \u001b[0m 0.2003  \u001b[0m | \u001b[0m 0.3374  \u001b[0m |\n",
      "| \u001b[0m 69      \u001b[0m | \u001b[0m-0.6426  \u001b[0m | \u001b[0m 0.05501 \u001b[0m | \u001b[0m 0.005053\u001b[0m | \u001b[0m 0.1983  \u001b[0m | \u001b[0m 0.3452  \u001b[0m |\n",
      "| \u001b[0m 70      \u001b[0m | \u001b[0m-0.6579  \u001b[0m | \u001b[0m 0.1304  \u001b[0m | \u001b[0m 0.005302\u001b[0m | \u001b[0m 0.02916 \u001b[0m | \u001b[0m 0.6776  \u001b[0m |\n",
      "| \u001b[0m 71      \u001b[0m | \u001b[0m-0.6954  \u001b[0m | \u001b[0m 0.1445  \u001b[0m | \u001b[0m 0.003732\u001b[0m | \u001b[0m 0.02965 \u001b[0m | \u001b[0m 0.6738  \u001b[0m |\n",
      "| \u001b[0m 72      \u001b[0m | \u001b[0m-0.6489  \u001b[0m | \u001b[0m 0.1372  \u001b[0m | \u001b[0m 0.002402\u001b[0m | \u001b[0m 0.01315 \u001b[0m | \u001b[0m 0.6897  \u001b[0m |\n",
      "| \u001b[0m 73      \u001b[0m | \u001b[0m-0.7322  \u001b[0m | \u001b[0m 0.1283  \u001b[0m | \u001b[0m 0.002487\u001b[0m | \u001b[0m 0.01611 \u001b[0m | \u001b[0m 0.689   \u001b[0m |\n",
      "| \u001b[0m 74      \u001b[0m | \u001b[0m-0.7302  \u001b[0m | \u001b[0m 0.2054  \u001b[0m | \u001b[0m 0.06687 \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.3101  \u001b[0m |\n",
      "| \u001b[0m 75      \u001b[0m | \u001b[0m-0.6448  \u001b[0m | \u001b[0m 0.1355  \u001b[0m | \u001b[0m 0.001251\u001b[0m | \u001b[0m 0.03385 \u001b[0m | \u001b[0m 0.6781  \u001b[0m |\n",
      "| \u001b[0m 76      \u001b[0m | \u001b[0m-0.7575  \u001b[0m | \u001b[0m 0.07229 \u001b[0m | \u001b[0m 0.005755\u001b[0m | \u001b[0m 0.2023  \u001b[0m | \u001b[0m 0.3397  \u001b[0m |\n",
      "| \u001b[0m 77      \u001b[0m | \u001b[0m-0.6865  \u001b[0m | \u001b[0m 0.1358  \u001b[0m | \u001b[0m 0.004864\u001b[0m | \u001b[0m 0.03822 \u001b[0m | \u001b[0m 0.6812  \u001b[0m |\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = {'dropout': (0.0, 0.499),\n",
    "           'lr': (0.0, 0.1),\n",
    "           'neuronPct': (0.01, 1),\n",
    "           'neuronShrink': (0.01, 1)\n",
    "          }\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=evaluate_network,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=10, n_iter=1000,)\n",
    "\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 8.5: Current Semester's Kaggle\n",
    "\n",
    "Kaggke competition site for current semester (Fall 2019):\n",
    "\n",
    "* Coming soon\n",
    "\n",
    "Previous Kaggle competition sites for this class (NOT this semester's assignment, feel free to use code):\n",
    "* [Spring 2019 Kaggle Assignment](https://www.kaggle.com/c/applications-of-deep-learningwustl-spring-2019)\n",
    "* [Fall 2018 Kaggle Assignment](https://www.kaggle.com/c/wustl-t81-558-washu-deep-learning-fall-2018)\n",
    "* [Spring 2018 Kaggle Assignment](https://www.kaggle.com/c/wustl-t81-558-washu-deep-learning-spring-2018)\n",
    "* [Fall 2017 Kaggle Assignment](https://www.kaggle.com/c/wustl-t81-558-washu-deep-learning-fall-2017)\n",
    "* [Spring 2017 Kaggle Assignment](https://inclass.kaggle.com/c/applications-of-deep-learning-wustl-spring-2017)\n",
    "* [Fall 2016 Kaggle Assignment](https://inclass.kaggle.com/c/wustl-t81-558-washu-deep-learning-fall-2016)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Module 8 Assignment\n",
    "\n",
    "You can find the first assignment here: [assignment 8](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class8.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow-2.0)",
   "language": "python",
   "name": "tensorflow-2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
