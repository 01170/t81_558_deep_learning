{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 8: Kaggle Data Sets**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Video Material\n",
    "\n",
    "Main video lecture:\n",
    "\n",
    "* [Part 8.1: Introduction to Kaggle](https://www.youtube.com/watch?v=XpGI4engRjQ&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN&index=24)\n",
    "* [Part 8.2: Building Ensembles with Scikit-Learn and Keras](https://www.youtube.com/watch?v=AA3KFxjPxCo&index=25&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN)\n",
    "* [Part 8.3: How Should you Architect Your Keras Neural Network: Hyperparameters](https://www.youtube.com/watch?v=GaKo-9c532c)\n",
    "* [Part 8.4: Bayesian Hyperparameter Optimization for Keras](https://www.youtube.com/watch?v=GaKo-9c532c)\n",
    "* [Part 8.5: Current Semester's Kaggle](https://www.youtube.com/watch?v=GaKo-9c532c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 8.1: Introduction to Kaggle\n",
    "\n",
    "[Kaggle](http://www.kaggle.com) runs competitions in which data scientists compete in order to provide the best model to fit the data. The capstone project of this chapter features Kaggle’s [Titanic data set](https://www.kaggle.com/c/titanic-gettingStarted). Before we get started with the Titanic example, it’s important to be aware of some Kaggle guidelines. First, most competitions end on a specific date. Website organizers have currently scheduled the Titanic competition to end on December 31, 2016. However, they have already extended the deadline several times, and an extension beyond 2014 is also possible. Second, the Titanic data set is considered a tutorial data set. In other words, there is no prize, and your score in the competition does not count towards becoming a Kaggle Master. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Ranks\n",
    "\n",
    "Kaggle ranks are achieved by earning gold, silver and bronze medals.\n",
    "\n",
    "* [Kaggle Top Users](https://www.kaggle.com/rankings)\n",
    "* [Current Top Kaggle User's Profile Page](https://www.kaggle.com/stasg7)\n",
    "* [Jeff Heaton's (your instructor) Kaggle Profile](https://www.kaggle.com/jeffheaton)\n",
    "* [Current Kaggle Ranking System](https://www.kaggle.com/progression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical Kaggle Competition\n",
    "\n",
    "A typical Kaggle competition will have several components.  Consider the Titanic tutorial:\n",
    "\n",
    "* [Competition Summary Page](https://www.kaggle.com/c/titanic)\n",
    "* [Data Page](https://www.kaggle.com/c/titanic/data)\n",
    "* [Evaluation Description Page](https://www.kaggle.com/c/titanic/details/evaluation)\n",
    "* [Leaderboard](https://www.kaggle.com/c/titanic/leaderboard)\n",
    "\n",
    "### How Kaggle Competitions are Scored\n",
    "\n",
    "Kaggle is provided with a data set by the competition sponsor.  This data set is divided up as follows:\n",
    "\n",
    "* **Complete Data Set** - This is the complete data set.\n",
    "    * **Training Data Set** - You are provided both the inputs and the outcomes for the training portion of the data set.\n",
    "    * **Test Data Set** - You are provided the complete test data set; however, you are not given the outcomes.  Your submission is  your predicted outcomes for this data set.\n",
    "        * **Public Leaderboard** - You are not told what part of the test data set contributes to the public leaderboard.  Your public score is calculated based on this part of the data set.\n",
    "        * **Private Leaderboard** - You are not told what part of the test data set contributes to the public leaderboard.  Your final score/rank is calculated based on this part.  You do not see your private leaderboard score until the end.\n",
    "\n",
    "![How Kaggle Competitions are Scored](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_3_kaggle.png \"How Kaggle Competitions are Scored\")\n",
    "\n",
    "### Preparing a Kaggle Submission\n",
    "\n",
    "Code need not be submitted to Kaggle.  For competitions, you are scored entirely on the accuracy of your sbmission file.  A Kaggle submission file is always a CSV file that contains the **Id** of the row you are predicting and the answer.  For the titanic competition, a submission file looks something like this:\n",
    "\n",
    "```\n",
    "PassengerId,Survived\n",
    "892,0\n",
    "893,1\n",
    "894,1\n",
    "895,0\n",
    "896,0\n",
    "897,1\n",
    "...\n",
    "```\n",
    "\n",
    "The above file states the prediction for each of various passengers.  You should only predict on ID's that are in the test file.  Likewise, you should render a prediction for every row in the test file.  Some competitions will have different formats for their answers.  For example, a multi-classification will usually have a column for each class and your predictions for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Kaggle Competitions\n",
    "\n",
    "There have been many interesting competitions on Kaggle, these are some of my favorites.\n",
    "\n",
    "## Predictive Modeling\n",
    "\n",
    "* [Otto Group Product Classification Challenge](https://www.kaggle.com/c/otto-group-product-classification-challenge)\n",
    "* [Galaxy Zoo - The Galaxy Challenge](https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge)\n",
    "* [Practice Fusion Diabetes Classification](https://www.kaggle.com/c/pf2012-diabetes)\n",
    "* [Predicting a Biological Response](https://www.kaggle.com/c/bioresponse)\n",
    "\n",
    "## Computer Vision\n",
    "\n",
    "* [Diabetic Retinopathy Detection](https://www.kaggle.com/c/diabetic-retinopathy-detection)\n",
    "* [Cats vs Dogs](https://www.kaggle.com/c/dogs-vs-cats)\n",
    "* [State Farm Distracted Driver Detection](https://www.kaggle.com/c/state-farm-distracted-driver-detection)\n",
    "\n",
    "## Time Series\n",
    "\n",
    "* [The Marinexplore and Cornell University Whale Detection Challenge](https://www.kaggle.com/c/whale-detection-challenge)\n",
    "\n",
    "## Other\n",
    "\n",
    "* [Helping Santa's Helpers](https://www.kaggle.com/c/helping-santas-helpers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris as a Kaggle Competition\n",
    "\n",
    "If the Iris data were used as a Kaggle, you would be given the following three files:\n",
    "\n",
    "* [kaggle_iris_test.csv](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/data/kaggle_iris_test.csv) - The data that Kaggle will evaluate you on.  Contains only input, you must provide answers.  (contains x)\n",
    "* [kaggle_iris_train.csv](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/data/kaggle_iris_train.csv) - The data that you will use to train. (contains x and y)\n",
    "* [kaggle_iris_sample.csv](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/data/kaggle_iris_sample.csv) - A sample submission for Kaggle. (contains x and y)\n",
    "\n",
    "Important features of the Kaggle iris files (that differ from how we've previously seen files):\n",
    "\n",
    "* The iris species is already index encoded.\n",
    "* Your training data is in a separate file.\n",
    "* You will load the test data to generate a submission file.\n",
    "\n",
    "The following program generates a submission file for \"Iris Kaggle\".  You can use it as a starting point for assignment 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/jheaton/miniconda3/envs/jeff/lib/python3.6/site-packages/ipykernel_launcher.py:73: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 3\n",
      "Epoch 00204: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a311541d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.learn.python.learn.metric_spec import MetricSpec\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "path = \"./data/\"\n",
    "    \n",
    "filename_train = os.path.join(path,\"kaggle_iris_train.csv\")\n",
    "filename_test = os.path.join(path,\"kaggle_iris_test.csv\")\n",
    "filename_submit = os.path.join(path,\"kaggle_iris_submit.csv\")\n",
    "\n",
    "df_train = pd.read_csv(filename_train,na_values=['NA','?'])\n",
    "\n",
    "# Encode feature vector\n",
    "encode_numeric_zscore(df_train,'petal_w')\n",
    "encode_numeric_zscore(df_train,'petal_l')\n",
    "encode_numeric_zscore(df_train,'sepal_w')\n",
    "encode_numeric_zscore(df_train,'sepal_l')\n",
    "df_train.drop('id', axis=1, inplace=True)\n",
    "\n",
    "num_classes = len(df_train.groupby('species').species.nunique())\n",
    "\n",
    "print(\"Number of classes: {}\".format(num_classes))\n",
    "\n",
    "# Create x & y for training\n",
    "\n",
    "# Create the x-side (feature vectors) of the training\n",
    "x, y = to_xy(df_train,'species')\n",
    "    \n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=45)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=x.shape[1], activation='relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=0,epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss score: 0.11395810696994886\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Calculate multi log loss error\n",
    "pred = model.predict(x_test)\n",
    "score = metrics.log_loss(y_test, pred)\n",
    "print(\"Log loss score: {}\".format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id     species-0  species-1     species-2\n",
      "0   100  1.145505e-02   0.873278  1.152669e-01\n",
      "1   101  2.029274e-04   0.096076  9.037215e-01\n",
      "2   102  1.023206e-04   0.765042  2.348556e-01\n",
      "3   103  9.915872e-01   0.008396  1.671125e-05\n",
      "4   104  9.991596e-01   0.000839  1.385523e-06\n",
      "5   105  9.907163e-01   0.009272  1.160378e-05\n",
      "6   106  9.991513e-01   0.000848  8.523589e-07\n",
      "7   107  1.347605e-04   0.115835  8.840307e-01\n",
      "8   108  2.345528e-03   0.986461  1.119331e-02\n",
      "9   109  4.955922e-05   0.016073  9.838778e-01\n",
      "10  110  9.583807e-07   0.006080  9.939189e-01\n",
      "11  111  9.964200e-01   0.003569  1.131497e-05\n",
      "12  112  1.803313e-02   0.973874  8.093000e-03\n",
      "13  113  2.367835e-06   0.000626  9.993712e-01\n",
      "14  114  9.956765e-01   0.004313  1.048486e-05\n",
      "15  115  5.618177e-04   0.146316  8.531219e-01\n",
      "16  116  2.395735e-02   0.971871  4.171699e-03\n",
      "17  117  9.804505e-01   0.019502  4.776156e-05\n",
      "18  118  9.971092e-01   0.002883  7.476215e-06\n",
      "19  119  9.985059e-01   0.001491  3.445359e-06\n",
      "20  120  5.681551e-03   0.912539  8.177989e-02\n",
      "21  121  5.357833e-03   0.968236  2.640597e-02\n",
      "22  122  6.970728e-04   0.116814  8.824892e-01\n",
      "23  123  1.780590e-06   0.001538  9.984598e-01\n",
      "24  124  9.768521e-06   0.000573  9.994174e-01\n",
      "25  125  9.983822e-01   0.001615  2.900644e-06\n",
      "26  126  6.920247e-06   0.002648  9.973453e-01\n",
      "27  127  7.262093e-05   0.025418  9.745096e-01\n",
      "28  128  8.588902e-06   0.004223  9.957683e-01\n",
      "29  129  9.978940e-01   0.002099  6.633331e-06\n",
      "30  130  4.250273e-03   0.986462  9.287665e-03\n",
      "31  131  9.979821e-01   0.002014  4.134891e-06\n",
      "32  132  1.624637e-04   0.359395  6.404424e-01\n",
      "33  133  9.027977e-03   0.968337  2.263536e-02\n",
      "34  134  1.010977e-02   0.850949  1.389411e-01\n",
      "35  135  9.951606e-01   0.004824  1.566120e-05\n",
      "36  136  8.688696e-07   0.001050  9.989496e-01\n",
      "37  137  2.549613e-04   0.937755  6.198969e-02\n",
      "38  138  9.962470e-01   0.003745  8.219194e-06\n",
      "39  139  5.491516e-04   0.497356  5.020952e-01\n",
      "40  140  9.883413e-01   0.011640  1.850504e-05\n",
      "41  141  4.633267e-06   0.001122  9.988735e-01\n",
      "42  142  1.714959e-07   0.000385  9.996150e-01\n",
      "43  143  1.916079e-04   0.012633  9.871759e-01\n",
      "44  144  9.962271e-01   0.003766  6.770358e-06\n",
      "45  145  2.940499e-03   0.993483  3.576949e-03\n",
      "46  146  3.751567e-02   0.958652  3.832055e-03\n",
      "47  147  7.045380e-03   0.962189  3.076531e-02\n",
      "48  148  7.220935e-02   0.840366  8.742424e-02\n",
      "49  149  9.982088e-01   0.001785  6.024040e-06\n",
      "50  150  9.884192e-01   0.011532  4.889777e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jheaton/miniconda3/envs/jeff/lib/python3.6/site-packages/ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# Generate Kaggle submit file\n",
    "\n",
    "# Encode feature vector\n",
    "df_test = pd.read_csv(filename_test,na_values=['NA','?'])\n",
    "\n",
    "encode_numeric_zscore(df_test,'petal_w')\n",
    "encode_numeric_zscore(df_test,'petal_l')\n",
    "encode_numeric_zscore(df_test,'sepal_w')\n",
    "encode_numeric_zscore(df_test,'sepal_l')\n",
    "ids = df_test['id']\n",
    "df_test.drop('id', axis=1, inplace=True)\n",
    "\n",
    "x = df_test.as_matrix().astype(np.float32)\n",
    "\n",
    "# Generate predictions\n",
    "pred = model.predict(x)\n",
    "#pred\n",
    "\n",
    "# Create submission data set\n",
    "\n",
    "df_submit = pd.DataFrame(pred)\n",
    "df_submit.insert(0,'id',ids)\n",
    "df_submit.columns = ['id','species-0','species-1','species-2']\n",
    "\n",
    "df_submit.to_csv(filename_submit, index=False)\n",
    "\n",
    "print(df_submit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPG as a Kaggle Competition (Regression)\n",
    "\n",
    "If the Iris data were used as a Kaggle, you would be given the following three files:\n",
    "\n",
    "* [kaggle_iris_test.csv](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/data/kaggle_iris_test.csv) - The data that Kaggle will evaluate you on.  Contains only input, you must provide answers.  (contains x)\n",
    "* [kaggle_iris_train.csv](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/data/kaggle_iris_train.csv) - The data that you will use to train. (contains x and y)\n",
    "* [kaggle_iris_sample.csv](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/data/kaggle_iris_sample.csv) - A sample submission for Kaggle. (contains x and y)\n",
    "\n",
    "Important features of the Kaggle iris files (that differ from how we've previously seen files):\n",
    "\n",
    "* The iris species is already index encoded.\n",
    "* Your training data is in a separate file.\n",
    "* You will load the test data to generate a submission file.\n",
    "\n",
    "The following program generates a submission file for \"Iris Kaggle\".  You can use it as a starting point for assignment 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8.2: Building Ensembles with Scikit-Learn and Keras\n",
    "\n",
    "### Evaluating Feature Importance\n",
    "\n",
    "Feature importance tells us how important each of the features (from the feature/import vector are to the prediction of a neural network, or other model.  There are many different ways to evaluate feature importance for neural networks.  The following paper presents a very good (and readable) overview of the various means of evaluating the importance of neural network inputs/features.\n",
    "\n",
    "Olden, J. D., Joy, M. K., & Death, R. G. (2004). [An accurate comparison of methods for quantifying variable importance in artificial neural networks using simulated data](http://depts.washington.edu/oldenlab/wordpress/wp-content/uploads/2013/03/EcologicalModelling_2004.pdf). *Ecological Modelling*, 178(3), 389-397.\n",
    "\n",
    "In summary, the following methods are available to neural networks:\n",
    "\n",
    "* Connection Weights Algorithm\n",
    "* Partial Derivatives\n",
    "* Input Perturbation\n",
    "* Sensitivity Analysis\n",
    "* Forward Stepwise Addition \n",
    "* Improved Stepwise Selection 1\n",
    "* Backward Stepwise Elimination\n",
    "* Improved Stepwise Selection\n",
    "\n",
    "For this class we will use the **Input Perturbation** feature ranking algorithm.  This algorithm will work with any regression or classification network.  implementation of the input perturbation algorithm for scikit-learn is given in the next section. This algorithm is implemented in a function below that will work with any scikit-learn model.\n",
    "\n",
    "This algorithm was introduced by [Breiman](https://en.wikipedia.org/wiki/Leo_Breiman) in his seminal paper on random forests.  Although he presented this algorithm in conjunction with random forests, it is model-independent and appropriate for any supervised learning model.  This algorithm, known as the input perturbation algorithm, works by evaluating a trained model’s accuracy with each of the inputs individually shuffled from a data set.  Shuffling an input causes it to become useless—effectively removing it from the model. More important inputs will produce a less accurate score when they are removed by shuffling them. This process makes sense, because important features will contribute to the accuracy of the model.  The TensorFlow version of this algorithm is taken from the following paper.\n",
    "\n",
    "Heaton, J., McElwee, S., & Cannady, J. (May 2017). Early stabilizing feature importance for TensorFlow deep neural networks. In *International Joint Conference on Neural Networks (IJCNN 2017)* (accepted for publication). IEEE.\n",
    "\n",
    "This algorithm will use logloss to evaluate a classification problem and RMSE for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn import metrics\n",
    "\n",
    "def perturbation_rank(model, x, y, names, regression):\n",
    "    errors = []\n",
    "\n",
    "    for i in range(x.shape[1]):\n",
    "        hold = np.array(x[:, i])\n",
    "        np.random.shuffle(x[:, i])\n",
    "        \n",
    "        if regression:\n",
    "            pred = model.predict(x)\n",
    "            error = metrics.mean_squared_error(y, pred)\n",
    "        else:\n",
    "            pred = model.predict_proba(x)\n",
    "            error = metrics.log_loss(y, pred)\n",
    "            \n",
    "        errors.append(error)\n",
    "        x[:, i] = hold\n",
    "        \n",
    "    max_error = np.max(errors)\n",
    "    importance = [e/max_error for e in errors]\n",
    "\n",
    "    data = {'name':names,'error':errors,'importance':importance}\n",
    "    result = pd.DataFrame(data, columns = ['name','error','importance'])\n",
    "    result.sort_values(by=['importance'], ascending=[0], inplace=True)\n",
    "    result.reset_index(inplace=True, drop=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and Input Perturbation Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "112/112 - 0s - loss: 1.3953\n",
      "Epoch 2/100\n",
      "112/112 - 0s - loss: 1.2085\n",
      "Epoch 3/100\n",
      "112/112 - 0s - loss: 1.0975\n",
      "Epoch 4/100\n",
      "112/112 - 0s - loss: 0.9936\n",
      "Epoch 5/100\n",
      "112/112 - 0s - loss: 0.9165\n",
      "Epoch 6/100\n",
      "112/112 - 0s - loss: 0.8591\n",
      "Epoch 7/100\n",
      "112/112 - 0s - loss: 0.8223\n",
      "Epoch 8/100\n",
      "112/112 - 0s - loss: 0.7763\n",
      "Epoch 9/100\n",
      "112/112 - 0s - loss: 0.7443\n",
      "Epoch 10/100\n",
      "112/112 - 0s - loss: 0.7191\n",
      "Epoch 11/100\n",
      "112/112 - 0s - loss: 0.6912\n",
      "Epoch 12/100\n",
      "112/112 - 0s - loss: 0.6672\n",
      "Epoch 13/100\n",
      "112/112 - 0s - loss: 0.6444\n",
      "Epoch 14/100\n",
      "112/112 - 0s - loss: 0.6226\n",
      "Epoch 15/100\n",
      "112/112 - 0s - loss: 0.6023\n",
      "Epoch 16/100\n",
      "112/112 - 0s - loss: 0.5839\n",
      "Epoch 17/100\n",
      "112/112 - 0s - loss: 0.5654\n",
      "Epoch 18/100\n",
      "112/112 - 0s - loss: 0.5494\n",
      "Epoch 19/100\n",
      "112/112 - 0s - loss: 0.5332\n",
      "Epoch 20/100\n",
      "112/112 - 0s - loss: 0.5190\n",
      "Epoch 21/100\n",
      "112/112 - 0s - loss: 0.5049\n",
      "Epoch 22/100\n",
      "112/112 - 0s - loss: 0.4920\n",
      "Epoch 23/100\n",
      "112/112 - 0s - loss: 0.4808\n",
      "Epoch 24/100\n",
      "112/112 - 0s - loss: 0.4690\n",
      "Epoch 25/100\n",
      "112/112 - 0s - loss: 0.4582\n",
      "Epoch 26/100\n",
      "112/112 - 0s - loss: 0.4453\n",
      "Epoch 27/100\n",
      "112/112 - 0s - loss: 0.4372\n",
      "Epoch 28/100\n",
      "112/112 - 0s - loss: 0.4265\n",
      "Epoch 29/100\n",
      "112/112 - 0s - loss: 0.4167\n",
      "Epoch 30/100\n",
      "112/112 - 0s - loss: 0.4088\n",
      "Epoch 31/100\n",
      "112/112 - 0s - loss: 0.4003\n",
      "Epoch 32/100\n",
      "112/112 - 0s - loss: 0.3932\n",
      "Epoch 33/100\n",
      "112/112 - 0s - loss: 0.3861\n",
      "Epoch 34/100\n",
      "112/112 - 0s - loss: 0.3772\n",
      "Epoch 35/100\n",
      "112/112 - 0s - loss: 0.3701\n",
      "Epoch 36/100\n",
      "112/112 - 0s - loss: 0.3627\n",
      "Epoch 37/100\n",
      "112/112 - 0s - loss: 0.3561\n",
      "Epoch 38/100\n",
      "112/112 - 0s - loss: 0.3514\n",
      "Epoch 39/100\n",
      "112/112 - 0s - loss: 0.3436\n",
      "Epoch 40/100\n",
      "112/112 - 0s - loss: 0.3368\n",
      "Epoch 41/100\n",
      "112/112 - 0s - loss: 0.3305\n",
      "Epoch 42/100\n",
      "112/112 - 0s - loss: 0.3240\n",
      "Epoch 43/100\n",
      "112/112 - 0s - loss: 0.3184\n",
      "Epoch 44/100\n",
      "112/112 - 0s - loss: 0.3134\n",
      "Epoch 45/100\n",
      "112/112 - 0s - loss: 0.3030\n",
      "Epoch 46/100\n",
      "112/112 - 0s - loss: 0.2984\n",
      "Epoch 47/100\n",
      "112/112 - 0s - loss: 0.2893\n",
      "Epoch 48/100\n",
      "112/112 - 0s - loss: 0.2833\n",
      "Epoch 49/100\n",
      "112/112 - 0s - loss: 0.2804\n",
      "Epoch 50/100\n",
      "112/112 - 0s - loss: 0.2719\n",
      "Epoch 51/100\n",
      "112/112 - 0s - loss: 0.2648\n",
      "Epoch 52/100\n",
      "112/112 - 0s - loss: 0.2595\n",
      "Epoch 53/100\n",
      "112/112 - 0s - loss: 0.2541\n",
      "Epoch 54/100\n",
      "112/112 - 0s - loss: 0.2518\n",
      "Epoch 55/100\n",
      "112/112 - 0s - loss: 0.2451\n",
      "Epoch 56/100\n",
      "112/112 - 0s - loss: 0.2383\n",
      "Epoch 57/100\n",
      "112/112 - 0s - loss: 0.2291\n",
      "Epoch 58/100\n",
      "112/112 - 0s - loss: 0.2274\n",
      "Epoch 59/100\n",
      "112/112 - 0s - loss: 0.2231\n",
      "Epoch 60/100\n",
      "112/112 - 0s - loss: 0.2188\n",
      "Epoch 61/100\n",
      "112/112 - 0s - loss: 0.2137\n",
      "Epoch 62/100\n",
      "112/112 - 0s - loss: 0.2081\n",
      "Epoch 63/100\n",
      "112/112 - 0s - loss: 0.2039\n",
      "Epoch 64/100\n",
      "112/112 - 0s - loss: 0.1970\n",
      "Epoch 65/100\n",
      "112/112 - 0s - loss: 0.1927\n",
      "Epoch 66/100\n",
      "112/112 - 0s - loss: 0.1892\n",
      "Epoch 67/100\n",
      "112/112 - 0s - loss: 0.1846\n",
      "Epoch 68/100\n",
      "112/112 - 0s - loss: 0.1834\n",
      "Epoch 69/100\n",
      "112/112 - 0s - loss: 0.1781\n",
      "Epoch 70/100\n",
      "112/112 - 0s - loss: 0.1723\n",
      "Epoch 71/100\n",
      "112/112 - 0s - loss: 0.1683\n",
      "Epoch 72/100\n",
      "112/112 - 0s - loss: 0.1652\n",
      "Epoch 73/100\n",
      "112/112 - 0s - loss: 0.1633\n",
      "Epoch 74/100\n",
      "112/112 - 0s - loss: 0.1577\n",
      "Epoch 75/100\n",
      "112/112 - 0s - loss: 0.1562\n",
      "Epoch 76/100\n",
      "112/112 - 0s - loss: 0.1538\n",
      "Epoch 77/100\n",
      "112/112 - 0s - loss: 0.1513\n",
      "Epoch 78/100\n",
      "112/112 - 0s - loss: 0.1468\n",
      "Epoch 79/100\n",
      "112/112 - 0s - loss: 0.1441\n",
      "Epoch 80/100\n",
      "112/112 - 0s - loss: 0.1412\n",
      "Epoch 81/100\n",
      "112/112 - 0s - loss: 0.1396\n",
      "Epoch 82/100\n",
      "112/112 - 0s - loss: 0.1375\n",
      "Epoch 83/100\n",
      "112/112 - 0s - loss: 0.1388\n",
      "Epoch 84/100\n",
      "112/112 - 0s - loss: 0.1348\n",
      "Epoch 85/100\n",
      "112/112 - 0s - loss: 0.1300\n",
      "Epoch 86/100\n",
      "112/112 - 0s - loss: 0.1279\n",
      "Epoch 87/100\n",
      "112/112 - 0s - loss: 0.1261\n",
      "Epoch 88/100\n",
      "112/112 - 0s - loss: 0.1245\n",
      "Epoch 89/100\n",
      "112/112 - 0s - loss: 0.1223\n",
      "Epoch 90/100\n",
      "112/112 - 0s - loss: 0.1218\n",
      "Epoch 91/100\n",
      "112/112 - 0s - loss: 0.1194\n",
      "Epoch 92/100\n",
      "112/112 - 0s - loss: 0.1179\n",
      "Epoch 93/100\n",
      "112/112 - 0s - loss: 0.1161\n",
      "Epoch 94/100\n",
      "112/112 - 0s - loss: 0.1152\n",
      "Epoch 95/100\n",
      "112/112 - 0s - loss: 0.1131\n",
      "Epoch 96/100\n",
      "112/112 - 0s - loss: 0.1120\n",
      "Epoch 97/100\n",
      "112/112 - 0s - loss: 0.1109\n",
      "Epoch 98/100\n",
      "112/112 - 0s - loss: 0.1111\n",
      "Epoch 99/100\n",
      "112/112 - 0s - loss: 0.1078\n",
      "Epoch 100/100\n",
      "112/112 - 0s - loss: 0.1088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a2d1efc50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/iris.csv\", \n",
    "    na_values=['NA', '?'])\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "x = df[['sepal_l', 'sepal_w', 'petal_l', 'petal_w']].values\n",
    "dummies = pd.get_dummies(df['species']) # Classification\n",
    "species = dummies.columns\n",
    "y = dummies.values\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Build neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "model.add(Dense(25, activation='relu')) # Hidden 2\n",
    "model.add(Dense(y.shape[1],activation='softmax')) # Output\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(x_train,y_train,verbose=2,epochs=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "predict_classes = np.argmax(pred,axis=1)\n",
    "expected_classes = np.argmax(y_test,axis=1)\n",
    "correct = accuracy_score(expected_classes,predict_classes)\n",
    "print(f\"Accuracy: {correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>petal_l</td>\n",
       "      <td>3.679627</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>petal_w</td>\n",
       "      <td>0.733134</td>\n",
       "      <td>0.199241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sepal_l</td>\n",
       "      <td>0.161628</td>\n",
       "      <td>0.043925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sepal_w</td>\n",
       "      <td>0.092983</td>\n",
       "      <td>0.025270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name     error  importance\n",
       "0  petal_l  3.679627    1.000000\n",
       "1  petal_w  0.733134    0.199241\n",
       "2  sepal_l  0.161628    0.043925\n",
       "3  sepal_w  0.092983    0.025270"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rank the features\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = list(df.columns) # x+y column names\n",
    "names.remove(\"species\") # remove the target(y)\n",
    "rank = perturbation_rank(model, x_test, y_test, names, False)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression and Input Perturbation Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encode_text_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d3cfe3be6b44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NA'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'?'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mactivity_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_text_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Activity'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'encode_text_index' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import KFold\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "filename_train = os.path.join(path,\"bio_train.csv\")\n",
    "filename_test = os.path.join(path,\"bio_test.csv\")\n",
    "filename_submit = os.path.join(path,\"bio_submit.csv\")\n",
    "df_train = pd.read_csv(filename_train,na_values=['NA','?'])\n",
    "df_test = pd.read_csv(filename_test,na_values=['NA','?'])\n",
    "\n",
    "activity_classes = encode_text_index(df_train,'Activity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biological Response with Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.contrib.learn as skflow\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "# Set the desired TensorFlow output level for this example\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Encode feature vector\n",
    "x, y = to_xy(df_train,'Activity')\n",
    "x_submit = df_test.as_matrix().astype(np.float32)\n",
    "num_classes = len(activity_classes)\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=42) \n",
    "\n",
    "print(\"Fitting/Training...\")\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=x.shape[1], activation='relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=0,epochs=1000)\n",
    "print(\"Fitting done...\")\n",
    "\n",
    "# Give logloss error\n",
    "pred = model.predict(x_test)\n",
    "pred2 = np.argmax(pred,axis=1)\n",
    "pred = pred[:,1]\n",
    "# Clip so that min is never exactly 0, max never 1\n",
    "pred = np.clip(pred,a_min=1e-6,a_max=(1-1e-6)) \n",
    "print(\"Validation logloss: {}\".format(sklearn.metrics.log_loss(y_test,pred)))\n",
    "\n",
    "# Evaluate success using accuracy\n",
    "pred_submit = pred.copy()\n",
    "y_test = y_test[:,1]\n",
    "score = metrics.accuracy_score(y_test, pred2)\n",
    "print(\"Validation accuracy score: {}\".format(score))\n",
    "\n",
    "# Build real submit file\n",
    "pred_submit = model.predict(x_submit)\n",
    "pred_submit = pred_submit[:,1]\n",
    "\n",
    "# Clip so that min is never exactly 0, max never 1\n",
    "pred = np.clip(pred,a_min=1e-6,a_max=(1-1e-6)) \n",
    "submit_df = pd.DataFrame({'MoleculeId':[x+1 for x in range(len(pred_submit))],'PredictedProbability':pred_submit})\n",
    "submit_df.to_csv(filename_submit, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Features/Columns are Important\n",
    "The following uses perturbation ranking to evaluate the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank the features\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = list(df_train.columns) # x+y column names\n",
    "names.remove(\"Activity\") # remove the target(y)\n",
    "rank = perturbation_rank(model, x_test, y_test, names, False)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Ensemble\n",
    "\n",
    "A neural network ensemble combines neural network predictions with other models. The exact blend of all of these models is determined by logistic regression. The following code performs this blend for a classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "PATH = \"./data/\"\n",
    "SHUFFLE = False\n",
    "FOLDS = 10\n",
    "\n",
    "def build_ann(input_size,classes,neurons):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=input_size, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Dense(classes,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def mlogloss(y_test, preds):\n",
    "    epsilon = 1e-15\n",
    "    sum = 0\n",
    "    for row in zip(preds,y_test):\n",
    "        x = row[0][row[1]]\n",
    "        x = max(epsilon,x)\n",
    "        x = min(1-epsilon,x)\n",
    "        sum+=math.log(x)\n",
    "    return( (-1/len(preds))*sum)\n",
    "\n",
    "def stretch(y):\n",
    "    return (y - y.min()) / (y.max() - y.min())\n",
    "\n",
    "\n",
    "def blend_ensemble(x, y, x_submit):\n",
    "    kf = StratifiedKFold(FOLDS)\n",
    "    folds = list(kf.split(x,y))\n",
    "    feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x.shape[0])]\n",
    "\n",
    "    models = [\n",
    "        KerasClassifier(build_fn=build_ann,neurons=20,input_size=x.shape[1],classes=2),\n",
    "        KNeighborsClassifier(n_neighbors=3),\n",
    "        RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "        RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50)]\n",
    "\n",
    "    dataset_blend_train = np.zeros((x.shape[0], len(models)))\n",
    "    dataset_blend_test = np.zeros((x_submit.shape[0], len(models)))\n",
    "\n",
    "    for j, model in enumerate(models):\n",
    "        print(\"Model: {} : {}\".format(j, model) )\n",
    "        fold_sums = np.zeros((x_submit.shape[0], len(folds)))\n",
    "        total_loss = 0\n",
    "        for i, (train, test) in enumerate(folds):\n",
    "            x_train = x[train]\n",
    "            y_train = y[train]\n",
    "            x_test = x[test]\n",
    "            y_test = y[test]\n",
    "            model.fit(x_train, y_train)\n",
    "            pred = np.array(model.predict_proba(x_test))\n",
    "            # pred = model.predict_proba(x_test)\n",
    "            dataset_blend_train[test, j] = pred[:, 1]\n",
    "            pred2 = np.array(model.predict_proba(x_submit))\n",
    "            #fold_sums[:, i] = model.predict_proba(x_submit)[:, 1]\n",
    "            fold_sums[:, i] = pred2[:, 1]\n",
    "            loss = mlogloss(y_test, pred)\n",
    "            total_loss+=loss\n",
    "            print(\"Fold #{}: loss={}\".format(i,loss))\n",
    "        print(\"{}: Mean loss={}\".format(model.__class__.__name__,total_loss/len(folds)))\n",
    "        dataset_blend_test[:, j] = fold_sums.mean(1)\n",
    "\n",
    "    print()\n",
    "    print(\"Blending models.\")\n",
    "    blend = LogisticRegression()\n",
    "    blend.fit(dataset_blend_train, y)\n",
    "    return blend.predict_proba(dataset_blend_test)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    np.random.seed(42)  # seed to shuffle the train set\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    filename_train = os.path.join(PATH, \"bio_train.csv\")\n",
    "    df_train = pd.read_csv(filename_train, na_values=['NA', '?'])\n",
    "\n",
    "    filename_submit = os.path.join(PATH, \"bio_test.csv\")\n",
    "    df_submit = pd.read_csv(filename_submit, na_values=['NA', '?'])\n",
    "\n",
    "    predictors = list(df_train.columns.values)\n",
    "    predictors.remove('Activity')\n",
    "    x = df_train[predictors].values\n",
    "    y = df_train['Activity']\n",
    "    x_submit = df_submit.values\n",
    "\n",
    "    if SHUFFLE:\n",
    "        idx = np.random.permutation(y.size)\n",
    "        x = x[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    submit_data = blend_ensemble(x, y, x_submit)\n",
    "    submit_data = stretch(submit_data)\n",
    "\n",
    "    ####################\n",
    "    # Build submit file\n",
    "    ####################\n",
    "    ids = [id+1 for id in range(submit_data.shape[0])]\n",
    "    submit_filename = os.path.join(PATH, \"bio_submit.csv\")\n",
    "    submit_df = pd.DataFrame({'MoleculeId': ids, 'PredictedProbability': submit_data[:, 1]},\n",
    "                             columns=['MoleculeId','PredictedProbability'])\n",
    "    submit_df.to_csv(submit_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and Input Perturbation Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8.3: How Should you Architect Your Keras Neural Network: Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8.4: Bayesian Hyperparameter Optimization for Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 8.5: Current Semester's Kaggle\n",
    "\n",
    "Kaggke competition site for current semester (Fall 2019):\n",
    "\n",
    "* Coming soon\n",
    "\n",
    "Previous Kaggle competition sites for this class (NOT this semester's assignment, feel free to use code):\n",
    "* [Spring 2019 Kaggle Assignment](https://www.kaggle.com/c/applications-of-deep-learningwustl-spring-2019)\n",
    "* [Fall 2018 Kaggle Assignment](https://www.kaggle.com/c/wustl-t81-558-washu-deep-learning-fall-2018)\n",
    "* [Spring 2018 Kaggle Assignment](https://www.kaggle.com/c/wustl-t81-558-washu-deep-learning-spring-2018)\n",
    "* [Fall 2017 Kaggle Assignment](https://www.kaggle.com/c/wustl-t81-558-washu-deep-learning-fall-2017)\n",
    "* [Spring 2017 Kaggle Assignment](https://inclass.kaggle.com/c/applications-of-deep-learning-wustl-spring-2017)\n",
    "* [Fall 2016 Kaggle Assignment](https://inclass.kaggle.com/c/wustl-t81-558-washu-deep-learning-fall-2016)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Module 8 Assignment\n",
    "\n",
    "You can find the first assignment here: [assignment 8](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class8.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow-2.0)",
   "language": "python",
   "name": "tensorflow-2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
