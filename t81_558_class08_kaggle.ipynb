{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# T81-558: Applications of Deep Neural Networks\n**Module 8: Kaggle Data Sets**\n* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Module Video Material\n\nMain video lecture:\n\n* [Part 8.1: Introduction to Kaggle](https://www.youtube.com/watch?v=XpGI4engRjQ&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN&index=24)\n* [Part 8.2: Building Ensembles with Scikit-Learn and Keras](https://www.youtube.com/watch?v=AA3KFxjPxCo&index=25&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN)\n* [Part 8.3: How Should you Architect Your Keras Neural Network: Hyperparameters](https://www.youtube.com/watch?v=GaKo-9c532c)\n* [Part 8.4: Bayesian Hyperparameter Optimization for Keras](https://www.youtube.com/watch?v=GaKo-9c532c)\n* [Part 8.5: Current Semester's Kaggle](https://www.youtube.com/watch?v=GaKo-9c532c)\n"
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Part 8.1: Introduction to Kaggle\n\n[Kaggle](http://www.kaggle.com) runs competitions in which data scientists compete in order to provide the best model to fit the data. The capstone project of this chapter features Kaggle’s [Titanic data set](https://www.kaggle.com/c/titanic-gettingStarted). Before we get started with the Titanic example, it’s important to be aware of some Kaggle guidelines. First, most competitions end on a specific date. Website organizers have currently scheduled the Titanic competition to end on December 31, 2016. However, they have already extended the deadline several times, and an extension beyond 2014 is also possible. Second, the Titanic data set is considered a tutorial data set. In other words, there is no prize, and your score in the competition does not count towards becoming a Kaggle Master. test"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Kaggle Ranks\n\nKaggle ranks are achieved by earning gold, silver and bronze medals.\n\n* [Kaggle Top Users](https://www.kaggle.com/rankings)\n* [Current Top Kaggle User's Profile Page](https://www.kaggle.com/stasg7)\n* [Jeff Heaton's (your instructor) Kaggle Profile](https://www.kaggle.com/jeffheaton)\n* [Current Kaggle Ranking System](https://www.kaggle.com/progression)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Typical Kaggle Competition\n\nA typical Kaggle competition will have several components.  Consider the Titanic tutorial:\n\n* [Competition Summary Page](https://www.kaggle.com/c/titanic)\n* [Data Page](https://www.kaggle.com/c/titanic/data)\n* [Evaluation Description Page](https://www.kaggle.com/c/titanic/details/evaluation)\n* [Leaderboard](https://www.kaggle.com/c/titanic/leaderboard)\n\n### How Kaggle Competitions are Scored\n\nKaggle is provided with a data set by the competition sponsor.  This data set is divided up as follows:\n\n* **Complete Data Set** - This is the complete data set.\n    * **Training Data Set** - You are provided both the inputs and the outcomes for the training portion of the data set.\n    * **Test Data Set** - You are provided the complete test data set; however, you are not given the outcomes.  Your submission is  your predicted outcomes for this data set.\n        * **Public Leaderboard** - You are not told what part of the test data set contributes to the public leaderboard.  Your public score is calculated based on this part of the data set.\n        * **Private Leaderboard** - You are not told what part of the test data set contributes to the public leaderboard.  Your final score/rank is calculated based on this part.  You do not see your private leaderboard score until the end.\n\n![How Kaggle Competitions are Scored](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_3_kaggle.png \"How Kaggle Competitions are Scored\")\n\n### Preparing a Kaggle Submission\n\nCode need not be submitted to Kaggle.  For competitions, you are scored entirely on the accuracy of your sbmission file.  A Kaggle submission file is always a CSV file that contains the **Id** of the row you are predicting and the answer.  For the titanic competition, a submission file looks something like this:\n\n```\nPassengerId,Survived\n892,0\n893,1\n894,1\n895,0\n896,0\n897,1\n...\n```\n\nThe above file states the prediction for each of various passengers.  You should only predict on ID's that are in the test file.  Likewise, you should render a prediction for every row in the test file.  Some competitions will have different formats for their answers.  For example, a multi-classification will usually have a column for each class and your predictions for each class."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Select Kaggle Competitions\n\nThere have been many interesting competitions on Kaggle, these are some of my favorites.\n\n## Predictive Modeling\n\n* [Otto Group Product Classification Challenge](https://www.kaggle.com/c/otto-group-product-classification-challenge)\n* [Galaxy Zoo - The Galaxy Challenge](https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge)\n* [Practice Fusion Diabetes Classification](https://www.kaggle.com/c/pf2012-diabetes)\n* [Predicting a Biological Response](https://www.kaggle.com/c/bioresponse)\n\n## Computer Vision\n\n* [Diabetic Retinopathy Detection](https://www.kaggle.com/c/diabetic-retinopathy-detection)\n* [Cats vs Dogs](https://www.kaggle.com/c/dogs-vs-cats)\n* [State Farm Distracted Driver Detection](https://www.kaggle.com/c/state-farm-distracted-driver-detection)\n\n## Time Series\n\n* [The Marinexplore and Cornell University Whale Detection Challenge](https://www.kaggle.com/c/whale-detection-challenge)\n\n## Other\n\n* [Helping Santa's Helpers](https://www.kaggle.com/c/helping-santas-helpers)\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Iris as a Kaggle Competition\n\nIf the Iris data were used as a Kaggle, you would be given the following three files:\n\n* [kaggle_iris_test.csv](https://data.heatonresearch.com/data/t81-558/datasets/kaggle_iris_test.csv) - The data that Kaggle will evaluate you on.  Contains only input, you must provide answers.  (contains x)\n* [kaggle_iris_train.csv](https://data.heatonresearch.com/data/t81-558/datasets/kaggle_iris_train.csv) - The data that you will use to train. (contains x and y)\n* [kaggle_iris_sample.csv](https://data.heatonresearch.com/data/t81-558/datasets/kaggle_iris_sample.csv) - A sample submission for Kaggle. (contains x and y)\n\nImportant features of the Kaggle iris files (that differ from how we've previously seen files):\n\n* The iris species is already index encoded.\n* Your training data is in a separate file.\n* You will load the test data to generate a submission file.\n\nThe following program generates a submission file for \"Iris Kaggle\".  You can use it as a starting point for assignment 3."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\n\ndf_train = pd.read_csv(\"https://data.heatonresearch.com/data/t81-558/datasets/kaggle_iris_train.csv\",\n                       na_values=['NA','?'])\n\n# Encode feature vector\ndf_train.drop('id', axis=1, inplace=True)\n\nnum_classes = len(df_train.groupby('species').species.nunique())\n\nprint(\"Number of classes: {}\".format(num_classes))\n\n# Convert to numpy - Classification\nx = df_train[['sepal_l', 'sepal_w', 'petal_l', 'petal_w']].values\ndummies = pd.get_dummies(df_train['species']) # Classification\nspecies = dummies.columns\ny = dummies.values\n    \n# Split into train/test\nx_train, x_test, y_train, y_test = train_test_split(    \n    x, y, test_size=0.25, random_state=45)\n\n# Train, with early stopping\nmodel = Sequential()\nmodel.add(Dense(0, input_dim=x.shape[1], activation='relu'))\nmodel.add(Dense(20))\nmodel.add(Dense(y.shape[1],activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\nmonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto',\n                       restore_best_weights=True)\n\nmodel.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=0,epochs=1000)",
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Number of classes: 3\nRestoring model weights from the end of the best epoch.\nEpoch 00311: early stopping\n"
        },
        {
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1a380c79b0>"
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn import metrics\n\n# Calculate multi log loss error\npred = model.predict(x_test)\nscore = metrics.log_loss(y_test, pred)\nprint(\"Log loss score: {}\".format(score))\n",
      "execution_count": 18,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Log loss score: 0.023757144288974814\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Generate Kaggle submit file\n\n# Encode feature vector\ndf_test = pd.read_csv(\"https://data.heatonresearch.com/data/t81-558/datasets/kaggle_iris_test.csv\",\n                      na_values=['NA','?'])\n\n# Convert to numpy - Classification\nids = df_test['id']\ndf_test.drop('id', axis=1, inplace=True)\nx = df_test[['sepal_l', 'sepal_w', 'petal_l', 'petal_w']].values\ny = dummies.values\n\n# Generate predictions\npred = model.predict(x)\n#pred\n\n# Create submission data set\n\ndf_submit = pd.DataFrame(pred)\ndf_submit.insert(0,'id',ids)\ndf_submit.columns = ['id','species-0','species-1','species-2']\n\ndf_submit.to_csv(\"iris_submit.csv\", index=False) # Write submit file locally\n\nprint(df_submit)\n",
      "execution_count": 24,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "     id     species-0  species-1     species-2\n0   100  3.045681e-03   0.985162  1.179271e-02\n1   101  3.497185e-05   0.194923  8.050420e-01\n2   102  4.291264e-05   0.257433  7.425241e-01\n3   103  9.941856e-01   0.005814  4.016067e-13\n4   104  9.964947e-01   0.003505  1.539487e-13\n5   105  9.935989e-01   0.006401  1.518143e-13\n6   106  9.992448e-01   0.000755  1.248318e-16\n7   107  1.417579e-06   0.022125  9.778739e-01\n8   108  3.034286e-03   0.989315  7.650502e-03\n9   109  1.571568e-07   0.014305  9.856946e-01\n10  110  7.784679e-10   0.000204  9.997965e-01\n11  111  9.996639e-01   0.000336  7.656645e-18\n12  112  2.542428e-02   0.974253  3.226090e-04\n13  113  4.249036e-09   0.000521  9.994790e-01\n14  114  9.969503e-01   0.003050  2.393938e-14\n15  115  1.163287e-04   0.638071  3.618126e-01\n16  116  2.081596e-02   0.977822  1.362586e-03\n17  117  9.950796e-01   0.004920  2.605337e-14\n18  118  9.980367e-01   0.001963  5.106467e-15\n19  119  9.986822e-01   0.001318  2.043191e-15\n20  120  2.513996e-03   0.994891  2.595227e-03\n21  121  5.488432e-03   0.992898  1.613215e-03\n22  122  4.080776e-05   0.203068  7.968907e-01\n23  123  9.478534e-09   0.000292  9.997076e-01\n24  124  1.568250e-08   0.001691  9.983093e-01\n25  125  9.982161e-01   0.001784  2.768862e-15\n26  126  8.770471e-07   0.017610  9.823888e-01\n27  127  4.325702e-07   0.014047  9.859523e-01\n28  128  1.996197e-07   0.008463  9.915365e-01\n29  129  9.984077e-01   0.001592  5.606248e-15\n30  130  5.792776e-03   0.991962  2.245403e-03\n31  131  9.968176e-01   0.003182  6.063090e-14\n32  132  1.207925e-05   0.131360  8.686283e-01\n33  133  2.458394e-03   0.984136  1.340520e-02\n34  134  7.297331e-04   0.785947  2.133229e-01\n35  135  9.952751e-01   0.004725  1.424939e-13\n36  136  1.731268e-09   0.000194  9.998056e-01\n37  137  3.096495e-04   0.834478  1.652121e-01\n38  138  9.967808e-01   0.003219  2.400064e-14\n39  139  1.334545e-06   0.006478  9.935209e-01\n40  140  9.941970e-01   0.005803  2.077976e-13\n41  141  4.960742e-08   0.003107  9.968926e-01\n42  142  4.192627e-11   0.000055  9.999454e-01\n43  143  3.995325e-07   0.006838  9.931613e-01\n44  144  9.943367e-01   0.005663  1.073975e-13\n45  145  3.686049e-03   0.962349  3.396509e-02\n46  146  8.626970e-02   0.913645  8.567782e-05\n47  147  3.359596e-03   0.949646  4.699398e-02\n48  148  2.384406e-03   0.985552  1.206337e-02\n49  149  9.985020e-01   0.001498  5.141223e-15\n50  150  9.913275e-01   0.008673  8.540413e-13\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# MPG as a Kaggle Competition (Regression)\n\nIf the Auto MPG data were used as a Kaggle, you would be given the following three files:\n\n* [kaggle_mpg_test.csv](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/data/kaggle_mpg_test.csv) - The data that Kaggle will evaluate you on.  Contains only input, you must provide answers.  (contains x)\n* [kaggle_mpg_train.csv](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/data/kaggle_mpg_train.csv) - The data that you will use to train. (contains x and y)\n* [kaggle_mpg_sample.csv](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/data/kaggle_mpg_sample.csv) - A sample submission for Kaggle. (contains x and y)\n\nImportant features of the Kaggle iris files (that differ from how we've previously seen files):\n\nThe following program generates a submission file for \"MPG Kaggle\".  "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Part 8.2: Building Ensembles with Scikit-Learn and Keras\n\n### Evaluating Feature Importance\n\nFeature importance tells us how important each of the features (from the feature/import vector are to the prediction of a neural network, or other model.  There are many different ways to evaluate feature importance for neural networks.  The following paper presents a very good (and readable) overview of the various means of evaluating the importance of neural network inputs/features.\n\nOlden, J. D., Joy, M. K., & Death, R. G. (2004). [An accurate comparison of methods for quantifying variable importance in artificial neural networks using simulated data](http://depts.washington.edu/oldenlab/wordpress/wp-content/uploads/2013/03/EcologicalModelling_2004.pdf). *Ecological Modelling*, 178(3), 389-397.\n\nIn summary, the following methods are available to neural networks:\n\n* Connection Weights Algorithm\n* Partial Derivatives\n* Input Perturbation\n* Sensitivity Analysis\n* Forward Stepwise Addition \n* Improved Stepwise Selection 1\n* Backward Stepwise Elimination\n* Improved Stepwise Selection\n\nFor this class we will use the **Input Perturbation** feature ranking algorithm.  This algorithm will work with any regression or classification network.  implementation of the input perturbation algorithm for scikit-learn is given in the next section. This algorithm is implemented in a function below that will work with any scikit-learn model.\n\nThis algorithm was introduced by [Breiman](https://en.wikipedia.org/wiki/Leo_Breiman) in his seminal paper on random forests.  Although he presented this algorithm in conjunction with random forests, it is model-independent and appropriate for any supervised learning model.  This algorithm, known as the input perturbation algorithm, works by evaluating a trained model’s accuracy with each of the inputs individually shuffled from a data set.  Shuffling an input causes it to become useless—effectively removing it from the model. More important inputs will produce a less accurate score when they are removed by shuffling them. This process makes sense, because important features will contribute to the accuracy of the model.  The TensorFlow version of this algorithm is taken from the following paper.\n\nHeaton, J., McElwee, S., & Cannady, J. (May 2017). Early stabilizing feature importance for TensorFlow deep neural networks. In *International Joint Conference on Neural Networks (IJCNN 2017)* (accepted for publication). IEEE.\n\nThis algorithm will use logloss to evaluate a classification problem and RMSE for regression."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn import metrics\nimport scipy as sp\nimport numpy as np\nimport math\nfrom sklearn import metrics\n\ndef perturbation_rank(model, x, y, names, regression):\n    errors = []\n\n    for i in range(x.shape[1]):\n        hold = np.array(x[:, i])\n        np.random.shuffle(x[:, i])\n        \n        if regression:\n            pred = model.predict(x)\n            error = metrics.mean_squared_error(y, pred)\n        else:\n            pred = model.predict_proba(x)\n            error = metrics.log_loss(y, pred)\n            \n        errors.append(error)\n        x[:, i] = hold\n        \n    max_error = np.max(errors)\n    importance = [e/max_error for e in errors]\n\n    data = {'name':names,'error':errors,'importance':importance}\n    result = pd.DataFrame(data, columns = ['name','error','importance'])\n    result.sort_values(by=['importance'], ascending=[0], inplace=True)\n    result.reset_index(inplace=True, drop=True)\n    return result",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Classification and Input Perturbation Ranking"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport io\nimport requests\nimport numpy as np\nfrom sklearn import metrics\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\n\ndf = pd.read_csv(\n    \"https://data.heatonresearch.com/data/t81-558/iris.csv\", \n    na_values=['NA', '?'])\n\n# Convert to numpy - Classification\nx = df[['sepal_l', 'sepal_w', 'petal_l', 'petal_w']].values\ndummies = pd.get_dummies(df['species']) # Classification\nspecies = dummies.columns\ny = dummies.values\n\n# Split into train/test\nx_train, x_test, y_train, y_test = train_test_split(    \n    x, y, test_size=0.25, random_state=42)\n\n# Build neural network\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=x.shape[1], activation='relu')) # Hidden 1\nmodel.add(Dense(25, activation='relu')) # Hidden 2\nmodel.add(Dense(y.shape[1],activation='softmax')) # Output\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\nmodel.fit(x_train,y_train,verbose=2,epochs=100)\n\n",
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Epoch 1/100\n112/112 - 0s - loss: 1.3953\nEpoch 2/100\n112/112 - 0s - loss: 1.2085\nEpoch 3/100\n112/112 - 0s - loss: 1.0975\nEpoch 4/100\n112/112 - 0s - loss: 0.9936\nEpoch 5/100\n112/112 - 0s - loss: 0.9165\nEpoch 6/100\n112/112 - 0s - loss: 0.8591\nEpoch 7/100\n112/112 - 0s - loss: 0.8223\nEpoch 8/100\n112/112 - 0s - loss: 0.7763\nEpoch 9/100\n112/112 - 0s - loss: 0.7443\nEpoch 10/100\n112/112 - 0s - loss: 0.7191\nEpoch 11/100\n112/112 - 0s - loss: 0.6912\nEpoch 12/100\n112/112 - 0s - loss: 0.6672\nEpoch 13/100\n112/112 - 0s - loss: 0.6444\nEpoch 14/100\n112/112 - 0s - loss: 0.6226\nEpoch 15/100\n112/112 - 0s - loss: 0.6023\nEpoch 16/100\n112/112 - 0s - loss: 0.5839\nEpoch 17/100\n112/112 - 0s - loss: 0.5654\nEpoch 18/100\n112/112 - 0s - loss: 0.5494\nEpoch 19/100\n112/112 - 0s - loss: 0.5332\nEpoch 20/100\n112/112 - 0s - loss: 0.5190\nEpoch 21/100\n112/112 - 0s - loss: 0.5049\nEpoch 22/100\n112/112 - 0s - loss: 0.4920\nEpoch 23/100\n112/112 - 0s - loss: 0.4808\nEpoch 24/100\n112/112 - 0s - loss: 0.4690\nEpoch 25/100\n112/112 - 0s - loss: 0.4582\nEpoch 26/100\n112/112 - 0s - loss: 0.4453\nEpoch 27/100\n112/112 - 0s - loss: 0.4372\nEpoch 28/100\n112/112 - 0s - loss: 0.4265\nEpoch 29/100\n112/112 - 0s - loss: 0.4167\nEpoch 30/100\n112/112 - 0s - loss: 0.4088\nEpoch 31/100\n112/112 - 0s - loss: 0.4003\nEpoch 32/100\n112/112 - 0s - loss: 0.3932\nEpoch 33/100\n112/112 - 0s - loss: 0.3861\nEpoch 34/100\n112/112 - 0s - loss: 0.3772\nEpoch 35/100\n112/112 - 0s - loss: 0.3701\nEpoch 36/100\n112/112 - 0s - loss: 0.3627\nEpoch 37/100\n112/112 - 0s - loss: 0.3561\nEpoch 38/100\n112/112 - 0s - loss: 0.3514\nEpoch 39/100\n112/112 - 0s - loss: 0.3436\nEpoch 40/100\n112/112 - 0s - loss: 0.3368\nEpoch 41/100\n112/112 - 0s - loss: 0.3305\nEpoch 42/100\n112/112 - 0s - loss: 0.3240\nEpoch 43/100\n112/112 - 0s - loss: 0.3184\nEpoch 44/100\n112/112 - 0s - loss: 0.3134\nEpoch 45/100\n112/112 - 0s - loss: 0.3030\nEpoch 46/100\n112/112 - 0s - loss: 0.2984\nEpoch 47/100\n112/112 - 0s - loss: 0.2893\nEpoch 48/100\n112/112 - 0s - loss: 0.2833\nEpoch 49/100\n112/112 - 0s - loss: 0.2804\nEpoch 50/100\n112/112 - 0s - loss: 0.2719\nEpoch 51/100\n112/112 - 0s - loss: 0.2648\nEpoch 52/100\n112/112 - 0s - loss: 0.2595\nEpoch 53/100\n112/112 - 0s - loss: 0.2541\nEpoch 54/100\n112/112 - 0s - loss: 0.2518\nEpoch 55/100\n112/112 - 0s - loss: 0.2451\nEpoch 56/100\n112/112 - 0s - loss: 0.2383\nEpoch 57/100\n112/112 - 0s - loss: 0.2291\nEpoch 58/100\n112/112 - 0s - loss: 0.2274\nEpoch 59/100\n112/112 - 0s - loss: 0.2231\nEpoch 60/100\n112/112 - 0s - loss: 0.2188\nEpoch 61/100\n112/112 - 0s - loss: 0.2137\nEpoch 62/100\n112/112 - 0s - loss: 0.2081\nEpoch 63/100\n112/112 - 0s - loss: 0.2039\nEpoch 64/100\n112/112 - 0s - loss: 0.1970\nEpoch 65/100\n112/112 - 0s - loss: 0.1927\nEpoch 66/100\n112/112 - 0s - loss: 0.1892\nEpoch 67/100\n112/112 - 0s - loss: 0.1846\nEpoch 68/100\n112/112 - 0s - loss: 0.1834\nEpoch 69/100\n112/112 - 0s - loss: 0.1781\nEpoch 70/100\n112/112 - 0s - loss: 0.1723\nEpoch 71/100\n112/112 - 0s - loss: 0.1683\nEpoch 72/100\n112/112 - 0s - loss: 0.1652\nEpoch 73/100\n112/112 - 0s - loss: 0.1633\nEpoch 74/100\n112/112 - 0s - loss: 0.1577\nEpoch 75/100\n112/112 - 0s - loss: 0.1562\nEpoch 76/100\n112/112 - 0s - loss: 0.1538\nEpoch 77/100\n112/112 - 0s - loss: 0.1513\nEpoch 78/100\n112/112 - 0s - loss: 0.1468\nEpoch 79/100\n112/112 - 0s - loss: 0.1441\nEpoch 80/100\n112/112 - 0s - loss: 0.1412\nEpoch 81/100\n112/112 - 0s - loss: 0.1396\nEpoch 82/100\n112/112 - 0s - loss: 0.1375\nEpoch 83/100\n112/112 - 0s - loss: 0.1388\nEpoch 84/100\n112/112 - 0s - loss: 0.1348\nEpoch 85/100\n112/112 - 0s - loss: 0.1300\nEpoch 86/100\n112/112 - 0s - loss: 0.1279\nEpoch 87/100\n112/112 - 0s - loss: 0.1261\nEpoch 88/100\n112/112 - 0s - loss: 0.1245\nEpoch 89/100\n112/112 - 0s - loss: 0.1223\nEpoch 90/100\n112/112 - 0s - loss: 0.1218\nEpoch 91/100\n112/112 - 0s - loss: 0.1194\nEpoch 92/100\n112/112 - 0s - loss: 0.1179\nEpoch 93/100\n112/112 - 0s - loss: 0.1161\nEpoch 94/100\n112/112 - 0s - loss: 0.1152\nEpoch 95/100\n112/112 - 0s - loss: 0.1131\nEpoch 96/100\n112/112 - 0s - loss: 0.1120\nEpoch 97/100\n112/112 - 0s - loss: 0.1109\nEpoch 98/100\n112/112 - 0s - loss: 0.1111\nEpoch 99/100\n112/112 - 0s - loss: 0.1078\nEpoch 100/100\n112/112 - 0s - loss: 0.1088\n"
        },
        {
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1a2d1efc50>"
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import accuracy_score\n\npred = model.predict(x_test)\npredict_classes = np.argmax(pred,axis=1)\nexpected_classes = np.argmax(y_test,axis=1)\ncorrect = accuracy_score(expected_classes,predict_classes)\nprint(f\"Accuracy: {correct}\")",
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Accuracy: 0.9736842105263158\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Rank the features\nfrom IPython.display import display, HTML\n\nnames = list(df.columns) # x+y column names\nnames.remove(\"species\") # remove the target(y)\nrank = perturbation_rank(model, x_test, y_test, names, False)\ndisplay(rank)",
      "execution_count": 13,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>error</th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>petal_l</td>\n      <td>3.679627</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>petal_w</td>\n      <td>0.733134</td>\n      <td>0.199241</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sepal_l</td>\n      <td>0.161628</td>\n      <td>0.043925</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sepal_w</td>\n      <td>0.092983</td>\n      <td>0.025270</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "      name     error  importance\n0  petal_l  3.679627    1.000000\n1  petal_w  0.733134    0.199241\n2  sepal_l  0.161628    0.043925\n3  sepal_w  0.092983    0.025270"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Regression and Input Perturbation Ranking"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport os\nimport numpy as np\nfrom sklearn import metrics\nfrom scipy.stats import zscore\nfrom sklearn.model_selection import KFold\nfrom IPython.display import HTML, display\n\npath = \"./data/\"\n\nfilename_train = os.path.join(path,\"bio_train.csv\")\nfilename_test = os.path.join(path,\"bio_test.csv\")\nfilename_submit = os.path.join(path,\"bio_submit.csv\")\ndf_train = pd.read_csv(filename_train,na_values=['NA','?'])\ndf_test = pd.read_csv(filename_test,na_values=['NA','?'])\n\nactivity_classes = encode_text_index(df_train,'Activity')",
      "execution_count": 14,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'encode_text_index' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d3cfe3be6b44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NA'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'?'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mactivity_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_text_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Activity'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'encode_text_index' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(df_train.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Biological Response with Neural Network"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import os\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport tensorflow.contrib.learn as skflow\nimport numpy as np\nimport sklearn\n\n# Set the desired TensorFlow output level for this example\ntf.logging.set_verbosity(tf.logging.ERROR)\n\n# Encode feature vector\nx, y = to_xy(df_train,'Activity')\nx_submit = df_test.as_matrix().astype(np.float32)\nnum_classes = len(activity_classes)\n\n# Split into train/test\nx_train, x_test, y_train, y_test = train_test_split(    \n    x, y, test_size=0.25, random_state=42) \n\nprint(\"Fitting/Training...\")\nmodel = Sequential()\nmodel.add(Dense(25, input_dim=x.shape[1], activation='relu'))\nmodel.add(Dense(10))\nmodel.add(Dense(y.shape[1],activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\nmonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\nmodel.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=0,epochs=1000)\nprint(\"Fitting done...\")\n\n# Give logloss error\npred = model.predict(x_test)\npred2 = np.argmax(pred,axis=1)\npred = pred[:,1]\n# Clip so that min is never exactly 0, max never 1\npred = np.clip(pred,a_min=1e-6,a_max=(1-1e-6)) \nprint(\"Validation logloss: {}\".format(sklearn.metrics.log_loss(y_test,pred)))\n\n# Evaluate success using accuracy\npred_submit = pred.copy()\ny_test = y_test[:,1]\nscore = metrics.accuracy_score(y_test, pred2)\nprint(\"Validation accuracy score: {}\".format(score))\n\n# Build real submit file\npred_submit = model.predict(x_submit)\npred_submit = pred_submit[:,1]\n\n# Clip so that min is never exactly 0, max never 1\npred = np.clip(pred,a_min=1e-6,a_max=(1-1e-6)) \nsubmit_df = pd.DataFrame({'MoleculeId':[x+1 for x in range(len(pred_submit))],'PredictedProbability':pred_submit})\nsubmit_df.to_csv(filename_submit, index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### What Features/Columns are Important\nThe following uses perturbation ranking to evaluate the neural network."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Rank the features\nfrom IPython.display import display, HTML\n\nnames = list(df_train.columns) # x+y column names\nnames.remove(\"Activity\") # remove the target(y)\nrank = perturbation_rank(model, x_test, y_test, names, False)\ndisplay(rank)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Neural Network Ensemble\n\nA neural network ensemble combines neural network predictions with other models. The exact blend of all of these models is determined by logistic regression. The following code performs this blend for a classification."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport os\nimport pandas as pd\nimport math\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nPATH = \"./data/\"\nSHUFFLE = False\nFOLDS = 10\n\ndef build_ann(input_size,classes,neurons):\n    model = Sequential()\n    model.add(Dense(neurons, input_dim=input_size, activation='relu'))\n    model.add(Dense(1))\n    model.add(Dense(classes,activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    return model\n\ndef mlogloss(y_test, preds):\n    epsilon = 1e-15\n    sum = 0\n    for row in zip(preds,y_test):\n        x = row[0][row[1]]\n        x = max(epsilon,x)\n        x = min(1-epsilon,x)\n        sum+=math.log(x)\n    return( (-1/len(preds))*sum)\n\ndef stretch(y):\n    return (y - y.min()) / (y.max() - y.min())\n\n\ndef blend_ensemble(x, y, x_submit):\n    kf = StratifiedKFold(FOLDS)\n    folds = list(kf.split(x,y))\n    feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x.shape[0])]\n\n    models = [\n        KerasClassifier(build_fn=build_ann,neurons=20,input_size=x.shape[1],classes=2),\n        KNeighborsClassifier(n_neighbors=3),\n        RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n        RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50)]\n\n    dataset_blend_train = np.zeros((x.shape[0], len(models)))\n    dataset_blend_test = np.zeros((x_submit.shape[0], len(models)))\n\n    for j, model in enumerate(models):\n        print(\"Model: {} : {}\".format(j, model) )\n        fold_sums = np.zeros((x_submit.shape[0], len(folds)))\n        total_loss = 0\n        for i, (train, test) in enumerate(folds):\n            x_train = x[train]\n            y_train = y[train]\n            x_test = x[test]\n            y_test = y[test]\n            model.fit(x_train, y_train)\n            pred = np.array(model.predict_proba(x_test))\n            # pred = model.predict_proba(x_test)\n            dataset_blend_train[test, j] = pred[:, 1]\n            pred2 = np.array(model.predict_proba(x_submit))\n            #fold_sums[:, i] = model.predict_proba(x_submit)[:, 1]\n            fold_sums[:, i] = pred2[:, 1]\n            loss = mlogloss(y_test, pred)\n            total_loss+=loss\n            print(\"Fold #{}: loss={}\".format(i,loss))\n        print(\"{}: Mean loss={}\".format(model.__class__.__name__,total_loss/len(folds)))\n        dataset_blend_test[:, j] = fold_sums.mean(1)\n\n    print()\n    print(\"Blending models.\")\n    blend = LogisticRegression()\n    blend.fit(dataset_blend_train, y)\n    return blend.predict_proba(dataset_blend_test)\n\nif __name__ == '__main__':\n\n    np.random.seed(42)  # seed to shuffle the train set\n\n    print(\"Loading data...\")\n    filename_train = os.path.join(PATH, \"bio_train.csv\")\n    df_train = pd.read_csv(filename_train, na_values=['NA', '?'])\n\n    filename_submit = os.path.join(PATH, \"bio_test.csv\")\n    df_submit = pd.read_csv(filename_submit, na_values=['NA', '?'])\n\n    predictors = list(df_train.columns.values)\n    predictors.remove('Activity')\n    x = df_train[predictors].values\n    y = df_train['Activity']\n    x_submit = df_submit.values\n\n    if SHUFFLE:\n        idx = np.random.permutation(y.size)\n        x = x[idx]\n        y = y[idx]\n\n    submit_data = blend_ensemble(x, y, x_submit)\n    submit_data = stretch(submit_data)\n\n    ####################\n    # Build submit file\n    ####################\n    ids = [id+1 for id in range(submit_data.shape[0])]\n    submit_filename = os.path.join(PATH, \"bio_submit.csv\")\n    submit_df = pd.DataFrame({'MoleculeId': ids, 'PredictedProbability': submit_data[:, 1]},\n                             columns=['MoleculeId','PredictedProbability'])\n    submit_df.to_csv(submit_filename, index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Classification and Input Perturbation Ranking"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Part 8.3: How Should you Architect Your Keras Neural Network: Hyperparameters\n"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Part 8.4: Bayesian Hyperparameter Optimization for Keras\n"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Part 8.5: Current Semester's Kaggle\n\nKaggke competition site for current semester (Fall 2019):\n\n* Coming soon\n\nPrevious Kaggle competition sites for this class (NOT this semester's assignment, feel free to use code):\n* [Spring 2019 Kaggle Assignment](https://www.kaggle.com/c/applications-of-deep-learningwustl-spring-2019)\n* [Fall 2018 Kaggle Assignment](https://www.kaggle.com/c/wustl-t81-558-washu-deep-learning-fall-2018)\n* [Spring 2018 Kaggle Assignment](https://www.kaggle.com/c/wustl-t81-558-washu-deep-learning-spring-2018)\n* [Fall 2017 Kaggle Assignment](https://www.kaggle.com/c/wustl-t81-558-washu-deep-learning-fall-2017)\n* [Spring 2017 Kaggle Assignment](https://inclass.kaggle.com/c/applications-of-deep-learning-wustl-spring-2017)\n* [Fall 2016 Kaggle Assignment](https://inclass.kaggle.com/c/wustl-t81-558-washu-deep-learning-fall-2016)\n"
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Module 8 Assignment\n\nYou can find the first assignment here: [assignment 8](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class8.ipynb)"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}